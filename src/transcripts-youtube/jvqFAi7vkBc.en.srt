1
00:00:00,000 --> 00:00:02,818
- I think compute is gonna be
the currency of the future.

2
00:00:02,818 --> 00:00:05,617
I think it'll be maybe the
most precious commodity

3
00:00:05,617 --> 00:00:06,615
in the world.

4
00:00:06,615 --> 00:00:09,739
I expect that by the end of this decade.

5
00:00:09,739 --> 00:00:12,989
And possibly somewhat sooner than that,

6
00:00:15,429 --> 00:00:18,136
we will have quite capable systems

7
00:00:18,136 --> 00:00:21,309
that we look at and say, wow,
that's really remarkable.

8
00:00:21,309 --> 00:00:24,899
The road to AGI should be
a giant power struggle.

9
00:00:24,899 --> 00:00:26,640
I expect that to be the case.

10
00:00:26,640 --> 00:00:30,557
- Whoever builds AGI
first gets a lot of power.

11
00:00:32,038 --> 00:00:35,621
Do you trust yourself
with that much power?

12
00:00:36,537 --> 00:00:40,091
The following is a
conversation with Sam Altman,

13
00:00:40,091 --> 00:00:41,931
his second time in the podcast.

14
00:00:41,931 --> 00:00:44,275
He is the CEO of OpenAI,

15
00:00:44,275 --> 00:00:47,608
the company behind GPT-4, ChatGPT, Sora,

16
00:00:48,923 --> 00:00:53,673
and perhaps one day the very
company that will build AGI.

17
00:00:55,649 --> 00:00:57,815
This is Lex Fridman Podcast.

18
00:00:57,815 --> 00:00:58,648
To support it,

19
00:00:58,648 --> 00:01:01,140
please check out our
sponsors in the description.

20
00:01:01,140 --> 00:01:02,565
And now, dear friends,

21
00:01:02,565 --> 00:01:04,065
here's Sam Altman.

22
00:01:05,559 --> 00:01:08,228
Take me through the OpenAI board saga

23
00:01:08,228 --> 00:01:11,070
that started on Thursday, November 16th,

24
00:01:11,070 --> 00:01:13,527
maybe Friday, November 17th for you.

25
00:01:13,527 --> 00:01:14,646
- That was definitely

26
00:01:14,646 --> 00:01:18,557
the most painful professional
experience of my life

27
00:01:18,557 --> 00:01:21,890
and chaotic, and shameful, and upsetting

28
00:01:27,572 --> 00:01:30,348
and a bunch of other negative things.

29
00:01:30,348 --> 00:01:32,134
There were great things about it too

30
00:01:32,134 --> 00:01:34,301
and I wish it had not been

31
00:01:36,348 --> 00:01:39,002
in such an adrenaline rush
that I wasn't able to stop

32
00:01:39,002 --> 00:01:41,669
and appreciate them at the time.

33
00:01:45,085 --> 00:01:47,041
I came across this old tweet of mine

34
00:01:47,041 --> 00:01:48,523
or this tweet of mine
from that time period,

35
00:01:48,523 --> 00:01:51,906
which was it was like kind
of going to your own eulogy,

36
00:01:51,906 --> 00:01:53,816
watching people say all
these great things about you

37
00:01:53,816 --> 00:01:57,479
and just like unbelievable support

38
00:01:57,479 --> 00:02:00,312
from people I love and care about.

39
00:02:01,819 --> 00:02:03,569
That was really nice.

40
00:02:04,461 --> 00:02:05,537
That whole weekend

41
00:02:05,537 --> 00:02:08,677
I kind of like felt
with one big exception,

42
00:02:08,677 --> 00:02:11,344
I felt like a great deal of love

43
00:02:13,359 --> 00:02:15,026
and very little hate

44
00:02:18,482 --> 00:02:21,548
even though it felt like I
have no idea what's happening

45
00:02:21,548 --> 00:02:22,742
and what's gonna happen here

46
00:02:22,742 --> 00:02:24,129
and this feels really bad.

47
00:02:24,129 --> 00:02:25,941
And there were definitely times

48
00:02:25,941 --> 00:02:28,288
I thought it was gonna be
like one of the worst things

49
00:02:28,288 --> 00:02:30,601
to ever happen for AI safety.

50
00:02:30,601 --> 00:02:32,238
Well, I also think I'm happy

51
00:02:32,238 --> 00:02:34,983
that it happened relatively early.

52
00:02:34,983 --> 00:02:36,313
I thought at some point

53
00:02:36,313 --> 00:02:39,236
between when OpenAI started

54
00:02:39,236 --> 00:02:41,416
and when we created AGI,

55
00:02:41,416 --> 00:02:42,874
there was gonna be something crazy

56
00:02:42,874 --> 00:02:44,851
and explosive that happened,

57
00:02:44,851 --> 00:02:49,735
but there may be more crazy
and explosive things happen.

58
00:02:49,735 --> 00:02:53,985
It still I think helped us
build up some resilience

59
00:02:55,154 --> 00:02:59,071
and be ready for more
challenges in the future.

60
00:03:02,095 --> 00:03:05,124
- But the thing you had a sense

61
00:03:05,124 --> 00:03:08,724
that you would experience is
some kind of power struggle.

62
00:03:08,724 --> 00:03:12,095
- The road to AGI should
be a giant power struggle.

63
00:03:12,095 --> 00:03:14,261
Like the world should...

64
00:03:14,261 --> 00:03:15,636
Well, not should.

65
00:03:15,636 --> 00:03:17,240
I expect that to be the case.

66
00:03:17,240 --> 00:03:20,272
- And so you have to go through that,

67
00:03:20,272 --> 00:03:21,634
like you said,

68
00:03:21,634 --> 00:03:23,967
iterate as often as possible

69
00:03:24,810 --> 00:03:26,819
in figuring out how to
have a board structure,

70
00:03:26,819 --> 00:03:28,279
how to have organization,

71
00:03:28,279 --> 00:03:31,699
how to have the kind of people
that you're working with,

72
00:03:31,699 --> 00:03:33,276
how to communicate all that

73
00:03:33,276 --> 00:03:36,537
in order to deescalate the power struggle

74
00:03:36,537 --> 00:03:38,937
as much as possible, pacify it.

75
00:03:38,937 --> 00:03:40,249
- But at this point,

76
00:03:40,249 --> 00:03:43,916
it feels like something
that was in the past

77
00:03:47,685 --> 00:03:51,411
that was really unpleasant and
really difficult and painful.

78
00:03:51,411 --> 00:03:55,535
But we're back to work
and things are so busy

79
00:03:55,535 --> 00:03:59,284
and so intense that I don't spend a lot

80
00:03:59,284 --> 00:04:00,702
of time thinking about it.

81
00:04:00,702 --> 00:04:03,208
There was a time after.

82
00:04:03,208 --> 00:04:05,662
There was like this fugue state

83
00:04:05,662 --> 00:04:07,843
for kind of like the month after,

84
00:04:07,843 --> 00:04:09,802
maybe 45 days after

85
00:04:09,802 --> 00:04:13,870
that was I was just sort of
like drifting through the days,

86
00:04:13,870 --> 00:04:16,160
I was so out of it.

87
00:04:16,160 --> 00:04:17,864
I was feeling so down

88
00:04:17,864 --> 00:04:19,864
- [Lex] Just on a personal
psychological level.

89
00:04:19,864 --> 00:04:20,771
- Yeah.

90
00:04:20,771 --> 00:04:22,021
Really painful.

91
00:04:22,996 --> 00:04:26,246
And hard to have to keep running OpenAI

92
00:04:27,345 --> 00:04:28,322
in the middle of that.

93
00:04:28,322 --> 00:04:30,542
I just wanted to crawl into a cave

94
00:04:30,542 --> 00:04:32,876
and kind of recover for a while.

95
00:04:32,876 --> 00:04:35,704
But now it's like we're just back

96
00:04:35,704 --> 00:04:38,506
to working on the mission.

97
00:04:38,506 --> 00:04:41,714
- Well, it's still useful to go back there

98
00:04:41,714 --> 00:04:44,381
and reflect on board structures,

99
00:04:45,728 --> 00:04:47,228
on power dynamics,

100
00:04:48,817 --> 00:04:50,963
on how companies are run,

101
00:04:50,963 --> 00:04:54,926
the tension between research,
and product development,

102
00:04:54,926 --> 00:04:57,315
and money and all this kind of stuff

103
00:04:57,315 --> 00:05:01,207
so that you have a very high potential

104
00:05:01,207 --> 00:05:05,687
of building AGI would do so
in a slightly more organized,

105
00:05:05,687 --> 00:05:07,803
less dramatic way in the future.

106
00:05:07,803 --> 00:05:09,220
So there's value there

107
00:05:09,220 --> 00:05:12,487
to go both the personal
psychological aspects of you

108
00:05:12,487 --> 00:05:15,964
as a leader and also
just the board structure

109
00:05:15,964 --> 00:05:18,261
and all this kind of messy stuff.

110
00:05:18,261 --> 00:05:23,011
- Definitely learned a lot
about structure and incentives

111
00:05:23,983 --> 00:05:26,566
and what we need out of a board

112
00:05:28,570 --> 00:05:31,093
And I think that it is valuable

113
00:05:31,093 --> 00:05:34,176
that this happened now in some sense.

114
00:05:35,124 --> 00:05:36,740
I think this is probably not

115
00:05:36,740 --> 00:05:39,082
like the last high
stress moment of OpenAI,

116
00:05:39,082 --> 00:05:41,094
but it was quite a high stress moment.

117
00:05:41,094 --> 00:05:43,549
Company very nearly got destroyed.

118
00:05:43,549 --> 00:05:45,966
And we think a lot about many

119
00:05:48,148 --> 00:05:50,580
of the other things we've
gotta get right for AGI.

120
00:05:50,580 --> 00:05:53,714
But thinking about how
to build a resilient org

121
00:05:53,714 --> 00:05:56,312
and how to build a
structure that will stand up

122
00:05:56,312 --> 00:05:57,775
to a lot of pressure the world,

123
00:05:57,775 --> 00:06:00,226
which I expect more and
more as we get closer.

124
00:06:00,226 --> 00:06:02,107
I think that's super important.

125
00:06:02,107 --> 00:06:04,028
- Do you have a sense of how deep

126
00:06:04,028 --> 00:06:07,549
and rigorous the deliberation
process by the board was?

127
00:06:07,549 --> 00:06:10,309
Can you shine some light

128
00:06:10,309 --> 00:06:12,927
on just human dynamics involved
in situations like this?

129
00:06:12,927 --> 00:06:14,604
Was it just a few conversations

130
00:06:14,604 --> 00:06:16,444
and all of a sudden it escalates

131
00:06:16,444 --> 00:06:19,443
and why don't we fire Sam kind of thing?

132
00:06:19,443 --> 00:06:22,526
- I think the board members were far,

133
00:06:26,109 --> 00:06:28,859
well-meaning people on the whole.

134
00:06:30,785 --> 00:06:34,285
And I believe that in stressful situations

135
00:06:40,692 --> 00:06:44,359
where people feel time
pressure or whatever,

136
00:06:47,460 --> 00:06:49,863
people understandably
make suboptimal decisions.

137
00:06:49,863 --> 00:06:51,512
And I think one of the challenges

138
00:06:51,512 --> 00:06:56,035
for OpenAI will be we're
gonna have to have a board

139
00:06:56,035 --> 00:07:00,773
and a team that are good at
operating under pressure.

140
00:07:00,773 --> 00:07:03,114
- Do you think the board
had too much power?

141
00:07:03,114 --> 00:07:06,515
- I think boards are supposed
to have a lot of power,

142
00:07:06,515 --> 00:07:09,120
but one of the things that we did see is

143
00:07:09,120 --> 00:07:11,273
in most corporate structures,

144
00:07:11,273 --> 00:07:14,937
boards are usually
answerable to shareholders.

145
00:07:14,937 --> 00:07:18,589
Sometimes people have like
super voting shares or whatever.

146
00:07:18,589 --> 00:07:19,422
In this case,

147
00:07:19,422 --> 00:07:21,347
I think one of the
things with our structure

148
00:07:21,347 --> 00:07:25,649
that we maybe should have
thought about more than we did is

149
00:07:25,649 --> 00:07:28,745
that the board of a nonprofit has,

150
00:07:28,745 --> 00:07:31,352
unless you put other rules in place,

151
00:07:31,352 --> 00:07:32,633
like quite a lot of power,

152
00:07:32,633 --> 00:07:34,950
they don't really answer
to anyone but themselves.

153
00:07:34,950 --> 00:07:37,205
And there's ways in which that's good,

154
00:07:37,205 --> 00:07:40,093
but what we'd really like
is for the board of OpenAI

155
00:07:40,093 --> 00:07:42,213
to answer to the world as a whole

156
00:07:42,213 --> 00:07:43,991
as much as that's a practical thing.

157
00:07:43,991 --> 00:07:46,587
- So there's a new board announced.

158
00:07:46,587 --> 00:07:47,565
- [Sam] Yeah.

159
00:07:47,565 --> 00:07:51,632
- There's, I guess, a new
smaller board of first

160
00:07:51,632 --> 00:07:53,716
and now there's a new final board.

161
00:07:53,716 --> 00:07:54,830
- Not a final board yet.

162
00:07:54,830 --> 00:07:55,663
We've added some,

163
00:07:55,663 --> 00:07:56,496
we'll add more.

164
00:07:56,496 --> 00:07:57,804
- Added some, okay.

165
00:07:57,804 --> 00:08:02,137
What is fixed in the new
one that was perhaps broken

166
00:08:04,380 --> 00:08:05,995
in the previous one?

167
00:08:05,995 --> 00:08:09,274
- The old board sort of got smaller

168
00:08:09,274 --> 00:08:10,559
over the course of about a year.

169
00:08:10,559 --> 00:08:12,763
It was nine and then it went down to six

170
00:08:12,763 --> 00:08:15,801
and then we couldn't agree on who to add.

171
00:08:15,801 --> 00:08:20,250
And the board also, I
think, didn't have a lot

172
00:08:20,250 --> 00:08:22,095
of experienced board members

173
00:08:22,095 --> 00:08:24,337
and a lot of the new board members

174
00:08:24,337 --> 00:08:29,337
at OpenAI have just have more
experience as board members.

175
00:08:29,555 --> 00:08:31,324
I think that'll help.

176
00:08:31,324 --> 00:08:33,818
- It's been criticized some of the people

177
00:08:33,818 --> 00:08:35,166
that are added to the board.

178
00:08:35,166 --> 00:08:37,139
I heard a lot of people
criticizing the addition

179
00:08:37,139 --> 00:08:39,365
of Larry Summers, for example.

180
00:08:39,365 --> 00:08:41,680
What was the process
of selecting the board?

181
00:08:41,680 --> 00:08:43,057
What's involved in that?

182
00:08:43,057 --> 00:08:46,755
- So Bret and Larry were
kind of decided in the heat

183
00:08:46,755 --> 00:08:49,196
of the moment over this very tense weekend

184
00:08:49,196 --> 00:08:50,037
and that was...

185
00:08:50,037 --> 00:08:52,272
I mean, that weekend was
like a real rollercoaster,

186
00:08:52,272 --> 00:08:54,689
like a lot of lots and downs.

187
00:08:56,552 --> 00:09:00,552
And we were trying to
agree on new board members

188
00:09:01,465 --> 00:09:05,493
that both sort of the executive team here

189
00:09:05,493 --> 00:09:10,180
and the old board members
felt would be reasonable.

190
00:09:10,180 --> 00:09:11,652
Larry was actually one
of their suggestions,

191
00:09:11,652 --> 00:09:13,926
the old board members.

192
00:09:13,926 --> 00:09:17,448
Bret, previous to that weekend, suggested,

193
00:09:17,448 --> 00:09:19,403
but he was busy and didn't wanna do it.

194
00:09:19,403 --> 00:09:22,291
And then we really needed help in wood.

195
00:09:22,291 --> 00:09:24,081
We talked about a lot of other people too,

196
00:09:24,081 --> 00:09:27,748
but I felt like if I
was going to come back,

197
00:09:30,961 --> 00:09:33,211
I needed new board members.

198
00:09:35,366 --> 00:09:38,345
I didn't think I could work
with the old board again

199
00:09:38,345 --> 00:09:39,503
in the same configuration,

200
00:09:39,503 --> 00:09:41,503
although we then decided,

201
00:09:41,503 --> 00:09:44,450
and I'm grateful that Adam would stay,

202
00:09:44,450 --> 00:09:46,401
but we wanted to get to...

203
00:09:46,401 --> 00:09:48,469
We considered various configurations,

204
00:09:48,469 --> 00:09:50,758
decided we wanted to
get to a board of three

205
00:09:50,758 --> 00:09:53,966
and had to find two new board members

206
00:09:53,966 --> 00:09:57,664
over the course of sort
of a short period of time.

207
00:09:57,664 --> 00:10:00,347
So those were decided honestly without...

208
00:10:00,347 --> 00:10:02,807
That's like you kind of do
that on the battlefield.

209
00:10:02,807 --> 00:10:05,829
You don't have time to design
a rigorous process then.

210
00:10:05,829 --> 00:10:06,983
For new board members,

211
00:10:06,983 --> 00:10:11,268
since new board members
will add going forward,

212
00:10:11,268 --> 00:10:15,491
we have some criteria that
we think are important

213
00:10:15,491 --> 00:10:17,888
for the board to have different expertise

214
00:10:17,888 --> 00:10:19,652
that we want the board to have.

215
00:10:19,652 --> 00:10:21,193
Unlike hiring an executive

216
00:10:21,193 --> 00:10:22,723
where you need them to do one role,

217
00:10:22,723 --> 00:10:25,259
well, the board needs to
do a whole role of kind

218
00:10:25,259 --> 00:10:28,312
of governance and thoughtfulness.

219
00:10:28,312 --> 00:10:32,030
Well, and so one thing that Bret says,

220
00:10:32,030 --> 00:10:34,476
which I really like is that
we wanna hire board members

221
00:10:34,476 --> 00:10:35,559
in slates,

222
00:10:35,559 --> 00:10:37,673
not as individuals one at a time.

223
00:10:37,673 --> 00:10:40,070
And thinking about a group of people

224
00:10:40,070 --> 00:10:43,040
that will bring nonprofit expertise,

225
00:10:43,040 --> 00:10:44,508
expertise at running companies,

226
00:10:44,508 --> 00:10:47,277
sort of good legal and
governance expertise.

227
00:10:47,277 --> 00:10:48,972
That's kind of what we've
tried to optimize for.

228
00:10:48,972 --> 00:10:50,947
- So is technical savvy important

229
00:10:50,947 --> 00:10:52,150
for the individual board members?

230
00:10:52,150 --> 00:10:53,289
- Not for every board member,

231
00:10:53,289 --> 00:10:55,015
but certainly some you need that.

232
00:10:55,015 --> 00:10:56,518
That's part of what the board needs to do.

233
00:10:56,518 --> 00:10:58,492
- So I mean, the interesting thing

234
00:10:58,492 --> 00:11:00,223
that people probably don't understand

235
00:11:00,223 --> 00:11:02,904
about OpenAI certainly
is like all the details

236
00:11:02,904 --> 00:11:04,090
of running the business.

237
00:11:04,090 --> 00:11:06,659
When they think about the
board given the drama,

238
00:11:06,659 --> 00:11:07,723
they think about you,

239
00:11:07,723 --> 00:11:11,435
they think about like if you reach AGI

240
00:11:11,435 --> 00:11:14,373
or you reach some of these
incredibly impactful products

241
00:11:14,373 --> 00:11:16,285
and you build them and deploy them,

242
00:11:16,285 --> 00:11:18,305
what's the conversation
with the board like?

243
00:11:18,305 --> 00:11:19,803
And they kind of think,

244
00:11:19,803 --> 00:11:22,317
all right, what's the right squad

245
00:11:22,317 --> 00:11:25,855
to have in that kind of
situation to deliberate?

246
00:11:25,855 --> 00:11:27,184
- Look, I think you definitely need

247
00:11:27,184 --> 00:11:28,966
some technical experts there

248
00:11:28,966 --> 00:11:32,730
and then you need some
people who are like,

249
00:11:32,730 --> 00:11:34,575
how can we deploy this in a way

250
00:11:34,575 --> 00:11:37,159
that will help people
in the world the most

251
00:11:37,159 --> 00:11:40,113
and people who have a very
different perspective?

252
00:11:40,113 --> 00:11:41,840
I think a mistake that you

253
00:11:41,840 --> 00:11:42,955
or I might make is to think

254
00:11:42,955 --> 00:11:45,575
that only the technical
understanding matters.

255
00:11:45,575 --> 00:11:46,916
And that's definitely part

256
00:11:46,916 --> 00:11:48,853
of the conversation you
want that board to have.

257
00:11:48,853 --> 00:11:50,011
But there's a lot more

258
00:11:50,011 --> 00:11:51,685
about how that's gonna
just like impact society

259
00:11:51,685 --> 00:11:53,126
and people's lives

260
00:11:53,126 --> 00:11:56,810
that you really want
represented in there too.

261
00:11:56,810 --> 00:11:59,264
- Are you looking at the
track record of people

262
00:11:59,264 --> 00:12:00,900
or you're just having conversations?

263
00:12:00,900 --> 00:12:02,443
- Track record's a big deal.

264
00:12:02,443 --> 00:12:06,110
You, of course, have a
lot of conversations.

265
00:12:08,565 --> 00:12:09,568
There's some roles

266
00:12:09,568 --> 00:12:13,859
where I kind of totally
ignore track record

267
00:12:13,859 --> 00:12:16,250
and just look at slope,

268
00:12:16,250 --> 00:12:18,230
kind of ignore the y-intercept.

269
00:12:18,230 --> 00:12:19,063
- Thank you.

270
00:12:19,063 --> 00:12:21,473
Thank you for making it
mathematical for the audience,

271
00:12:21,473 --> 00:12:22,904
- For a board member,

272
00:12:22,904 --> 00:12:24,954
I do care much more about the y-intercept.

273
00:12:24,954 --> 00:12:26,537
I think there is something deep

274
00:12:26,537 --> 00:12:30,400
to say about track record
there and experiences,

275
00:12:30,400 --> 00:12:32,647
something's very hard to replace.

276
00:12:32,647 --> 00:12:34,540
- Do you try to fit a polynomial function

277
00:12:34,540 --> 00:12:36,488
or exponential one to track record?

278
00:12:36,488 --> 00:12:37,596
- That's not that.

279
00:12:37,596 --> 00:12:39,177
An analogy doesn't carry that far.

280
00:12:39,177 --> 00:12:40,010
- All right.

281
00:12:40,010 --> 00:12:44,451
You mentioned some of the
low points that weekend.

282
00:12:44,451 --> 00:12:47,606
What were some of the low
points psychologically for you?

283
00:12:47,606 --> 00:12:50,852
Did you consider going
to the Amazon jungle

284
00:12:50,852 --> 00:12:54,137
and just taking Ayahuasca
disappearing forever or?

285
00:12:54,137 --> 00:12:55,676
- I mean, there's so many low,

286
00:12:55,676 --> 00:12:58,276
like it was a very bad period of time.

287
00:12:58,276 --> 00:13:01,026
There were great high points too.

288
00:13:02,747 --> 00:13:05,540
My phone was just like
sort of nonstop blowing up

289
00:13:05,540 --> 00:13:07,439
with nice messages from people
I worked with every day,

290
00:13:07,439 --> 00:13:09,406
people I hadn't talked to in a decade.

291
00:13:09,406 --> 00:13:11,772
I didn't get to appreciate
that as much as I should have.

292
00:13:11,772 --> 00:13:13,784
'cause I was just like in
the middle of this firefight,

293
00:13:13,784 --> 00:13:14,837
but that was really nice.

294
00:13:14,837 --> 00:13:15,670
But on the whole,

295
00:13:15,670 --> 00:13:17,535
it was like a very painful weekend

296
00:13:17,535 --> 00:13:19,868
and also just like a very...

297
00:13:22,281 --> 00:13:26,489
It was like a battle fought in
public to a surprising degree

298
00:13:26,489 --> 00:13:28,663
and that was extremely exhausting to me,

299
00:13:28,663 --> 00:13:30,743
much more than I expected.

300
00:13:30,743 --> 00:13:32,521
I think fights are generally exhausting,

301
00:13:32,521 --> 00:13:35,029
but this one really was.

302
00:13:35,029 --> 00:13:38,029
The board did this Friday afternoon.

303
00:13:39,402 --> 00:13:42,011
I really couldn't get much
in the way of answers,

304
00:13:42,011 --> 00:13:43,347
but I also was just like,

305
00:13:43,347 --> 00:13:45,299
"Well, the board gets to do this."

306
00:13:45,299 --> 00:13:48,317
And so I'm gonna think for a little bit

307
00:13:48,317 --> 00:13:49,490
about what I want to do,

308
00:13:49,490 --> 00:13:52,288
but I'll try to find the, the
blessing in disguise here.

309
00:13:52,288 --> 00:13:56,371
And I was like, "Well,
my current job at OpenAI,

310
00:13:58,005 --> 00:14:01,850
it was like to like run
a decently-sized company

311
00:14:01,850 --> 00:14:02,990
at this point."

312
00:14:02,990 --> 00:14:05,175
And the thing I'd always liked
the most was just getting

313
00:14:05,175 --> 00:14:07,131
to work with the researchers.

314
00:14:07,131 --> 00:14:08,936
And I was like, yeah,
I can just go do like

315
00:14:08,936 --> 00:14:11,264
a very focused AI research effort.

316
00:14:11,264 --> 00:14:12,972
And I got excited about.

317
00:14:12,972 --> 00:14:14,842
That didn't even occur to me at the time

318
00:14:14,842 --> 00:14:17,513
to like possibly that this
was all gonna get undone.

319
00:14:17,513 --> 00:14:18,732
This was like Friday afternoon.

320
00:14:18,732 --> 00:14:21,190
- Oh, so you've accepted the death-

321
00:14:21,190 --> 00:14:23,667
- Very quickly, very quickly.

322
00:14:23,667 --> 00:14:26,870
I mean, I went through
like a little period

323
00:14:26,870 --> 00:14:27,863
of confusion and rage,

324
00:14:27,863 --> 00:14:28,971
but very quickly.

325
00:14:28,971 --> 00:14:30,031
And by Friday night,

326
00:14:30,031 --> 00:14:33,893
I was talking to people
about what was gonna be next

327
00:14:33,893 --> 00:14:36,310
and I was excited about that.

328
00:14:37,659 --> 00:14:40,345
I think it was Friday night evening

329
00:14:40,345 --> 00:14:42,096
for the first time that I
heard from the exec team here,

330
00:14:42,096 --> 00:14:44,993
which is like, hey, we're
gonna like fight this

331
00:14:44,993 --> 00:14:46,010
and we think...

332
00:14:46,010 --> 00:14:47,357
Well, whatever.

333
00:14:47,357 --> 00:14:50,396
And then I went to bed just
still being like, okay,

334
00:14:50,396 --> 00:14:51,229
excited.

335
00:14:51,229 --> 00:14:52,279
- Like onward,

336
00:14:52,279 --> 00:14:54,028
were you able to sleep?

337
00:14:54,028 --> 00:14:54,861
- Not a lot.

338
00:14:54,861 --> 00:14:56,951
It was one of the weird
things was there was this

339
00:14:56,951 --> 00:14:59,489
like period of four and a half days

340
00:14:59,489 --> 00:15:01,833
where sort of didn't sleep much,

341
00:15:01,833 --> 00:15:02,876
didn't eat much

342
00:15:02,876 --> 00:15:05,869
and still kind of had like a
surprising amount of energy.

343
00:15:05,869 --> 00:15:09,197
You learn like a weird
thing about adrenaline

344
00:15:09,197 --> 00:15:10,030
and more time.

345
00:15:10,030 --> 00:15:12,978
- So you kind of accepted the
death of this baby OpenAI?

346
00:15:12,978 --> 00:15:14,528
- And I was excited for the new thing.

347
00:15:14,528 --> 00:15:16,945
I was just like, okay, this
was crazy, but whatever.

348
00:15:16,945 --> 00:15:18,379
- It's a very good coping mechanism.

349
00:15:18,379 --> 00:15:20,242
- And then Saturday morning,

350
00:15:20,242 --> 00:15:21,345
two of the board members called

351
00:15:21,345 --> 00:15:23,235
and said, "Hey, we destabilize.

352
00:15:23,235 --> 00:15:25,020
We didn't mean to destabilize things.

353
00:15:25,020 --> 00:15:27,027
We don't restore a lot of value here.

354
00:15:27,027 --> 00:15:29,381
Can we talk about you coming back?"

355
00:15:29,381 --> 00:15:32,299
And I immediately didn't wanna do that,

356
00:15:32,299 --> 00:15:34,104
but I thought a little more

357
00:15:34,104 --> 00:15:37,123
and I was like, "Well, I really
care about the people here,

358
00:15:37,123 --> 00:15:39,596
the partners, shareholders.

359
00:15:39,596 --> 00:15:41,284
I love this company."

360
00:15:41,284 --> 00:15:42,117
And so I thought about it

361
00:15:42,117 --> 00:15:43,134
and I was like, "Well, okay,

362
00:15:43,134 --> 00:15:45,714
but here's the stuff I would need."

363
00:15:45,714 --> 00:15:48,903
And then the most painful time of all

364
00:15:48,903 --> 00:15:51,570
over the course of that weekend,

365
00:15:52,658 --> 00:15:56,072
I kept thinking and being told...

366
00:15:56,072 --> 00:15:56,951
Not just me,

367
00:15:56,951 --> 00:15:58,513
like the whole team here kept thinking.

368
00:15:58,513 --> 00:16:01,259
Well, we were trying to
keep OpenAI stabilized

369
00:16:01,259 --> 00:16:02,901
while the whole world was
trying to break it apart,

370
00:16:02,901 --> 00:16:04,308
people trying to recruit, whatever.

371
00:16:04,308 --> 00:16:05,238
We kept being told like,

372
00:16:05,238 --> 00:16:06,071
"All right, we're almost done,

373
00:16:06,071 --> 00:16:06,904
we're almost done.

374
00:16:06,904 --> 00:16:09,279
We just need like a little bit more time."

375
00:16:09,279 --> 00:16:11,459
And it was this like very confusing state.

376
00:16:11,459 --> 00:16:13,967
And then Sunday evening

377
00:16:13,967 --> 00:16:15,828
when again like every few hours,

378
00:16:15,828 --> 00:16:17,408
I expected that we were gonna be done

379
00:16:17,408 --> 00:16:20,464
and we're gonna figure
out a way for me to return

380
00:16:20,464 --> 00:16:23,264
and things to go back to how they were,

381
00:16:23,264 --> 00:16:26,764
the board then appointed a new interim CEO

382
00:16:27,942 --> 00:16:29,158
and then I was like...

383
00:16:29,158 --> 00:16:30,779
I mean, that feels really bad.

384
00:16:30,779 --> 00:16:34,279
That was the low point of the whole thing.

385
00:16:36,164 --> 00:16:37,644
You know, I'll tell you something,

386
00:16:37,644 --> 00:16:39,394
it felt very painful,

387
00:16:40,415 --> 00:16:44,206
but I felt a lot of
love that whole weekend.

388
00:16:44,206 --> 00:16:47,839
It was not other than that
one moment, Sunday night,

389
00:16:47,839 --> 00:16:52,445
I would not characterize my
emotions as anger or hate,

390
00:16:52,445 --> 00:16:54,930
but I really just like...

391
00:16:54,930 --> 00:16:59,055
I felt a lot of love from
people towards people.

392
00:16:59,055 --> 00:17:00,336
It was like painful,

393
00:17:00,336 --> 00:17:02,222
but it was like the dominant emotion

394
00:17:02,222 --> 00:17:04,213
of the weekend was love, not hate.

395
00:17:04,213 --> 00:17:07,839
- You've spoken highly of
Mira Murati that she helped,

396
00:17:07,839 --> 00:17:09,681
especially as you put in a tweet,

397
00:17:09,681 --> 00:17:12,192
"In the quiet moments when it counts,

398
00:17:12,192 --> 00:17:14,330
perhaps we could take a bit of a tangent."

399
00:17:14,330 --> 00:17:16,171
What do you admire about Mira?

400
00:17:16,171 --> 00:17:18,748
- Well, she did a great
job during that weekend

401
00:17:18,748 --> 00:17:20,071
in a lot of chaos,

402
00:17:20,071 --> 00:17:23,009
but people often see leaders

403
00:17:23,009 --> 00:17:25,926
in the crisis moments, good or bad.

404
00:17:27,889 --> 00:17:29,301
But a thing I really value

405
00:17:29,301 --> 00:17:33,080
in leaders is how people
act on a boring Tuesday

406
00:17:33,080 --> 00:17:34,913
at 9:46 in the morning

407
00:17:35,777 --> 00:17:40,777
and in just sort of the normal
drudgery of the day-to-day,

408
00:17:41,404 --> 00:17:43,233
how someone shows up in a meeting,

409
00:17:43,233 --> 00:17:45,661
the quality of the decisions they make.

410
00:17:45,661 --> 00:17:48,032
That was what I meant
about the quiet moments.

411
00:17:48,032 --> 00:17:52,018
- Meaning like most of the
work is done on a day by day

412
00:17:52,018 --> 00:17:53,918
in a meeting by meeting,

413
00:17:53,918 --> 00:17:57,335
just be present and make great decisions.

414
00:17:58,270 --> 00:17:59,103
- Yeah.

415
00:17:59,103 --> 00:18:00,607
I mean, look, what you have wanted

416
00:18:00,607 --> 00:18:02,492
to spend the last 20 minutes about

417
00:18:02,492 --> 00:18:06,419
and I understand is like this
one very dramatic weekend.

418
00:18:06,419 --> 00:18:08,305
But that's not really
what OpenAI is about.

419
00:18:08,305 --> 00:18:10,642
OpenAI is really about
the other seven years.

420
00:18:10,642 --> 00:18:14,216
- Well, yeah, human civilization
is not about the invasion

421
00:18:14,216 --> 00:18:16,302
of the Soviet Union by Nazi Germany,

422
00:18:16,302 --> 00:18:18,942
but still that's something
people totally focus on.

423
00:18:18,942 --> 00:18:20,377
- Very understandable.

424
00:18:20,377 --> 00:18:22,727
- It gives us an insight
into human nature,

425
00:18:22,727 --> 00:18:24,683
the extremes of human nature,

426
00:18:24,683 --> 00:18:25,686
and perhaps some of the damage

427
00:18:25,686 --> 00:18:27,285
and some of the triumphs

428
00:18:27,285 --> 00:18:29,499
of human civilization can
happen in those moments.

429
00:18:29,499 --> 00:18:31,527
So it's like illustrative.

430
00:18:31,527 --> 00:18:33,807
Let me ask you about Ilya.

431
00:18:33,807 --> 00:18:36,492
Is he being held hostage in
a secret nuclear facility?

432
00:18:36,492 --> 00:18:37,325
- No.

433
00:18:37,325 --> 00:18:39,466
- What about a regular secret facility?

434
00:18:39,466 --> 00:18:40,299
- No.

435
00:18:40,299 --> 00:18:41,807
- What about a nuclear
non-secure facility?

436
00:18:41,807 --> 00:18:43,704
- Neither, not that either.

437
00:18:43,704 --> 00:18:45,380
- I mean, this is becoming
a meme at some point.

438
00:18:45,380 --> 00:18:47,851
You've known Ilya for a long time.

439
00:18:47,851 --> 00:18:50,816
He was obviously part of this drama

440
00:18:50,816 --> 00:18:54,093
with the board and all that kind of stuff.

441
00:18:54,093 --> 00:18:57,199
What's your relationship with him now?

442
00:18:57,199 --> 00:18:58,110
- I love Ilya.

443
00:18:58,110 --> 00:19:01,416
I have tremendous respect for Ilya.

444
00:19:01,416 --> 00:19:03,958
I don't have anything I can
say about his plans right now.

445
00:19:03,958 --> 00:19:06,893
That's a question for him.

446
00:19:06,893 --> 00:19:08,755
But I really hope we work together

447
00:19:08,755 --> 00:19:12,284
for certainly the rest of my career.

448
00:19:12,284 --> 00:19:13,261
He's a little bit younger than me,

449
00:19:13,261 --> 00:19:15,938
maybe he works a little bit longer.

450
00:19:15,938 --> 00:19:19,188
- There's a meme that he saw something,

451
00:19:20,083 --> 00:19:21,573
like he maybe saw AGI

452
00:19:21,573 --> 00:19:25,132
and that gave him a lot
of worry internally.

453
00:19:25,132 --> 00:19:26,632
What did Ilya see?

454
00:19:28,782 --> 00:19:30,150
- Ilya has not seen AGI,

455
00:19:30,150 --> 00:19:31,216
none of us have seen AGI.

456
00:19:31,216 --> 00:19:32,966
We've not built AGII.

457
00:19:35,855 --> 00:19:38,565
I do think one of the many things

458
00:19:38,565 --> 00:19:42,315
that I really love about
Ilya is he takes AGI

459
00:19:43,243 --> 00:19:46,660
and the safety concerns broadly speaking,

460
00:19:47,912 --> 00:19:51,009
including things like the
impact this is gonna have

461
00:19:51,009 --> 00:19:53,617
on society very seriously.

462
00:19:53,617 --> 00:19:57,617
And as we continue to
make significant progress,

463
00:19:58,797 --> 00:20:00,708
Ilya is one of the people

464
00:20:00,708 --> 00:20:02,288
that I've spent the most time

465
00:20:02,288 --> 00:20:04,108
over the last couple of years talking

466
00:20:04,108 --> 00:20:06,747
about what this is going to mean,

467
00:20:06,747 --> 00:20:08,767
what we need to do to
ensure we get it right

468
00:20:08,767 --> 00:20:11,974
to ensure that we succeed at the mission.

469
00:20:11,974 --> 00:20:13,974
So Ilya did not see AGI.

470
00:20:17,350 --> 00:20:20,017
But Ilya is a credit to humanity

471
00:20:23,867 --> 00:20:26,719
in terms of how much he thinks

472
00:20:26,719 --> 00:20:30,532
and worries about making
sure we get this right.

473
00:20:30,532 --> 00:20:32,543
- I've had a bunch of
conversation with him in the past.

474
00:20:32,543 --> 00:20:34,557
I think when he talks about technology,

475
00:20:34,557 --> 00:20:37,185
he's always like doing
this long-term thinking

476
00:20:37,185 --> 00:20:38,556
type of thing.

477
00:20:38,556 --> 00:20:40,666
So he is not thinking about
what this is gonna be in a year.

478
00:20:40,666 --> 00:20:42,303
He's thinking about in 10 years.

479
00:20:42,303 --> 00:20:43,136
- [Sam] Yeah.

480
00:20:43,136 --> 00:20:44,503
- Just thinking from first principles

481
00:20:44,503 --> 00:20:46,670
like, okay, if the scales,

482
00:20:47,793 --> 00:20:49,529
what are the fundamentals here?

483
00:20:49,529 --> 00:20:50,749
Where's this going?

484
00:20:50,749 --> 00:20:52,593
And so that's a foundation

485
00:20:52,593 --> 00:20:55,425
for them thinking about like
all the other safety concerns

486
00:20:55,425 --> 00:20:58,097
and all that kind of stuff,

487
00:20:58,097 --> 00:21:02,636
which makes him a really
fascinating human to talk with.

488
00:21:02,636 --> 00:21:05,894
Do you have any idea why
he's been kind of quiet?

489
00:21:05,894 --> 00:21:08,857
Is it he's just doing some soul searching?

490
00:21:08,857 --> 00:21:11,745
- Again, I don't wanna speak for Ilya.

491
00:21:11,745 --> 00:21:14,828
I think that you should ask him that.

492
00:21:18,001 --> 00:21:20,751
He's definitely a thoughtful guy.

493
00:21:23,045 --> 00:21:24,457
I think I kind of think of Ilya

494
00:21:24,457 --> 00:21:27,309
as like always on a soul
search in a really good way.

495
00:21:27,309 --> 00:21:28,142
- Yes.

496
00:21:28,142 --> 00:21:28,975
Yeah.

497
00:21:28,975 --> 00:21:32,143
Also he appreciates the power of silence.

498
00:21:32,143 --> 00:21:34,412
Also, I'm told he can be a silly guy,

499
00:21:34,412 --> 00:21:35,990
which I've never seen that side of him.

500
00:21:35,990 --> 00:21:39,604
- It's very sweet when that happens.

501
00:21:39,604 --> 00:21:41,651
- I've never witnessed a silly Ilya,

502
00:21:41,651 --> 00:21:43,666
but I look forward to that as well.

503
00:21:43,666 --> 00:21:45,184
- I was at a dinner
party with him recently

504
00:21:45,184 --> 00:21:47,049
and he was playing with a puppy.

505
00:21:47,049 --> 00:21:49,203
And he was like in a very
silly move, very endearing

506
00:21:49,203 --> 00:21:50,367
and I was thinking like, oh man,

507
00:21:50,367 --> 00:21:53,234
this is like not the side of the Ilya

508
00:21:53,234 --> 00:21:55,637
that the world sees the most.

509
00:21:55,637 --> 00:21:58,327
- So just to wrap up this whole saga,

510
00:21:58,327 --> 00:22:01,191
are you feeling good
about the board structure

511
00:22:01,191 --> 00:22:03,991
about all of this and where it's moving?

512
00:22:03,991 --> 00:22:05,480
- I feel great about the new board.

513
00:22:05,480 --> 00:22:08,022
In terms of the structure of OpenAI,

514
00:22:08,022 --> 00:22:10,655
one of the board's
tasks is to look at that

515
00:22:10,655 --> 00:22:13,255
and see where we can make it more robust.

516
00:22:13,255 --> 00:22:17,152
We wanted to get new board
members in place first,

517
00:22:17,152 --> 00:22:20,019
but we clearly learned
a lesson about structure

518
00:22:20,019 --> 00:22:21,617
throughout this process.

519
00:22:21,617 --> 00:22:25,067
I don't have I think
super deep things to say.

520
00:22:25,067 --> 00:22:27,674
It was a crazy, very painful experience.

521
00:22:27,674 --> 00:22:29,893
I think it was like a
perfect storm of weirdness.

522
00:22:29,893 --> 00:22:32,014
It was like a preview for
me of what's gonna happen

523
00:22:32,014 --> 00:22:33,895
as the stakes get higher and higher

524
00:22:33,895 --> 00:22:36,362
and the need that we have like
robust governance structures

525
00:22:36,362 --> 00:22:38,445
and processes and people.

526
00:22:39,954 --> 00:22:42,510
I am kind of happy it
happened when it did,

527
00:22:42,510 --> 00:22:47,500
but it was a shockingly
painful thing to go through.

528
00:22:47,500 --> 00:22:50,779
- Did it make you be more
hesitant in trusting people?

529
00:22:50,779 --> 00:22:51,612
- Yes.

530
00:22:51,612 --> 00:22:52,445
- Just on a personal level.

531
00:22:52,445 --> 00:22:53,278
- Yes.

532
00:22:53,278 --> 00:22:54,935
- I think I'm like an
extremely trusting person.

533
00:22:54,935 --> 00:22:56,917
I've always had a life philosophy

534
00:22:56,917 --> 00:22:59,392
of like don't worry about
all of the paranoia,

535
00:22:59,392 --> 00:23:01,218
don't worry about the edge cases.

536
00:23:01,218 --> 00:23:05,256
You get a little bit screwed in exchange

537
00:23:05,256 --> 00:23:07,637
for getting to live with your guard down.

538
00:23:07,637 --> 00:23:09,722
And this was so shocking to me.

539
00:23:09,722 --> 00:23:11,140
I was so caught off guard

540
00:23:11,140 --> 00:23:13,990
that it has definitely changed

541
00:23:13,990 --> 00:23:15,461
and I really don't like this.

542
00:23:15,461 --> 00:23:17,040
It's definitely changed how I think

543
00:23:17,040 --> 00:23:18,932
about just like default trust of people

544
00:23:18,932 --> 00:23:21,394
and planning for the bad scenarios.

545
00:23:21,394 --> 00:23:23,207
- You gotta be careful with that.

546
00:23:23,207 --> 00:23:26,341
Are you worried about
becoming a little too cynical?

547
00:23:26,341 --> 00:23:28,339
- I'm not worried about
becoming too cynical.

548
00:23:28,339 --> 00:23:30,672
I think I'm like the extreme
opposite of a cynical person.

549
00:23:30,672 --> 00:23:33,526
But I'm worried about
just becoming like less

550
00:23:33,526 --> 00:23:35,969
of a default trusting person.

551
00:23:35,969 --> 00:23:38,366
- I'm actually not sure which
mode is best to operate in

552
00:23:38,366 --> 00:23:41,199
for a person who's developing AGI,

553
00:23:42,694 --> 00:23:44,529
trusting or untrusting.

554
00:23:44,529 --> 00:23:47,696
It's an interesting journey you're on.

555
00:23:48,550 --> 00:23:50,149
But in terms of structure,

556
00:23:50,149 --> 00:23:52,920
see, I'm more interested
on the human level.

557
00:23:52,920 --> 00:23:53,963
How do you surround yourself

558
00:23:53,963 --> 00:23:56,574
with humans that are building cool shit,

559
00:23:56,574 --> 00:24:00,014
but also are making wise decisions?

560
00:24:00,014 --> 00:24:02,402
Because the more money you start making,

561
00:24:02,402 --> 00:24:05,528
the more power the thing
has the weirder people get.

562
00:24:05,528 --> 00:24:08,790
- I think you could make
all kinds of comments

563
00:24:08,790 --> 00:24:10,707
about the board members

564
00:24:11,771 --> 00:24:14,000
and the level of trust
I should have had there

565
00:24:14,000 --> 00:24:16,443
or how I should have
done things differently.

566
00:24:16,443 --> 00:24:18,942
But in terms of the team here,

567
00:24:18,942 --> 00:24:20,907
I think you'd have to like
give me a very good grade

568
00:24:20,907 --> 00:24:21,907
on that one.

569
00:24:23,578 --> 00:24:26,995
And I have just like
enormous gratitude and trust

570
00:24:26,995 --> 00:24:30,057
and respect for the people
that I work with every day.

571
00:24:30,057 --> 00:24:31,330
And I think being surrounded

572
00:24:31,330 --> 00:24:34,830
with people like that is really important.

573
00:24:39,955 --> 00:24:43,038
- Our mutual friend Elon sued OpenAI.

574
00:24:44,928 --> 00:24:48,517
What is the essence of
what he's criticizing?

575
00:24:48,517 --> 00:24:50,196
To what degree does he have a point?

576
00:24:50,196 --> 00:24:52,388
To what degree is he wrong?

577
00:24:52,388 --> 00:24:54,333
- I don't know what it's really about.

578
00:24:54,333 --> 00:24:58,571
We started off just thinking
we were gonna be a research lab

579
00:24:58,571 --> 00:25:03,258
and having no idea about how
this technology was gonna go.

580
00:25:03,258 --> 00:25:05,234
Because it was only
seven or eight years ago,

581
00:25:05,234 --> 00:25:06,366
it's hard to go back

582
00:25:06,366 --> 00:25:08,207
and really remember what it was like then.

583
00:25:08,207 --> 00:25:10,329
But before language
models were a big deal,

584
00:25:10,329 --> 00:25:12,770
this was before we had
any idea about an API

585
00:25:12,770 --> 00:25:15,140
or selling access to a chat bot.

586
00:25:15,140 --> 00:25:17,063
It was before we had any
idea we were gonna productize

587
00:25:17,063 --> 00:25:17,896
at all.

588
00:25:17,896 --> 00:25:20,732
So we're like we're just
gonna try to do research

589
00:25:20,732 --> 00:25:23,369
and we don't really know what
we're gonna do with that.

590
00:25:23,369 --> 00:25:25,856
I think with many new
fundamentally new things,

591
00:25:25,856 --> 00:25:27,966
you start fumbling through the dark

592
00:25:27,966 --> 00:25:29,352
and you make some assumptions,

593
00:25:29,352 --> 00:25:31,694
most of which turn out to be wrong.

594
00:25:31,694 --> 00:25:34,074
And then it became clear

595
00:25:34,074 --> 00:25:38,157
that we were going to need
to do different things

596
00:25:41,257 --> 00:25:44,081
and also have huge amounts more capital.

597
00:25:44,081 --> 00:25:45,242
So we said, "Okay, well,

598
00:25:45,242 --> 00:25:46,783
the structure doesn't quite work for that.

599
00:25:46,783 --> 00:25:49,173
How do we patch the structure?"

600
00:25:49,173 --> 00:25:50,831
And then you patch it
again and patch it again

601
00:25:50,831 --> 00:25:51,937
and you end up with something

602
00:25:51,937 --> 00:25:56,282
that does look kind of eyebrow
raising to say the least.

603
00:25:56,282 --> 00:25:57,939
But we got here gradually

604
00:25:57,939 --> 00:25:59,772
with I think reasonable decisions

605
00:25:59,772 --> 00:26:01,477
at each point along the way

606
00:26:01,477 --> 00:26:04,779
and doesn't mean I wouldn't
do it totally differently

607
00:26:04,779 --> 00:26:06,287
if we could go back now with an oracle,

608
00:26:06,287 --> 00:26:08,394
but you don't get the oracle at the time.

609
00:26:08,394 --> 00:26:09,227
But anyway,

610
00:26:09,227 --> 00:26:11,180
in terms of what Elon's
real motivations here are

611
00:26:11,180 --> 00:26:12,865
I don't know.

612
00:26:12,865 --> 00:26:14,439
- To the degree you remember,

613
00:26:14,439 --> 00:26:19,125
what was the response that
OpenAI gave in the blog post?

614
00:26:19,125 --> 00:26:21,535
Can you summarize it?

615
00:26:21,535 --> 00:26:25,952
- Oh, we just said like Elon
said this set of things,

616
00:26:26,953 --> 00:26:28,956
here's our characterization

617
00:26:28,956 --> 00:26:32,055
or here's this sort of
not our characterization,

618
00:26:32,055 --> 00:26:35,501
here's like the characterization
of how this went down.

619
00:26:35,501 --> 00:26:37,563
We tried to not make it emotional

620
00:26:37,563 --> 00:26:41,313
and just sort of say
like here's the history.

621
00:26:44,169 --> 00:26:48,502
- I do think there's a
degree of mischaracterization

622
00:26:51,719 --> 00:26:54,001
from Elon here about one

623
00:26:54,001 --> 00:26:56,077
of the points you just made,

624
00:26:56,077 --> 00:27:00,055
which is the degree of
uncertainty you had at the time.

625
00:27:00,055 --> 00:27:02,197
You guys are a bunch of like a small group

626
00:27:02,197 --> 00:27:05,530
of researchers crazily talking about AGI

627
00:27:06,448 --> 00:27:09,088
when everybody's laughing at that thought.

628
00:27:09,088 --> 00:27:11,363
- Wasn't that long ago
Elon was crazily talking

629
00:27:11,363 --> 00:27:13,398
about launching rockets

630
00:27:13,398 --> 00:27:16,829
when people were laughing at that thought?

631
00:27:16,829 --> 00:27:20,812
So I think he'd have
more empathy for this.

632
00:27:20,812 --> 00:27:25,187
- I mean, I do think that
there's personal stuff here

633
00:27:25,187 --> 00:27:28,020
that there was a split that OpenAI

634
00:27:29,014 --> 00:27:31,757
and a lot of amazing people here chose

635
00:27:31,757 --> 00:27:33,069
to part ways of Elon.

636
00:27:33,069 --> 00:27:33,902
So there's a personal-

637
00:27:33,902 --> 00:27:36,069
- Elon chose to part ways.

638
00:27:37,505 --> 00:27:40,014
- Can you describe that exactly,

639
00:27:40,014 --> 00:27:41,993
the choosing to part ways?

640
00:27:41,993 --> 00:27:44,294
- He thought OpenAI was gonna fail.

641
00:27:44,294 --> 00:27:46,516
He wanted total control
to sort of turn it around.

642
00:27:46,516 --> 00:27:48,038
We wanted to keep going in the direction

643
00:27:48,038 --> 00:27:50,180
that now has become OpenAI.

644
00:27:50,180 --> 00:27:51,373
He also wanted Tesla to be able

645
00:27:51,373 --> 00:27:52,818
to build an AGI effort.

646
00:27:52,818 --> 00:27:53,932
At various times,

647
00:27:53,932 --> 00:27:58,133
he wanted to make OpenAI
into a for-profit company

648
00:27:58,133 --> 00:27:59,256
that he could have control of

649
00:27:59,256 --> 00:28:01,753
or have it merged with Tesla.

650
00:28:01,753 --> 00:28:02,926
We didn't want to do that

651
00:28:02,926 --> 00:28:04,271
and he decided to leave,

652
00:28:04,271 --> 00:28:05,771
which that's fine.

653
00:28:07,195 --> 00:28:08,574
- And that's one of the things

654
00:28:08,574 --> 00:28:12,165
that the blog post says
is that he wanted OpenAI

655
00:28:12,165 --> 00:28:14,915
to be basically acquired by Tesla

656
00:28:17,042 --> 00:28:19,803
in those same way that or
maybe something similar

657
00:28:19,803 --> 00:28:21,188
or maybe something more dramatic

658
00:28:21,188 --> 00:28:23,074
than the partnership with Microsoft.

659
00:28:23,074 --> 00:28:25,094
- My memory is the proposal was just like,

660
00:28:25,094 --> 00:28:26,876
yeah, like get acquired by Tesla

661
00:28:26,876 --> 00:28:28,422
and have Tesla have full control over it.

662
00:28:28,422 --> 00:28:30,022
I'm pretty sure that's what it was.

663
00:28:30,022 --> 00:28:33,439
- So what is the word open in OpenAI mean

664
00:28:35,035 --> 00:28:36,593
to Elon at the time?

665
00:28:36,593 --> 00:28:39,228
Ilya has talked about this
in in the email exchanges

666
00:28:39,228 --> 00:28:40,121
and all this kind of stuff.

667
00:28:40,121 --> 00:28:41,949
What does it mean to you at the time?

668
00:28:41,949 --> 00:28:43,689
What does it mean to you now?

669
00:28:43,689 --> 00:28:44,705
- I would definitely pick a diff...

670
00:28:44,705 --> 00:28:45,874
Speaking of going back with an oracle,

671
00:28:45,874 --> 00:28:48,041
I'd pick a different name.

672
00:28:49,164 --> 00:28:51,481
One of the things that
I think OpenAI is doing

673
00:28:51,481 --> 00:28:53,698
that is the most important of everything

674
00:28:53,698 --> 00:28:57,615
that we're doing is
putting powerful technology

675
00:28:58,554 --> 00:29:03,055
in the hands of people
for free as a public good.

676
00:29:03,055 --> 00:29:05,490
We don't run ads on our free version.

677
00:29:05,490 --> 00:29:08,407
We don't monetize it in other ways.

678
00:29:08,407 --> 00:29:10,532
We just say it's part of our mission.

679
00:29:10,532 --> 00:29:12,256
We wanna put increasingly powerful tools

680
00:29:12,256 --> 00:29:13,832
in the hands of people for free

681
00:29:13,832 --> 00:29:15,322
and get them to use them.

682
00:29:15,322 --> 00:29:19,405
And I think that kind of
open is really important

683
00:29:21,720 --> 00:29:22,553
to our mission.

684
00:29:22,553 --> 00:29:23,886
I think if you give people great tools

685
00:29:23,886 --> 00:29:25,674
and teach them to use them
or don't even teach them,

686
00:29:25,674 --> 00:29:26,722
they'll figure it out

687
00:29:26,722 --> 00:29:28,998
and let them go build an incredible future

688
00:29:28,998 --> 00:29:30,858
for each other with that.

689
00:29:30,858 --> 00:29:31,964
That's a big deal.

690
00:29:31,964 --> 00:29:34,951
So if we can keep putting free or low cost

691
00:29:34,951 --> 00:29:39,344
or free and low cost powerful
AI tools out in the world,

692
00:29:39,344 --> 00:29:40,264
I think that's a huge deal

693
00:29:40,264 --> 00:29:42,847
for how we fulfill the mission.

694
00:29:43,809 --> 00:29:44,681
Open source or not,

695
00:29:44,681 --> 00:29:46,789
yeah, I think we should
open source some stuff

696
00:29:46,789 --> 00:29:48,456
and not other stuff.

697
00:29:49,423 --> 00:29:51,604
It does become this like
religious battle line

698
00:29:51,604 --> 00:29:53,040
where nuance is hard to have,

699
00:29:53,040 --> 00:29:55,388
but I think nuance is the right answer.

700
00:29:55,388 --> 00:29:58,023
- So he said change your name to ClosedAI

701
00:29:58,023 --> 00:29:59,572
and I'll drop the lawsuit.

702
00:29:59,572 --> 00:30:03,573
I mean, is it going to
become this battleground

703
00:30:03,573 --> 00:30:05,955
in the land of memes about the name?

704
00:30:05,955 --> 00:30:10,020
- I think that speaks to the seriousness

705
00:30:10,020 --> 00:30:12,853
with which Elon means the lawsuit.

706
00:30:18,049 --> 00:30:21,443
I mean, that's like an
astonishing thing to say, I think.

707
00:30:21,443 --> 00:30:23,880
- Well, I don't think the lawsuit maybe,

708
00:30:23,880 --> 00:30:25,140
correct me if I'm wrong,

709
00:30:25,140 --> 00:30:28,636
but I don't think the
lawsuit is legally serious.

710
00:30:28,636 --> 00:30:31,675
It's more to make a point
about the future of AGI

711
00:30:31,675 --> 00:30:35,758
and the company that's
currently leading the way.

712
00:30:37,106 --> 00:30:40,522
- Look, I mean Grok had
not open sourced anything

713
00:30:40,522 --> 00:30:42,684
until people pointed out it
was a little bit hypocritical

714
00:30:42,684 --> 00:30:43,517
and then he announced

715
00:30:43,517 --> 00:30:45,842
that Grok open source things this week.

716
00:30:45,842 --> 00:30:47,527
I don't think open source versus not is

717
00:30:47,527 --> 00:30:48,908
what this is really about for him.

718
00:30:48,908 --> 00:30:50,823
- Well, we'll talk about
open source and not.

719
00:30:50,823 --> 00:30:53,803
I do think maybe criticizing
the competition is great,

720
00:30:53,803 --> 00:30:54,934
just talking a little shit,

721
00:30:54,934 --> 00:30:56,267
that's great,

722
00:30:56,267 --> 00:30:57,643
but friendly competition

723
00:30:57,643 --> 00:31:01,046
versus like I personally hate lawsuits.

724
00:31:01,046 --> 00:31:03,154
- Look, I think this whole
thing is like unbecoming

725
00:31:03,154 --> 00:31:04,108
of a builder,

726
00:31:04,108 --> 00:31:09,108
and I respect Elon is one of
the great builders of our time.

727
00:31:09,512 --> 00:31:12,345
And I know he knows what it's like

728
00:31:14,131 --> 00:31:15,869
to have like haters attack him

729
00:31:15,869 --> 00:31:18,114
and it makes me extra
sad he's doing the toss.

730
00:31:18,114 --> 00:31:20,200
- Yeah, he is one of the
greatest builders of all time,

731
00:31:20,200 --> 00:31:22,731
potentially the greatest
builder of all time.

732
00:31:22,731 --> 00:31:24,238
- It makes me sad.

733
00:31:24,238 --> 00:31:25,630
And I think it makes a lot of people sad.

734
00:31:25,630 --> 00:31:27,539
There's a lot of people
who've really looked up to him

735
00:31:27,539 --> 00:31:29,554
for a long time and said this.

736
00:31:29,554 --> 00:31:30,859
I said in some interview

737
00:31:30,859 --> 00:31:32,790
or something that I missed the old Elon

738
00:31:32,790 --> 00:31:34,363
and the number of messages I got being

739
00:31:34,363 --> 00:31:36,886
like that exactly encapsulates how I feel.

740
00:31:36,886 --> 00:31:39,089
- I think he should just win.

741
00:31:39,089 --> 00:31:41,839
He should just make Grok beat GPT

742
00:31:43,447 --> 00:31:45,134
and then GPT beats Grok

743
00:31:45,134 --> 00:31:46,869
and it's just a competition,

744
00:31:46,869 --> 00:31:49,791
and it's beautiful for everybody.

745
00:31:49,791 --> 00:31:51,419
But on the question of open source,

746
00:31:51,419 --> 00:31:53,952
do you think there's a
lot of companies playing

747
00:31:53,952 --> 00:31:54,963
with this idea?

748
00:31:54,963 --> 00:31:56,084
It's quite interesting.

749
00:31:56,084 --> 00:32:00,667
I would say Meta, surprisingly,
has led the way on this

750
00:32:01,657 --> 00:32:05,923
or like at least took the
first step in the game of chess

751
00:32:05,923 --> 00:32:08,191
of really open sourcing the model.

752
00:32:08,191 --> 00:32:10,772
Of course, it's not the
state of the art model,

753
00:32:10,772 --> 00:32:12,705
but open sourcing Llama

754
00:32:12,705 --> 00:32:16,190
and Google is flirting with the idea

755
00:32:16,190 --> 00:32:18,222
of open sourcing a smaller version.

756
00:32:18,222 --> 00:32:20,646
What are the pros and
cons of open sourcing?

757
00:32:20,646 --> 00:32:22,476
Have you played around with this idea?

758
00:32:22,476 --> 00:32:25,017
- Yeah, I think there
is definitely a place

759
00:32:25,017 --> 00:32:26,208
for open source models,

760
00:32:26,208 --> 00:32:27,321
particularly smaller models

761
00:32:27,321 --> 00:32:28,571
that people can run locally,

762
00:32:28,571 --> 00:32:31,156
I think there's huge demand for.

763
00:32:31,156 --> 00:32:33,992
I think there will be
some open source models,

764
00:32:33,992 --> 00:32:36,182
there will be some closed source models.

765
00:32:36,182 --> 00:32:39,397
It won't be unlike other
ecosystems in that way.

766
00:32:39,397 --> 00:32:41,695
- I listened to all in podcasts talking

767
00:32:41,695 --> 00:32:43,837
about this lawsuit and
all that kind of stuff

768
00:32:43,837 --> 00:32:47,009
and they were more concerned
about the precedent

769
00:32:47,009 --> 00:32:50,926
of going from nonprofit
to this cap for profit.

770
00:32:52,026 --> 00:32:55,693
What precedent that
sets for other startups?

771
00:32:56,643 --> 00:32:58,652
- I would heavily discourage any startup

772
00:32:58,652 --> 00:33:01,251
that was thinking about
starting as a non-profit

773
00:33:01,251 --> 00:33:03,236
and adding like a for-profit arm later.

774
00:33:03,236 --> 00:33:04,296
I'd heavily discourage
them from doing that.

775
00:33:04,296 --> 00:33:05,883
I don't think we'll set a precedent here.

776
00:33:05,883 --> 00:33:06,716
- Okay.

777
00:33:06,716 --> 00:33:08,814
So most startups should go just-

778
00:33:08,814 --> 00:33:09,647
- For sure.

779
00:33:09,647 --> 00:33:11,370
And again, if we knew
what was gonna happen,

780
00:33:11,370 --> 00:33:12,406
we would've done that too.

781
00:33:12,406 --> 00:33:13,555
- Well, like in theory,

782
00:33:13,555 --> 00:33:16,655
if you like dance beautifully here,

783
00:33:16,655 --> 00:33:19,490
there's like some tax
incentives or whatever

784
00:33:19,490 --> 00:33:20,350
- But I don't think that's like

785
00:33:20,350 --> 00:33:22,102
how most people think about these things.

786
00:33:22,102 --> 00:33:24,548
- Just not possible to save a lot of money

787
00:33:24,548 --> 00:33:27,015
for a startup if you do it this way.

788
00:33:27,015 --> 00:33:28,983
- No, I think there's
like laws that would make

789
00:33:28,983 --> 00:33:30,382
that pretty difficult.

790
00:33:30,382 --> 00:33:33,531
- Where do you hope this goes with Elon?

791
00:33:33,531 --> 00:33:36,726
Well, this tension, this dance,

792
00:33:36,726 --> 00:33:38,167
what do you hope this?

793
00:33:38,167 --> 00:33:41,679
Like if we go one, two,
three years from now,

794
00:33:41,679 --> 00:33:44,788
your relationship with him
on a personal level too,

795
00:33:44,788 --> 00:33:47,268
like friendship, friendly competition,

796
00:33:47,268 --> 00:33:49,601
just all this kind of stuff.

797
00:33:51,644 --> 00:33:52,477
- Yeah.

798
00:33:52,477 --> 00:33:54,977
I mean, I really respect Elon.

799
00:34:00,884 --> 00:34:02,947
And I hope that years in the future,

800
00:34:02,947 --> 00:34:05,434
we have an amicable relationship.

801
00:34:05,434 --> 00:34:08,267
- Yeah, I hope you guys have
an amicable relationship

802
00:34:08,267 --> 00:34:09,517
like this month

803
00:34:11,050 --> 00:34:13,202
and just compete and win

804
00:34:13,202 --> 00:34:15,952
and explore these ideas together.

805
00:34:17,402 --> 00:34:20,220
I do suppose there's competition
for talent or whatever,

806
00:34:20,220 --> 00:34:23,387
but it should be friendly competition.

807
00:34:25,192 --> 00:34:27,525
Just build, build cool shit.

808
00:34:28,591 --> 00:34:31,151
And Elon is pretty good
at building cool shit,

809
00:34:31,151 --> 00:34:33,106
but so are you.

810
00:34:33,106 --> 00:34:35,189
So speaking of cool shit.

811
00:34:36,950 --> 00:34:39,962
Sora, there's like a million
questions I could ask.

812
00:34:39,962 --> 00:34:41,843
First of all, it's amazing,

813
00:34:41,843 --> 00:34:44,364
it truly is amazing on a product level,

814
00:34:44,364 --> 00:34:46,145
but also just on a philosophical level.

815
00:34:46,145 --> 00:34:50,214
So let me just
technical/philosophical ask.

816
00:34:50,214 --> 00:34:54,214
What do you think it
understands about the world

817
00:34:55,098 --> 00:34:57,692
more or less than GPT-4, for example,

818
00:34:57,692 --> 00:34:59,837
like the world model

819
00:34:59,837 --> 00:35:04,733
when you train on these
patches versus language tokens?

820
00:35:04,733 --> 00:35:09,203
- I think all of these models
understand something more

821
00:35:09,203 --> 00:35:12,785
about the world model than most
of us give them credit for.

822
00:35:12,785 --> 00:35:15,821
And because they're also very clear things

823
00:35:15,821 --> 00:35:18,439
they just don't understand
or don't get right,

824
00:35:18,439 --> 00:35:19,869
it's easy to look at the weaknesses,

825
00:35:19,869 --> 00:35:20,877
see through the veil

826
00:35:20,877 --> 00:35:23,032
and say this is all fake,

827
00:35:23,032 --> 00:35:24,288
but it's not all fake.

828
00:35:24,288 --> 00:35:28,341
It's just some of it works
and some of it doesn't work.

829
00:35:28,341 --> 00:35:30,279
I remember when I started
first watching Sora videos

830
00:35:30,279 --> 00:35:32,291
and I would see like a person walk

831
00:35:32,291 --> 00:35:34,631
in front of something for a
few seconds and occlude it

832
00:35:34,631 --> 00:35:35,464
and then walk away

833
00:35:35,464 --> 00:35:36,724
and the same thing was still there.

834
00:35:36,724 --> 00:35:39,108
I was like, "This is pretty good."

835
00:35:39,108 --> 00:35:40,341
Or there's examples

836
00:35:40,341 --> 00:35:43,566
where the underlying physics
looks so well represented

837
00:35:43,566 --> 00:35:46,562
over a lot of steps in a sequence.

838
00:35:46,562 --> 00:35:49,305
It's like oh this is
like quite impressive.

839
00:35:49,305 --> 00:35:52,744
But, fundamentally, these
models are just getting better

840
00:35:52,744 --> 00:35:54,460
and that will keep happening.

841
00:35:54,460 --> 00:35:55,976
If you look at the trajectory

842
00:35:55,976 --> 00:35:59,341
from DALL·E 1 to 2 to 3 to Sora,

843
00:35:59,341 --> 00:36:01,849
there were a lot of people that
were dunked on each version,

844
00:36:01,849 --> 00:36:03,747
saying it can't do this, it can't do that

845
00:36:03,747 --> 00:36:05,732
and I'm like look at it now.

846
00:36:05,732 --> 00:36:08,131
- Well, the thing you
just mentioned is kind of

847
00:36:08,131 --> 00:36:12,461
with the occlusions is
basically modeling the physics

848
00:36:12,461 --> 00:36:15,524
of three dimensional physics
of the world sufficiently well

849
00:36:15,524 --> 00:36:17,182
to capture those kinds of things.

850
00:36:17,182 --> 00:36:18,015
- Well.

851
00:36:19,555 --> 00:36:21,083
- Yeah, maybe you can tell me

852
00:36:21,083 --> 00:36:22,847
in order to deal with occlusions,

853
00:36:22,847 --> 00:36:24,684
what does the world model need to?

854
00:36:24,684 --> 00:36:26,608
- Yeah, so what I would
say is it's doing something

855
00:36:26,608 --> 00:36:28,251
to deal with occlusions really well.

856
00:36:28,251 --> 00:36:30,179
What I represent that it
has like a great underlying

857
00:36:30,179 --> 00:36:32,068
3D model of the world.

858
00:36:32,068 --> 00:36:33,565
It's a little bit more of a stretch

859
00:36:33,565 --> 00:36:35,805
- But can you get there
through just these kinds

860
00:36:35,805 --> 00:36:38,942
of two dimensional
training data approaches?

861
00:36:38,942 --> 00:36:41,925
- It looks like this approach
is gonna go surprisingly far.

862
00:36:41,925 --> 00:36:43,690
I don't wanna speculate too much

863
00:36:43,690 --> 00:36:46,025
about what limits it will
surmount and which it won't.

864
00:36:46,025 --> 00:36:47,894
- What are some interesting limitations

865
00:36:47,894 --> 00:36:50,115
of the system that you've seen?

866
00:36:50,115 --> 00:36:52,106
I mean, there's been some
fun ones you've posted.

867
00:36:52,106 --> 00:36:53,145
- There's all kinds of fun.

868
00:36:53,145 --> 00:36:56,170
I mean, like cats sprouting a extra limit

869
00:36:56,170 --> 00:36:58,199
at random points in a video,

870
00:36:58,199 --> 00:36:59,974
like pick what you want,

871
00:36:59,974 --> 00:37:01,076
but there's still a lot of problem,

872
00:37:01,076 --> 00:37:02,291
there's a lot of weaknesses.

873
00:37:02,291 --> 00:37:06,201
- Do you think that's a
fundamental flaw of the approach

874
00:37:06,201 --> 00:37:08,368
or is it just bigger model

875
00:37:09,966 --> 00:37:13,549
or better technical
details or better data,

876
00:37:14,469 --> 00:37:18,284
more data is going to solve
the cat sprouting extremes?

877
00:37:18,284 --> 00:37:20,057
- I would say yes to both.

878
00:37:20,057 --> 00:37:22,245
I think there is something
about the approach

879
00:37:22,245 --> 00:37:24,363
which just seems to feel different

880
00:37:24,363 --> 00:37:28,021
from how we think and learn and whatever.

881
00:37:28,021 --> 00:37:28,941
And then also,

882
00:37:28,941 --> 00:37:30,607
I think it'll get better with scale.

883
00:37:30,607 --> 00:37:33,298
- I mentioned LLMs have
tokens, text tokens

884
00:37:33,298 --> 00:37:35,397
and Sora has visual patches

885
00:37:35,397 --> 00:37:37,162
so it converts all visual data,

886
00:37:37,162 --> 00:37:38,927
a diverse kinds of visual data videos

887
00:37:38,927 --> 00:37:40,960
and images into patches.

888
00:37:40,960 --> 00:37:41,881
Is the training

889
00:37:41,881 --> 00:37:44,575
to the degree you can say
fully self-supervised there?

890
00:37:44,575 --> 00:37:46,592
Is there some manual labeling going on?

891
00:37:46,592 --> 00:37:49,825
What's the involvement
of humans in all this?

892
00:37:49,825 --> 00:37:51,616
- I mean, without saying anything specific

893
00:37:51,616 --> 00:37:53,227
about the Sora approach,

894
00:37:53,227 --> 00:37:56,394
we use lots of human data in our work.

895
00:38:00,849 --> 00:38:04,105
- But not internet scale data.

896
00:38:04,105 --> 00:38:05,535
So lots of humans,

897
00:38:05,535 --> 00:38:08,528
lots of complicated word, Sam.

898
00:38:08,528 --> 00:38:11,540
- I think lots is a
fair word in this case.

899
00:38:11,540 --> 00:38:12,461
- But it doesn't

900
00:38:12,461 --> 00:38:13,777
because to me, lots,

901
00:38:13,777 --> 00:38:15,254
like listen, I'm an introvert

902
00:38:15,254 --> 00:38:16,807
and when I hang out
with like three people,

903
00:38:16,807 --> 00:38:17,670
that's a lot of people.

904
00:38:17,670 --> 00:38:18,503
Yeah, four people,

905
00:38:18,503 --> 00:38:19,669
that's a lot.

906
00:38:19,669 --> 00:38:21,710
But I suppose you mean more than-

907
00:38:21,710 --> 00:38:23,616
- More than three people
work on labeling the data

908
00:38:23,616 --> 00:38:24,449
for these models, yeah.

909
00:38:24,449 --> 00:38:25,282
- Okay.

910
00:38:25,282 --> 00:38:26,115
All right.

911
00:38:26,115 --> 00:38:28,050
But fundamentally, there's a lot

912
00:38:28,050 --> 00:38:30,333
of self-supervised learning.

913
00:38:30,333 --> 00:38:32,206
'cause what you mentioned

914
00:38:32,206 --> 00:38:36,043
in the technical report
is internet scale data.

915
00:38:36,043 --> 00:38:36,941
That's another beautiful,

916
00:38:36,941 --> 00:38:39,000
it's like poetry.

917
00:38:39,000 --> 00:38:42,306
So it's a lot of data
that's not human label.

918
00:38:42,306 --> 00:38:45,065
It's self-supervised in that way.

919
00:38:45,065 --> 00:38:46,301
And then the question is,

920
00:38:46,301 --> 00:38:49,323
how much data is there on the internet

921
00:38:49,323 --> 00:38:52,914
that could be used in
this that is conducive

922
00:38:52,914 --> 00:38:54,401
to this kind of self-supervised way

923
00:38:54,401 --> 00:38:57,702
if only we knew the details
of the self-supervised?

924
00:38:57,702 --> 00:39:00,398
Do you have you considered opening it up

925
00:39:00,398 --> 00:39:02,574
a little more details

926
00:39:02,574 --> 00:39:03,407
- We have.

927
00:39:03,407 --> 00:39:04,506
You mean, for sora specifically?

928
00:39:04,506 --> 00:39:05,505
- Sora specifically

929
00:39:05,505 --> 00:39:07,838
because it's so interesting.

930
00:39:11,144 --> 00:39:12,340
Can the same magic

931
00:39:12,340 --> 00:39:15,890
of LLMs now start moving
towards visual data

932
00:39:15,890 --> 00:39:18,618
and what does that take to do that?

933
00:39:18,618 --> 00:39:20,815
- I mean, it looks to me like yes,

934
00:39:20,815 --> 00:39:22,141
but we have more work to do.

935
00:39:22,141 --> 00:39:22,974
- Sure.

936
00:39:22,974 --> 00:39:23,908
What are the dangers?

937
00:39:23,908 --> 00:39:27,675
Why are you concerned
about releasing the system?

938
00:39:27,675 --> 00:39:29,496
What are some possible dangers of this?

939
00:39:29,496 --> 00:39:30,520
- I mean, frankly speaking,

940
00:39:30,520 --> 00:39:31,766
one thing we have to do

941
00:39:31,766 --> 00:39:35,066
before releasing the
system is just like get it

942
00:39:35,066 --> 00:39:37,733
to work at a level of efficiency

943
00:39:38,868 --> 00:39:41,202
that will deliver the scale
people are gonna want from this.

944
00:39:41,202 --> 00:39:43,995
So that I don't wanna like downplay that

945
00:39:43,995 --> 00:39:47,365
and there's still a ton
of work to do there.

946
00:39:47,365 --> 00:39:49,948
But you can imagine like issues

947
00:39:52,501 --> 00:39:55,084
with deep fakes, misinformation.

948
00:39:56,666 --> 00:39:58,407
We try to be a thoughtful company

949
00:39:58,407 --> 00:40:00,502
about what we put out into the world

950
00:40:00,502 --> 00:40:02,533
and it doesn't take much thought

951
00:40:02,533 --> 00:40:05,206
to think about the ways this can go badly.

952
00:40:05,206 --> 00:40:08,036
- There's a lot of tough questions here.

953
00:40:08,036 --> 00:40:09,928
You're dealing in a very tough space.

954
00:40:09,928 --> 00:40:12,430
Do you think training AI should be

955
00:40:12,430 --> 00:40:14,949
or is fair use under copyright law?

956
00:40:14,949 --> 00:40:17,271
- I think the question
behind that question is,

957
00:40:17,271 --> 00:40:20,210
do people who create valuable
data deserve to have some way

958
00:40:20,210 --> 00:40:22,866
that they get compensated for use of it?

959
00:40:22,866 --> 00:40:25,999
And that I think the answer is yes.

960
00:40:25,999 --> 00:40:27,936
I don't know yet what the answer is.

961
00:40:27,936 --> 00:40:29,915
People have proposed a
lot of different things.

962
00:40:29,915 --> 00:40:31,938
We've some tried some different models.

963
00:40:31,938 --> 00:40:35,188
But if I'm like an artist, for example,

964
00:40:37,076 --> 00:40:38,857
I would like to be able to opt out

965
00:40:38,857 --> 00:40:41,440
of people generating art in my style

966
00:40:41,440 --> 00:40:43,597
and B, if they do
generate art in my style,

967
00:40:43,597 --> 00:40:46,703
I'd like to have some economic
model associated with that.

968
00:40:46,703 --> 00:40:51,092
- Yeah, it's that transition
from CDs to Napster to Spotify.

969
00:40:51,092 --> 00:40:52,698
We have to figure out some kind of model.

970
00:40:52,698 --> 00:40:53,644
- The model changes,

971
00:40:53,644 --> 00:40:55,826
but people have gotta get paid.

972
00:40:55,826 --> 00:40:57,567
- Well, there should be some kind

973
00:40:57,567 --> 00:41:00,808
of incentive if we zoom
out even more for humans

974
00:41:00,808 --> 00:41:02,227
to keep doing cool shit.

975
00:41:02,227 --> 00:41:03,614
- Everything I worry about,

976
00:41:03,614 --> 00:41:04,972
humans are gonna do cool shit

977
00:41:04,972 --> 00:41:08,670
and society's gonna find
some way to reward it.

978
00:41:08,670 --> 00:41:10,286
That seems pretty hardwired.

979
00:41:10,286 --> 00:41:11,322
We want to create.

980
00:41:11,322 --> 00:41:12,521
We want to be useful.

981
00:41:12,521 --> 00:41:15,878
We want to achieve status in whatever way

982
00:41:15,878 --> 00:41:16,822
that's not going anywhere,

983
00:41:16,822 --> 00:41:17,655
I don't think.

984
00:41:17,655 --> 00:41:20,924
- But the reward might not
be monetary, financial.

985
00:41:20,924 --> 00:41:25,127
It might be like fame and
celebration of other cool-

986
00:41:25,127 --> 00:41:27,192
- Maybe financial in some other way.

987
00:41:27,192 --> 00:41:29,345
Again, I don't think we've
seen like the last evolution

988
00:41:29,345 --> 00:41:31,251
of how the economic system's gonna work.

989
00:41:31,251 --> 00:41:32,084
- Yeah.

990
00:41:32,084 --> 00:41:34,212
But artists and creators are worried.

991
00:41:34,212 --> 00:41:35,257
When they see Sora,

992
00:41:35,257 --> 00:41:36,597
they're like, "Holy shit."

993
00:41:36,597 --> 00:41:37,430
- Sure.

994
00:41:37,430 --> 00:41:40,787
Artists were also super worried
when photography came out.

995
00:41:40,787 --> 00:41:42,678
And then photography became a new art form

996
00:41:42,678 --> 00:41:45,928
and people made a lot of
money taking pictures.

997
00:41:45,928 --> 00:41:47,608
And I think things like
that will keep happening.

998
00:41:47,608 --> 00:41:50,330
People will use the new tools in new ways.

999
00:41:50,330 --> 00:41:53,339
- If we just look on YouTube
or something like this,

1000
00:41:53,339 --> 00:41:56,026
how much of that will be using Sora,

1001
00:41:56,026 --> 00:41:59,780
like AI-generated content do you think

1002
00:41:59,780 --> 00:42:01,613
in the next five years?

1003
00:42:01,613 --> 00:42:03,826
- People talk about like
how many jobs is AI gonna do

1004
00:42:03,826 --> 00:42:05,187
in five years

1005
00:42:05,187 --> 00:42:08,602
and the framework that people
have is what percentage

1006
00:42:08,602 --> 00:42:10,436
of current jobs are just
gonna be totally replaced

1007
00:42:10,436 --> 00:42:12,519
by some AI doing the job?

1008
00:42:13,480 --> 00:42:15,531
The way I think about
it is not what percent

1009
00:42:15,531 --> 00:42:16,485
of jobs AI will do,

1010
00:42:16,485 --> 00:42:18,156
but what percent of tasks will AI do

1011
00:42:18,156 --> 00:42:19,772
and over what time horizon.

1012
00:42:19,772 --> 00:42:22,164
So if you think of all of
the like-five second tasks

1013
00:42:22,164 --> 00:42:25,283
in the economy, five-minute
tasks, the five-hour tasks,

1014
00:42:25,283 --> 00:42:27,579
maybe even the five-day tasks,

1015
00:42:27,579 --> 00:42:29,805
how many of those can AI do?

1016
00:42:29,805 --> 00:42:33,140
And I think that's a way
more interesting, impactful,

1017
00:42:33,140 --> 00:42:37,699
important question than
how many jobs AI can do

1018
00:42:37,699 --> 00:42:39,494
because it is a tool

1019
00:42:39,494 --> 00:42:42,454
that will work at increasing
levels of sophistication

1020
00:42:42,454 --> 00:42:45,526
and over longer and longer time horizons

1021
00:42:45,526 --> 00:42:46,774
for more and more tasks

1022
00:42:46,774 --> 00:42:49,734
and let people operate at a
higher level of abstraction.

1023
00:42:49,734 --> 00:42:53,078
So maybe people are way more
efficient at the job they do.

1024
00:42:53,078 --> 00:42:54,198
And at some point,

1025
00:42:54,198 --> 00:42:55,971
that's not just a quantitative change,

1026
00:42:55,971 --> 00:42:58,408
but it's a qualitative
one too about the kinds

1027
00:42:58,408 --> 00:43:00,470
of problems you can keep in your head.

1028
00:43:00,470 --> 00:43:02,814
I think that for videos on YouTube,

1029
00:43:02,814 --> 00:43:04,100
it'll be the same.

1030
00:43:04,100 --> 00:43:04,947
Many videos,

1031
00:43:04,947 --> 00:43:06,230
maybe most of them,

1032
00:43:06,230 --> 00:43:08,504
will use AI tools in the production,

1033
00:43:08,504 --> 00:43:10,664
but they'll still be fundamentally driven

1034
00:43:10,664 --> 00:43:12,701
by a person thinking about it,

1035
00:43:12,701 --> 00:43:13,893
putting it together,

1036
00:43:13,893 --> 00:43:15,618
doing parts of it,

1037
00:43:15,618 --> 00:43:18,101
sort of directing it and running it.

1038
00:43:18,101 --> 00:43:19,340
- Yeah, it's so interesting.

1039
00:43:19,340 --> 00:43:20,173
I mean, it's scary,

1040
00:43:20,173 --> 00:43:22,310
but it's interesting to think about.

1041
00:43:22,310 --> 00:43:26,020
I tend to believe that humans
like to watch other humans

1042
00:43:26,020 --> 00:43:26,853
or other human like-

1043
00:43:26,853 --> 00:43:29,628
- Humans really care
about other humans a lot.

1044
00:43:29,628 --> 00:43:30,461
- Yeah.

1045
00:43:30,461 --> 00:43:34,752
If there's a cooler thing
that's better than a human,

1046
00:43:34,752 --> 00:43:37,770
humans care about that for like two days

1047
00:43:37,770 --> 00:43:39,233
and then they go back to humans.

1048
00:43:39,233 --> 00:43:41,747
- That seems very deeply wired.

1049
00:43:41,747 --> 00:43:44,284
- It's the whole chest thing.

1050
00:43:44,284 --> 00:43:46,978
But now let's everybody keep playing chess

1051
00:43:46,978 --> 00:43:49,151
and let's ignore the alpha in the room

1052
00:43:49,151 --> 00:43:52,261
that humans are really bad at
chess relative to AI systems.

1053
00:43:52,261 --> 00:43:54,309
- We still run races and
cars are much faster.

1054
00:43:54,309 --> 00:43:56,121
I mean, there's like a lot of examples.

1055
00:43:56,121 --> 00:43:56,954
- Yeah.

1056
00:43:56,954 --> 00:43:59,366
And maybe it'll just be tooling

1057
00:43:59,366 --> 00:44:02,268
like in the Adobe suite type of way

1058
00:44:02,268 --> 00:44:04,118
where it can just make videos much easier

1059
00:44:04,118 --> 00:44:06,368
and all that kind of stuff.

1060
00:44:07,645 --> 00:44:09,517
Listen, I hate being
in front of the camera.

1061
00:44:09,517 --> 00:44:10,959
If I can figure out a way

1062
00:44:10,959 --> 00:44:12,351
to not be in front of the camera,

1063
00:44:12,351 --> 00:44:13,184
I would love it.

1064
00:44:13,184 --> 00:44:14,454
Unfortunately, it'll take a while.

1065
00:44:14,454 --> 00:44:17,100
Like that generating faces,

1066
00:44:17,100 --> 00:44:18,325
it's getting there,

1067
00:44:18,325 --> 00:44:20,754
but generating faces and
video format is tricky

1068
00:44:20,754 --> 00:44:24,402
when it's specific people
versus generic people.

1069
00:44:24,402 --> 00:44:27,170
Let me ask you about GPT-4.

1070
00:44:27,170 --> 00:44:28,958
There's so many questions.

1071
00:44:28,958 --> 00:44:31,208
First of all, also amazing.

1072
00:44:33,825 --> 00:44:35,846
Looking back, it'll probably be this kind

1073
00:44:35,846 --> 00:44:37,455
of historic pivotal moment

1074
00:44:37,455 --> 00:44:40,077
with three, five, and four which had GPT.

1075
00:44:40,077 --> 00:44:41,173
- Maybe five will be the pivotal moment.

1076
00:44:41,173 --> 00:44:42,256
I don't know.

1077
00:44:43,143 --> 00:44:44,697
Hard to say that looking forwards.

1078
00:44:44,697 --> 00:44:45,530
- We never know.

1079
00:44:45,530 --> 00:44:46,927
That's the annoying
thing about the future,

1080
00:44:46,927 --> 00:44:48,202
it's hard to predict.

1081
00:44:48,202 --> 00:44:49,035
But for me,

1082
00:44:49,035 --> 00:44:50,824
looking back GPT-4,

1083
00:44:50,824 --> 00:44:53,048
ChatGPT is pretty impressive,

1084
00:44:53,048 --> 00:44:55,100
historically impressive.

1085
00:44:55,100 --> 00:44:57,381
So allow me to ask,

1086
00:44:57,381 --> 00:45:00,453
what's been the most impressive
capabilities of GPT-4 to you

1087
00:45:00,453 --> 00:45:01,786
and GPT-4 Turbo?

1088
00:45:05,978 --> 00:45:07,768
- I think it kind of sucks.

1089
00:45:07,768 --> 00:45:08,601
- Hmm.

1090
00:45:08,601 --> 00:45:11,721
Typical human also gotten
used to an awesome thing.

1091
00:45:11,721 --> 00:45:14,020
- No, I think it is an amazing thing,

1092
00:45:14,020 --> 00:45:17,270
but relative to where we need to get to

1093
00:45:18,888 --> 00:45:21,805
and where I believe we will get to,

1094
00:45:22,953 --> 00:45:25,741
at the time of like GPT-3,

1095
00:45:25,741 --> 00:45:27,341
people were like, "Oh this is amazing.

1096
00:45:27,341 --> 00:45:29,071
This is this like marvel of technology,"

1097
00:45:29,071 --> 00:45:30,571
and it is, it was.

1098
00:45:31,541 --> 00:45:34,791
But now we have GPT-4 and look at GPT-3

1099
00:45:35,827 --> 00:45:39,278
and you're like that's
unimaginably horrible.

1100
00:45:39,278 --> 00:45:41,121
I expect that the delta between five

1101
00:45:41,121 --> 00:45:44,668
and four will be the same
as between four and three.

1102
00:45:44,668 --> 00:45:48,704
And I think it is our job to
live a few years in the future

1103
00:45:48,704 --> 00:45:52,960
and remember that the tools
we have now are gonna kind

1104
00:45:52,960 --> 00:45:55,196
of suck looking backwards at them

1105
00:45:55,196 --> 00:45:59,544
and that's how we make
sure the future is better.

1106
00:45:59,544 --> 00:46:04,287
- What are the most glorious
ways that GPT-4 sucks?

1107
00:46:04,287 --> 00:46:05,120
Meaning-

1108
00:46:05,120 --> 00:46:06,811
- [Sam] What are the
best things it can do?

1109
00:46:06,811 --> 00:46:09,500
- What are the best things
it can do in the limits

1110
00:46:09,500 --> 00:46:12,653
of those best things that
allow you to say it sucks,

1111
00:46:12,653 --> 00:46:16,513
therefore gives you an inspiration
and hope for the future?

1112
00:46:16,513 --> 00:46:17,637
- One thing I've been using it

1113
00:46:17,637 --> 00:46:22,637
for more recently is sort of a
like a brainstorming partner.

1114
00:46:23,680 --> 00:46:24,513
- [Lex] Yep.

1115
00:46:24,513 --> 00:46:25,726
Almost for that.

1116
00:46:25,726 --> 00:46:30,125
- There's a glimmer of
something amazing in there.

1117
00:46:30,125 --> 00:46:31,602
I don't think it gets...

1118
00:46:31,602 --> 00:46:33,023
When people talk about it,

1119
00:46:33,023 --> 00:46:33,856
what it does,

1120
00:46:33,856 --> 00:46:36,237
they're like, "Helps me
code more productively.

1121
00:46:36,237 --> 00:46:38,906
It helps me write more faster and better.

1122
00:46:38,906 --> 00:46:42,001
It helps me translate from
this language to another,"

1123
00:46:42,001 --> 00:46:43,664
all these like amazing things.

1124
00:46:43,664 --> 00:46:46,664
But there's something about the kind

1125
00:46:50,059 --> 00:46:52,469
of creative brainstorming partner.

1126
00:46:52,469 --> 00:46:53,820
I need to come up with
a name for this thing.

1127
00:46:53,820 --> 00:46:55,700
I need to think about this
problem in a different way.

1128
00:46:55,700 --> 00:46:58,851
I'm not sure what to do here.

1129
00:46:58,851 --> 00:47:00,362
That I think like gives a glimpse

1130
00:47:00,362 --> 00:47:03,268
of something I hope to see more of.

1131
00:47:03,268 --> 00:47:04,431
One of the other things

1132
00:47:04,431 --> 00:47:08,014
that you can see a very
small glimpse of is

1133
00:47:10,054 --> 00:47:12,974
what I can help on longer horizon tasks.

1134
00:47:12,974 --> 00:47:14,892
Break down something in multiple steps,

1135
00:47:14,892 --> 00:47:16,889
maybe execute some of those steps,

1136
00:47:16,889 --> 00:47:18,714
search the internet, write code, whatever.

1137
00:47:18,714 --> 00:47:20,541
Put that together.

1138
00:47:20,541 --> 00:47:21,374
When that works,

1139
00:47:21,374 --> 00:47:22,207
which is not very often,

1140
00:47:22,207 --> 00:47:24,344
it's like very magical,

1141
00:47:24,344 --> 00:47:27,349
- The iterative back
and forth with a human.

1142
00:47:27,349 --> 00:47:28,505
It works a lot for me.

1143
00:47:28,505 --> 00:47:29,705
What do you mean it works?

1144
00:47:29,705 --> 00:47:30,570
- Iterative back and forth to human,

1145
00:47:30,570 --> 00:47:31,403
it can get more often,

1146
00:47:31,403 --> 00:47:34,307
when it can go do like a
10-step problem on its own.

1147
00:47:34,307 --> 00:47:37,087
It doesn't work for that
too often sometimes.

1148
00:47:37,087 --> 00:47:39,271
- Add multiple layers of abstraction

1149
00:47:39,271 --> 00:47:40,886
or do you mean just sequential?

1150
00:47:40,886 --> 00:47:43,147
- Both like to break it down

1151
00:47:43,147 --> 00:47:45,041
and then do things that different layers

1152
00:47:45,041 --> 00:47:47,346
of abstraction put them together.

1153
00:47:47,346 --> 00:47:52,096
Look, I don't wanna downplay
the accomplishment of GPT-4,

1154
00:47:53,360 --> 00:47:55,435
but I don't wanna overstate it either.

1155
00:47:55,435 --> 00:47:58,504
And I think this point that we
are on an exponential curve,

1156
00:47:58,504 --> 00:48:01,327
we'll look back relatively soon at GPT-4

1157
00:48:01,327 --> 00:48:03,889
like we look back at GPT-3 now.

1158
00:48:03,889 --> 00:48:08,490
- That said, I mean
ChatGPT was the transition

1159
00:48:08,490 --> 00:48:10,690
to where people like started

1160
00:48:10,690 --> 00:48:14,781
to believe there is an
uptick of believing.

1161
00:48:14,781 --> 00:48:16,935
Not internally at OpenAI perhaps.

1162
00:48:16,935 --> 00:48:18,182
There's believers here,

1163
00:48:18,182 --> 00:48:19,342
but when you think-

1164
00:48:19,342 --> 00:48:20,354
- And in that sense,

1165
00:48:20,354 --> 00:48:21,250
I do think it'll be a moment

1166
00:48:21,250 --> 00:48:22,513
where a lot of the world went

1167
00:48:22,513 --> 00:48:25,292
from not believing to believing.

1168
00:48:25,292 --> 00:48:28,121
That was more about the ChatGPT interface.

1169
00:48:28,121 --> 00:48:30,711
And by the interface and product,

1170
00:48:30,711 --> 00:48:32,588
I also mean the post-training of the model

1171
00:48:32,588 --> 00:48:34,704
and how we tune it to be helpful to you

1172
00:48:34,704 --> 00:48:38,278
and how to use it than the
underlying model itself.

1173
00:48:38,278 --> 00:48:42,361
- How much of each of
those things are important?

1174
00:48:43,674 --> 00:48:46,424
The underlying model and the RLHF

1175
00:48:47,785 --> 00:48:50,575
or something of that nature that tunes it

1176
00:48:50,575 --> 00:48:52,684
to be more compelling to the human,

1177
00:48:52,684 --> 00:48:55,505
more effective and
productive for the human.

1178
00:48:55,505 --> 00:48:57,989
- I mean, they're both super important

1179
00:48:57,989 --> 00:49:00,903
but the RLHF, the post-training step,

1180
00:49:00,903 --> 00:49:02,911
the little wrapper of things

1181
00:49:02,911 --> 00:49:05,134
that from a compute perspective,

1182
00:49:05,134 --> 00:49:06,502
little wrapper of things that we do on top

1183
00:49:06,502 --> 00:49:07,335
of the base model,

1184
00:49:07,335 --> 00:49:08,848
even though it's a huge amount of work.

1185
00:49:08,848 --> 00:49:10,865
That's really important to
say nothing of the product

1186
00:49:10,865 --> 00:49:12,865
that we build around it.

1187
00:49:16,036 --> 00:49:17,726
In some sense,

1188
00:49:17,726 --> 00:49:19,089
we did have to do two things.

1189
00:49:19,089 --> 00:49:22,406
We had to invent we underlying technology

1190
00:49:22,406 --> 00:49:24,823
and then we had to figure out

1191
00:49:27,516 --> 00:49:30,673
how to make it into a
product people would love,

1192
00:49:30,673 --> 00:49:32,949
which is not just about the
actual product work itself,

1193
00:49:32,949 --> 00:49:34,934
but this whole other step

1194
00:49:34,934 --> 00:49:37,474
of how you align it and make it useful

1195
00:49:37,474 --> 00:49:40,216
- And how you make the scale work

1196
00:49:40,216 --> 00:49:42,232
where a lot of people can
use it at the same time,

1197
00:49:42,232 --> 00:49:44,127
all that kind of stuff.

1198
00:49:44,127 --> 00:49:47,390
- But that was like a
known difficult thing.

1199
00:49:47,390 --> 00:49:49,028
We knew we were gonna have to scale it up.

1200
00:49:49,028 --> 00:49:50,580
We had to go do two things

1201
00:49:50,580 --> 00:49:52,935
that had like never been done before

1202
00:49:52,935 --> 00:49:54,070
that were both, like,

1203
00:49:54,070 --> 00:49:55,905
I would say quite significant achievements

1204
00:49:55,905 --> 00:49:58,460
and then a lot of things
like scaling it up

1205
00:49:58,460 --> 00:50:01,361
that other companies
have had to do before.

1206
00:50:01,361 --> 00:50:03,770
- How does the context window

1207
00:50:03,770 --> 00:50:07,020
of going from 8K to 128K tokens compare

1208
00:50:08,377 --> 00:50:10,794
from GPT-4 to to GPT-4 Turbo?

1209
00:50:13,211 --> 00:50:15,955
- Most people don't
need all the way to 128,

1210
00:50:15,955 --> 00:50:18,075
most of the time although.

1211
00:50:18,075 --> 00:50:19,925
If we dream into the distant future,

1212
00:50:19,925 --> 00:50:21,591
we'll have like way distant future,

1213
00:50:21,591 --> 00:50:24,470
we'll have like context
length of several billion.

1214
00:50:24,470 --> 00:50:26,127
You will feed in all of your information,

1215
00:50:26,127 --> 00:50:27,624
all of your history time,

1216
00:50:27,624 --> 00:50:30,405
and it'll just get to
know you better and better

1217
00:50:30,405 --> 00:50:32,224
and that'll be great.

1218
00:50:32,224 --> 00:50:33,424
For now,

1219
00:50:33,424 --> 00:50:34,519
the way people use these models,

1220
00:50:34,519 --> 00:50:35,950
they're not doing that.

1221
00:50:35,950 --> 00:50:39,171
And people sometimes post in a paper

1222
00:50:39,171 --> 00:50:43,921
or a significant fraction of
a code repository, whatever.

1223
00:50:45,761 --> 00:50:46,804
But most usage

1224
00:50:46,804 --> 00:50:48,619
of the models is not
using the long context

1225
00:50:48,619 --> 00:50:50,114
most of the time.

1226
00:50:50,114 --> 00:50:54,201
- I like that this is your
I have a dream speech.

1227
00:50:54,201 --> 00:50:56,966
One day, you'll be judged
by the full context

1228
00:50:56,966 --> 00:51:00,395
of your character or
of your whole lifetime.

1229
00:51:00,395 --> 00:51:01,271
That's interesting.

1230
00:51:01,271 --> 00:51:02,776
So like that's part of the expansion

1231
00:51:02,776 --> 00:51:06,676
that you're hoping for is a
greater and greater context.

1232
00:51:06,676 --> 00:51:07,905
- I saw this internet clip once.

1233
00:51:07,905 --> 00:51:09,047
I'm gonna get the numbers wrong,

1234
00:51:09,047 --> 00:51:10,983
but it was like Bill Gates
talking about the amount

1235
00:51:10,983 --> 00:51:13,664
of memory on some early computer.

1236
00:51:13,664 --> 00:51:14,705
Maybe it was 64K,

1237
00:51:14,705 --> 00:51:16,310
maybe 640K, something like that.

1238
00:51:16,310 --> 00:51:19,374
And most of it was used
for the screen buffer.

1239
00:51:19,374 --> 00:51:22,212
And he just couldn't seem genuine.

1240
00:51:22,212 --> 00:51:23,788
This couldn't imagine

1241
00:51:23,788 --> 00:51:26,828
that the world would eventually
need gigabytes of memory

1242
00:51:26,828 --> 00:51:31,804
in a computer or terabytes
of memory in a computer.

1243
00:51:31,804 --> 00:51:33,221
And you always do

1244
00:51:34,605 --> 00:51:35,850
or you always do just need

1245
00:51:35,850 --> 00:51:38,581
to follow the exponential of technology.

1246
00:51:38,581 --> 00:51:41,125
We will find out how to
use better technology.

1247
00:51:41,125 --> 00:51:43,559
So I can't really imagine
what it's like right now

1248
00:51:43,559 --> 00:51:46,456
for context links to go
out to the billion someday

1249
00:51:46,456 --> 00:51:47,785
and they might not literally go there,

1250
00:51:47,785 --> 00:51:51,623
but effectively it'll feel like that.

1251
00:51:51,623 --> 00:51:53,887
But I know we'll use it

1252
00:51:53,887 --> 00:51:56,220
and really not wanna go
back once we have it.

1253
00:51:56,220 --> 00:51:58,827
- Yeah, even saying billions 10 years

1254
00:51:58,827 --> 00:52:00,256
from now might seem dumb

1255
00:52:00,256 --> 00:52:04,011
because it'll be like
trillions upon trillions.

1256
00:52:04,011 --> 00:52:04,934
- [Sam] Sure.

1257
00:52:04,934 --> 00:52:07,990
- There'd be some kind of breakthrough

1258
00:52:07,990 --> 00:52:10,454
that will effectively feel
like infinite context.

1259
00:52:10,454 --> 00:52:11,495
But even 120,

1260
00:52:11,495 --> 00:52:12,737
I have to be honest,

1261
00:52:12,737 --> 00:52:14,575
I haven't pushed it to that degree.

1262
00:52:14,575 --> 00:52:16,454
Maybe putting in entire books

1263
00:52:16,454 --> 00:52:20,700
or like parts of books and so on, papers.

1264
00:52:20,700 --> 00:52:22,139
What are some interesting use cases

1265
00:52:22,139 --> 00:52:23,590
of GPT-4 that you've seen?

1266
00:52:23,590 --> 00:52:26,160
- The thing that I find
most interesting is not any

1267
00:52:26,160 --> 00:52:27,804
particular use case that
we can talk about those,

1268
00:52:27,804 --> 00:52:31,159
but it's people who kind of like...

1269
00:52:31,159 --> 00:52:32,816
This is mostly younger people,

1270
00:52:32,816 --> 00:52:36,757
but people who use it as
like their default start

1271
00:52:36,757 --> 00:52:39,004
for any kind of knowledge work task.

1272
00:52:39,004 --> 00:52:40,484
And it's the fact that it can do a lot

1273
00:52:40,484 --> 00:52:41,977
of things reasonably well.

1274
00:52:41,977 --> 00:52:42,885
You can use GPTV,

1275
00:52:42,885 --> 00:52:44,339
you can use it to help you write code,

1276
00:52:44,339 --> 00:52:45,760
you can use it to help you do search,

1277
00:52:45,760 --> 00:52:48,193
you can use it to edit a paper.

1278
00:52:48,193 --> 00:52:49,648
The most interesting to me is the people

1279
00:52:49,648 --> 00:52:52,367
who just use it as the
start of their workflow.

1280
00:52:52,367 --> 00:52:54,038
- I do as well for many things.

1281
00:52:54,038 --> 00:52:58,455
Like I use it as a reading
partner for reading books.

1282
00:52:59,366 --> 00:53:00,601
It helps me think,

1283
00:53:00,601 --> 00:53:01,782
help me think through ideas,

1284
00:53:01,782 --> 00:53:03,056
especially when the books are classic,

1285
00:53:03,056 --> 00:53:04,472
so it's really well written about

1286
00:53:04,472 --> 00:53:06,222
and it actually is...

1287
00:53:08,011 --> 00:53:10,335
I find it often to be significantly better

1288
00:53:10,335 --> 00:53:13,290
than even like Wikipedia
on well-covered topics.

1289
00:53:13,290 --> 00:53:16,449
It's somehow more
balanced and more nuanced,

1290
00:53:16,449 --> 00:53:17,791
or maybe it's me,

1291
00:53:17,791 --> 00:53:19,053
but it inspires me

1292
00:53:19,053 --> 00:53:21,007
to think deeper than a
Wikipedia article does.

1293
00:53:21,007 --> 00:53:22,616
I'm not exactly sure what that is.

1294
00:53:22,616 --> 00:53:24,079
You mentioned like this collaboration,

1295
00:53:24,079 --> 00:53:25,345
I'm not sure where magic is,

1296
00:53:25,345 --> 00:53:26,722
if it's in here or if it's in there

1297
00:53:26,722 --> 00:53:29,112
or if it's somewhere in between.

1298
00:53:29,112 --> 00:53:30,973
I'm not sure.

1299
00:53:30,973 --> 00:53:33,063
But one of the things that concerns me

1300
00:53:33,063 --> 00:53:36,437
for knowledge task when
I start with GPT is

1301
00:53:36,437 --> 00:53:39,715
I'll usually have to
do fact checking after,

1302
00:53:39,715 --> 00:53:43,775
like check that it didn't
come up with fake stuff.

1303
00:53:43,775 --> 00:53:46,351
How do you figure that out

1304
00:53:46,351 --> 00:53:49,351
that GPT can come up with fake stuff

1305
00:53:51,011 --> 00:53:53,101
that sounds really convincing?

1306
00:53:53,101 --> 00:53:55,492
So how do you ground it in truth?

1307
00:53:55,492 --> 00:53:58,830
- That's obviously an area
of intense interest for us.

1308
00:53:58,830 --> 00:54:03,532
I think it's gonna get a lot
better with upcoming versions,

1309
00:54:03,532 --> 00:54:04,828
but we'll have to continue to work on it

1310
00:54:04,828 --> 00:54:07,161
and we're not gonna have it
like all solved this year.

1311
00:54:07,161 --> 00:54:10,365
- Well, the scary thing
is like as it gets better.

1312
00:54:10,365 --> 00:54:12,933
You'll start not doing the
fact checking more and more,

1313
00:54:12,933 --> 00:54:13,766
right?

1314
00:54:14,941 --> 00:54:16,134
- I'm of two minds about that.

1315
00:54:16,134 --> 00:54:18,209
I think people are like much
more sophisticated users

1316
00:54:18,209 --> 00:54:20,923
of technology than we
often give them credit for

1317
00:54:20,923 --> 00:54:23,808
and people seem to really
understand that GPT,

1318
00:54:23,808 --> 00:54:25,724
any of these models
hallucinate some of the time

1319
00:54:25,724 --> 00:54:26,631
and if it's mission critical,

1320
00:54:26,631 --> 00:54:27,875
you gotta check it.

1321
00:54:27,875 --> 00:54:29,766
- Except journalists don't
seem to understand that.

1322
00:54:29,766 --> 00:54:34,384
I've seen journalists
half-assedly just using GPT-4.

1323
00:54:34,384 --> 00:54:36,160
- Of the long list of things I'd like

1324
00:54:36,160 --> 00:54:37,464
to dunk on journalists for,

1325
00:54:37,464 --> 00:54:40,317
this is not my top criticism of them.

1326
00:54:40,317 --> 00:54:43,757
- Well, I think the bigger
criticism is perhaps

1327
00:54:43,757 --> 00:54:45,056
the pressures and the incentives

1328
00:54:45,056 --> 00:54:47,040
of being a journalist is that you have

1329
00:54:47,040 --> 00:54:49,700
to work really quickly
and this is a shortcut.

1330
00:54:49,700 --> 00:54:53,659
I would love our society
to incentivize like-

1331
00:54:53,659 --> 00:54:56,014
- [Sam] I would too.

1332
00:54:56,014 --> 00:54:59,201
- Journalistic efforts
that take days and weeks

1333
00:54:59,201 --> 00:55:02,270
and rewards great in depth journalism.

1334
00:55:02,270 --> 00:55:05,635
Also journalism that represent
stuff in a balanced way

1335
00:55:05,635 --> 00:55:07,660
where it's like celebrates people

1336
00:55:07,660 --> 00:55:08,877
while criticizing them

1337
00:55:08,877 --> 00:55:11,310
even though the criticism is
the thing that gets clicks

1338
00:55:11,310 --> 00:55:13,719
and making up also gets clicks

1339
00:55:13,719 --> 00:55:16,412
and headlines that
mischaracterize completely.

1340
00:55:16,412 --> 00:55:19,379
I'm sure you have a lot
of people dunking on...

1341
00:55:19,379 --> 00:55:21,514
Well, all that drama
probably got a lot of clicks.

1342
00:55:21,514 --> 00:55:22,764
- Probably did.

1343
00:55:24,716 --> 00:55:29,204
- And that's a bigger problem
about human civilization.

1344
00:55:29,204 --> 00:55:32,533
I would love to see solved is
where we celebrate a bit more.

1345
00:55:32,533 --> 00:55:34,841
You've given ChatGPT the
ability to have memories.

1346
00:55:34,841 --> 00:55:39,108
You've been playing with that
about previous conversations

1347
00:55:39,108 --> 00:55:41,265
and also the ability to turn off memory,

1348
00:55:41,265 --> 00:55:43,048
which I wish I could do that sometimes,

1349
00:55:43,048 --> 00:55:45,501
just turn on and off depending.

1350
00:55:45,501 --> 00:55:47,596
I guess sometimes alcohol can do that,

1351
00:55:47,596 --> 00:55:50,013
but not optimally, I suppose.

1352
00:55:51,301 --> 00:55:52,809
What have you seen through that,

1353
00:55:52,809 --> 00:55:54,711
like playing around with that idea

1354
00:55:54,711 --> 00:55:56,642
of remembering conversations and not?

1355
00:55:56,642 --> 00:55:58,828
- We're very early in
our explorations here,

1356
00:55:58,828 --> 00:56:00,608
but I think what people want

1357
00:56:00,608 --> 00:56:04,687
or at least what I want
for myself is a model

1358
00:56:04,687 --> 00:56:09,687
that gets to know me and gets
more useful to me over time.

1359
00:56:09,818 --> 00:56:12,929
This is an early exploration.

1360
00:56:12,929 --> 00:56:15,189
I think there's like a
lot of other things to do,

1361
00:56:15,189 --> 00:56:16,756
but that's where we'd like to head.

1362
00:56:16,756 --> 00:56:18,278
You'd like to use a model

1363
00:56:18,278 --> 00:56:19,633
and over the course of your life

1364
00:56:19,633 --> 00:56:20,466
or use a system,

1365
00:56:20,466 --> 00:56:21,616
it'd be many models.

1366
00:56:21,616 --> 00:56:23,137
And over the course of your life,

1367
00:56:23,137 --> 00:56:25,885
it gets better and better.

1368
00:56:25,885 --> 00:56:26,718
- Yeah.

1369
00:56:26,718 --> 00:56:27,551
How hard is that problem?

1370
00:56:27,551 --> 00:56:28,384
'Cause right now,

1371
00:56:28,384 --> 00:56:31,037
it's more like remembering little factoids

1372
00:56:31,037 --> 00:56:33,068
and preferences and so on.

1373
00:56:33,068 --> 00:56:33,901
What about remembering,

1374
00:56:33,901 --> 00:56:35,647
like don't you want GPT

1375
00:56:35,647 --> 00:56:39,086
to remember all the shit
you went through in November

1376
00:56:39,086 --> 00:56:40,240
and all the drama and then you can-

1377
00:56:40,240 --> 00:56:41,073
- Yeah, yeah, yeah.

1378
00:56:41,073 --> 00:56:41,906
- Because right now,

1379
00:56:41,906 --> 00:56:43,372
you're clearly blocking
it out a little bit.

1380
00:56:43,372 --> 00:56:45,594
- It's not just that I
want it to remember that.

1381
00:56:45,594 --> 00:56:49,337
I want it to integrate the lessons of that

1382
00:56:49,337 --> 00:56:53,504
and remind me in the future
what to do differently

1383
00:56:56,442 --> 00:56:58,622
or what to watch out for.

1384
00:56:58,622 --> 00:57:01,205
And we all gain from experience

1385
00:57:02,045 --> 00:57:05,753
over the course of our
lives, varying degrees.

1386
00:57:05,753 --> 00:57:10,420
And I'd like my AI agent to
gain with that experience too.

1387
00:57:10,420 --> 00:57:11,574
So if we go back

1388
00:57:11,574 --> 00:57:12,933
and let ourselves imagine

1389
00:57:12,933 --> 00:57:16,850
that trillions and
trillions of contact length,

1390
00:57:17,980 --> 00:57:19,616
if I can put every
conversation I've ever had

1391
00:57:19,616 --> 00:57:21,146
with anybody in my life in there,

1392
00:57:21,146 --> 00:57:23,654
if I can have all of
my emails input out...

1393
00:57:23,654 --> 00:57:24,923
Like all of my input/output

1394
00:57:24,923 --> 00:57:27,303
in the context window every
time I ask a question,

1395
00:57:27,303 --> 00:57:29,567
that'd be pretty cool, I think.

1396
00:57:29,567 --> 00:57:31,717
- Yeah, I think that would be very cool.

1397
00:57:31,717 --> 00:57:33,116
People sometimes will hear that

1398
00:57:33,116 --> 00:57:35,317
and be concerned about privacy.

1399
00:57:35,317 --> 00:57:39,193
What do you think about that aspect of it,

1400
00:57:39,193 --> 00:57:41,269
the more effective the AI becomes

1401
00:57:41,269 --> 00:57:44,825
that really integrating
all the experiences

1402
00:57:44,825 --> 00:57:46,827
and all the data that happened to you

1403
00:57:46,827 --> 00:57:48,476
and give you advice?

1404
00:57:48,476 --> 00:57:50,480
- I think the right answer
there is just user choice.

1405
00:57:50,480 --> 00:57:53,045
Anything I want stricken from
the record from my AI agent,

1406
00:57:53,045 --> 00:57:54,366
I wanna be able to take out.

1407
00:57:54,366 --> 00:57:55,266
If I don't want to remember anything,

1408
00:57:55,266 --> 00:57:56,599
I want that too.

1409
00:57:57,873 --> 00:58:00,542
You and I may have different opinions

1410
00:58:00,542 --> 00:58:03,468
about where on that
privacy utility trade off

1411
00:58:03,468 --> 00:58:04,475
for our own AI we wanna be,

1412
00:58:04,475 --> 00:58:06,095
which is totally fine.

1413
00:58:06,095 --> 00:58:08,532
But I think the answer is just
like really easy user choice.

1414
00:58:08,532 --> 00:58:11,815
- But there should be some high level

1415
00:58:11,815 --> 00:58:14,214
of transparency from a
company about the user choice

1416
00:58:14,214 --> 00:58:15,513
'cause sometimes company in the past,

1417
00:58:15,513 --> 00:58:17,364
companies in the past have been kind

1418
00:58:17,364 --> 00:58:20,723
of absolutely shady about like, yeah,

1419
00:58:20,723 --> 00:58:23,572
it's kind of presumed that
we're collecting all your data

1420
00:58:23,572 --> 00:58:25,480
and we're using it for a good reason

1421
00:58:25,480 --> 00:58:27,116
for advertisement and so on.

1422
00:58:27,116 --> 00:58:31,188
But there's not a transparency
about the details of that.

1423
00:58:31,188 --> 00:58:32,227
- That's totally true.

1424
00:58:32,227 --> 00:58:33,060
You mentioned earlier

1425
00:58:33,060 --> 00:58:35,069
that I'm like blocking
out the November stuff.

1426
00:58:35,069 --> 00:58:36,144
- I'm just teasing you.

1427
00:58:36,144 --> 00:58:40,477
- Well, I mean I think it
was a very traumatic thing

1428
00:58:41,534 --> 00:58:44,277
and it did immobilize me
for a long period of time.

1429
00:58:44,277 --> 00:58:46,610
Like definitely the hardest,

1430
00:58:47,663 --> 00:58:49,070
like the hardest work thing I've had

1431
00:58:49,070 --> 00:58:52,501
to do was just like
keep working that period

1432
00:58:52,501 --> 00:58:54,047
because I had to try to come back in here

1433
00:58:54,047 --> 00:58:55,189
and put the pieces together

1434
00:58:55,189 --> 00:58:59,189
while I was just like in
sort of shock and pain.

1435
00:59:00,067 --> 00:59:01,291
Nobody really cares about that.

1436
00:59:01,291 --> 00:59:02,722
I mean, the team gave me a pass

1437
00:59:02,722 --> 00:59:04,566
and I was not working at my normal level,

1438
00:59:04,566 --> 00:59:07,305
but there was a period
where I was just, like,

1439
00:59:07,305 --> 00:59:08,962
it was really hard to have to do both.

1440
00:59:08,962 --> 00:59:11,004
But I kind of woke up one morning

1441
00:59:11,004 --> 00:59:13,652
and I was like, "This was a
horrible thing to happen to me.

1442
00:59:13,652 --> 00:59:16,568
I think I could just feel
like a victim forever,"

1443
00:59:16,568 --> 00:59:18,501
or I can say, "This is like
the most important work

1444
00:59:18,501 --> 00:59:19,625
I'll ever touch in my life

1445
00:59:19,625 --> 00:59:21,148
and I need to get back to it."

1446
00:59:21,148 --> 00:59:23,717
And it doesn't mean that I've repressed it

1447
00:59:23,717 --> 00:59:26,758
because sometimes I wake in the middle

1448
00:59:26,758 --> 00:59:28,468
of the night thinking about it,

1449
00:59:28,468 --> 00:59:32,498
but I do feel like an obligation
to keep moving forward.

1450
00:59:32,498 --> 00:59:33,925
- Well, that's beautifully said,

1451
00:59:33,925 --> 00:59:36,217
but there could be some
lingering stuff in there.

1452
00:59:36,217 --> 00:59:39,050
What I would be concerned about is

1453
00:59:40,848 --> 00:59:42,946
that trust thing that you mentioned

1454
00:59:42,946 --> 00:59:45,533
that being paranoid about people

1455
00:59:45,533 --> 00:59:48,512
as opposed to just trusting
everybody or most people,

1456
00:59:48,512 --> 00:59:49,828
like using your gut.

1457
00:59:49,828 --> 00:59:51,527
It's a tricky dance for sure.

1458
00:59:51,527 --> 00:59:56,110
I mean, because I've seen in
my part-time explorations,

1459
00:59:58,399 --> 01:00:03,149
I've been diving deeply into
the Zelensky administration,

1460
01:00:04,047 --> 01:00:07,937
the Putin administration and
the dynamics there in wartime

1461
01:00:07,937 --> 01:00:10,358
in a very highly stressful environment.

1462
01:00:10,358 --> 01:00:12,901
And what happens is distrust

1463
01:00:12,901 --> 01:00:15,331
and you isolate yourself both

1464
01:00:15,331 --> 01:00:18,822
and you start to not
see the world clearly.

1465
01:00:18,822 --> 01:00:19,660
And that's a concern,

1466
01:00:19,660 --> 01:00:20,915
that's a human concern.

1467
01:00:20,915 --> 01:00:22,265
You seem to have taken it in stride

1468
01:00:22,265 --> 01:00:24,978
and kind of learned the good
lessons and felt the love

1469
01:00:24,978 --> 01:00:27,554
and let the love energize you,

1470
01:00:27,554 --> 01:00:28,387
which is great,

1471
01:00:28,387 --> 01:00:30,325
but still can linger in there.

1472
01:00:30,325 --> 01:00:32,677
There's just some questions I would love

1473
01:00:32,677 --> 01:00:37,510
to ask your intuition about
what's GPT able to do and not.

1474
01:00:38,534 --> 01:00:42,849
So it's allocating approximately
the same amount of compute

1475
01:00:42,849 --> 01:00:44,576
for each token it generates.

1476
01:00:44,576 --> 01:00:48,994
Is there room there in
this kind of approach

1477
01:00:48,994 --> 01:00:51,710
to slower thinking, sequential thinking?

1478
01:00:51,710 --> 01:00:53,325
- I think there will be a new paradigm

1479
01:00:53,325 --> 01:00:55,911
for that kind of thinking.

1480
01:00:55,911 --> 01:00:58,593
- Will it be similar like architecturally

1481
01:00:58,593 --> 01:01:00,326
as what we're seeing now with LLMs?

1482
01:01:00,326 --> 01:01:03,076
Is it a layer on top of the LLMs?

1483
01:01:04,216 --> 01:01:06,890
- I can imagine many
ways to implement that.

1484
01:01:06,890 --> 01:01:08,399
I think that's less important

1485
01:01:08,399 --> 01:01:10,293
than the question you were getting out,

1486
01:01:10,293 --> 01:01:14,136
which is do we need a
way to do a slower kind

1487
01:01:14,136 --> 01:01:18,803
of thinking where the answer
doesn't have to get like...

1488
01:01:21,365 --> 01:01:22,203
I guess like spiritually,

1489
01:01:22,203 --> 01:01:23,675
you could say that you want an AI

1490
01:01:23,675 --> 01:01:26,490
to be able to think harder
about a harder problem

1491
01:01:26,490 --> 01:01:28,580
and answer more quickly
about an easier problem.

1492
01:01:28,580 --> 01:01:30,301
And I think that will be important.

1493
01:01:30,301 --> 01:01:32,138
- Is that like a human thought
that we're just having,

1494
01:01:32,138 --> 01:01:33,407
you should be able to think hard?

1495
01:01:33,407 --> 01:01:34,968
Is that a wrong intuition?

1496
01:01:34,968 --> 01:01:37,075
- I suspect that's a reasonable intuition.

1497
01:01:37,075 --> 01:01:37,908
- Interesting.

1498
01:01:37,908 --> 01:01:38,741
So it's not possible

1499
01:01:38,741 --> 01:01:42,017
once the GPT gets like GPT-7 would

1500
01:01:42,017 --> 01:01:45,267
just be instantaneously be able to see,

1501
01:01:46,400 --> 01:01:49,059
here's the proof of from RSTM.

1502
01:01:49,059 --> 01:01:51,503
- It seems to me like you want to be able

1503
01:01:51,503 --> 01:01:55,170
to allocate more compute
to harder problems.

1504
01:01:56,543 --> 01:01:59,543
It seems to me that a system knowing

1505
01:02:02,690 --> 01:02:04,881
if you ask a system like that,

1506
01:02:04,881 --> 01:02:07,881
proof from us last theorem versus...

1507
01:02:09,006 --> 01:02:10,673
What's today's date?

1508
01:02:11,701 --> 01:02:14,114
Unless it already knew and
had memorized the answer

1509
01:02:14,114 --> 01:02:15,936
to the proof,

1510
01:02:15,936 --> 01:02:18,438
assuming it's gotta go figure that out,

1511
01:02:18,438 --> 01:02:20,615
seems like that will take more compute.

1512
01:02:20,615 --> 01:02:23,735
- But can it look like a
basically LLM talking to itself,

1513
01:02:23,735 --> 01:02:25,346
that kind of thing?

1514
01:02:25,346 --> 01:02:26,179
- Maybe.

1515
01:02:26,179 --> 01:02:27,773
I mean, there's a lot of things that

1516
01:02:27,773 --> 01:02:31,106
you could imagine working what the right

1517
01:02:32,078 --> 01:02:35,283
or the best way to do that will be.

1518
01:02:35,283 --> 01:02:36,450
We don't know.

1519
01:02:37,574 --> 01:02:40,975
- This does make me
think of the mysterious,

1520
01:02:40,975 --> 01:02:43,678
the lore behind Q-Star.

1521
01:02:43,678 --> 01:02:45,831
What's this mysterious Q-Star project?

1522
01:02:45,831 --> 01:02:49,164
Is it also in the same nuclear facility?

1523
01:02:50,127 --> 01:02:52,719
- There is no nuclear facility.

1524
01:02:52,719 --> 01:02:53,737
- That's what a person

1525
01:02:53,737 --> 01:02:54,951
with a nuclear facility always says.

1526
01:02:54,951 --> 01:02:57,780
- I would love to have a
secret nuclear facility.

1527
01:02:57,780 --> 01:02:59,542
There isn't one.

1528
01:02:59,542 --> 01:03:00,642
- All right.

1529
01:03:00,642 --> 01:03:01,475
- Maybe someday.

1530
01:03:01,475 --> 01:03:02,308
- Someday.

1531
01:03:02,308 --> 01:03:03,141
All right.

1532
01:03:04,783 --> 01:03:05,746
One can dream-

1533
01:03:05,746 --> 01:03:07,740
- OpenAI is not a good
company to keeping secrets.

1534
01:03:07,740 --> 01:03:08,573
It would be nice.

1535
01:03:08,573 --> 01:03:10,365
We're like been plagued by a lot of leaks

1536
01:03:10,365 --> 01:03:12,632
and it would be nice if we were able

1537
01:03:12,632 --> 01:03:14,155
to have something like that.

1538
01:03:14,155 --> 01:03:15,802
- Can you speak to what Q-Star is?

1539
01:03:15,802 --> 01:03:17,870
- We are not ready to talk about that.

1540
01:03:17,870 --> 01:03:19,553
- See, but an answer like
that means there's something

1541
01:03:19,553 --> 01:03:20,632
to talk about.

1542
01:03:20,632 --> 01:03:22,873
It's very mysterious, Sam.

1543
01:03:22,873 --> 01:03:26,456
- I mean, we work on
all kinds of research.

1544
01:03:28,834 --> 01:03:30,571
We have said for a while

1545
01:03:30,571 --> 01:03:33,071
that we think better reasoning

1546
01:03:34,848 --> 01:03:38,142
in these systems is an important direction

1547
01:03:38,142 --> 01:03:41,045
that we'd like to pursue.

1548
01:03:41,045 --> 01:03:43,712
We haven't cracked the code yet.

1549
01:03:44,803 --> 01:03:47,136
We're very interested in it.

1550
01:03:48,024 --> 01:03:50,961
- Is there gonna be moments Q-Star

1551
01:03:50,961 --> 01:03:54,024
or otherwise where there's
going to be leaps similar

1552
01:03:54,024 --> 01:03:56,528
to GPT where you're like-

1553
01:03:56,528 --> 01:03:59,444
- That's a good question.

1554
01:03:59,444 --> 01:04:01,694
What do I think about that?

1555
01:04:05,521 --> 01:04:08,544
It's interesting to me it
all feels pretty continuous.

1556
01:04:08,544 --> 01:04:09,888
- This is kind of a theme

1557
01:04:09,888 --> 01:04:12,993
that you're saying is you're
basically gradually going up

1558
01:04:12,993 --> 01:04:15,066
an exponential slope.

1559
01:04:15,066 --> 01:04:17,713
But from an outsider's perspective for me,

1560
01:04:17,713 --> 01:04:20,710
just watching it that it
does feel like there's leaps,

1561
01:04:20,710 --> 01:04:22,072
but to you there isn't.

1562
01:04:22,072 --> 01:04:24,720
- I do wonder if we should have...

1563
01:04:24,720 --> 01:04:25,963
So part of the reason

1564
01:04:25,963 --> 01:04:29,039
that we deploy the way
we do is that we think,

1565
01:04:29,039 --> 01:04:31,706
we call it iterative deployment.

1566
01:04:33,499 --> 01:04:34,926
Rather than go build in secret

1567
01:04:34,926 --> 01:04:36,259
until we got all the way to GPT-5,

1568
01:04:36,259 --> 01:04:39,958
we decided to talk
about GPT 1, 2, 3 and 4.

1569
01:04:39,958 --> 01:04:41,850
And part of the reason
there is, I think, AI

1570
01:04:41,850 --> 01:04:43,996
and surprise don't go together.

1571
01:04:43,996 --> 01:04:46,479
And also the world, people, institutions,

1572
01:04:46,479 --> 01:04:47,596
whatever you wanna call it,

1573
01:04:47,596 --> 01:04:50,603
need time to adapt and
think about these things.

1574
01:04:50,603 --> 01:04:52,684
And I think one of the best things

1575
01:04:52,684 --> 01:04:55,016
that OpenAI has done is this strategy

1576
01:04:55,016 --> 01:04:58,739
and we get the world to pay
attention to the progress

1577
01:04:58,739 --> 01:05:00,257
to take AGI seriously

1578
01:05:00,257 --> 01:05:04,204
to think about what
systems, and structures,

1579
01:05:04,204 --> 01:05:06,326
and governance we want in place before,

1580
01:05:06,326 --> 01:05:07,232
we're like under the gun

1581
01:05:07,232 --> 01:05:08,499
and have to make a rush decision.

1582
01:05:08,499 --> 01:05:09,796
I think that's really good.

1583
01:05:09,796 --> 01:05:11,627
But the fact that people like you

1584
01:05:11,627 --> 01:05:15,200
and others say you still feel like

1585
01:05:15,200 --> 01:05:17,940
there are these leaps makes me think

1586
01:05:17,940 --> 01:05:20,473
that maybe we should
be doing our releasing

1587
01:05:20,473 --> 01:05:22,066
even more iteratively.

1588
01:05:22,066 --> 01:05:22,899
I don't know what that would mean.

1589
01:05:22,899 --> 01:05:24,770
I don't have an answer ready to go.

1590
01:05:24,770 --> 01:05:28,896
But our goal is not to have
shock updates to the world.

1591
01:05:28,896 --> 01:05:29,765
The opposite.

1592
01:05:29,765 --> 01:05:30,645
- Yeah, for sure.

1593
01:05:30,645 --> 01:05:32,544
More iterative would be amazing.

1594
01:05:32,544 --> 01:05:34,647
I think that's just
beautiful for everybody.

1595
01:05:34,647 --> 01:05:35,922
- But that's what we're trying to do.

1596
01:05:35,922 --> 01:05:37,585
That's like our state of the strategy

1597
01:05:37,585 --> 01:05:39,674
and I think we're
somehow missing the mark.

1598
01:05:39,674 --> 01:05:41,906
So maybe we should think
about releasing GPT-5

1599
01:05:41,906 --> 01:05:44,276
in a different way or something like that.

1600
01:05:44,276 --> 01:05:45,859
- Yeah, 4.71, 4.72.

1601
01:05:47,764 --> 01:05:49,406
But people tend to like to celebrate,

1602
01:05:49,406 --> 01:05:50,730
people celebrate birthdays.

1603
01:05:50,730 --> 01:05:52,942
I don't know if you know humans,

1604
01:05:52,942 --> 01:05:54,664
but they kind of have these milestones.

1605
01:05:54,664 --> 01:05:56,170
- I do know some humans.

1606
01:05:56,170 --> 01:05:58,337
People do like milestones.

1607
01:05:59,328 --> 01:06:00,911
I totally get that.

1608
01:06:02,710 --> 01:06:05,353
I think we like milestones too.

1609
01:06:05,353 --> 01:06:06,869
It's like fun to say,

1610
01:06:06,869 --> 01:06:08,078
declare victory on this one

1611
01:06:08,078 --> 01:06:09,653
and go start the next thing.

1612
01:06:09,653 --> 01:06:11,813
But, yeah, I feel like
we're somehow getting

1613
01:06:11,813 --> 01:06:13,094
this a little bit wrong.

1614
01:06:13,094 --> 01:06:15,701
- So when is GPT-5 coming out again?

1615
01:06:15,701 --> 01:06:16,576
- I don't know.

1616
01:06:16,576 --> 01:06:18,163
That's an honest answer.

1617
01:06:18,163 --> 01:06:20,574
- Oh, that's the honest answer.

1618
01:06:20,574 --> 01:06:23,574
Is it blink twice if it's this year?

1619
01:06:29,839 --> 01:06:34,232
- We will release an
amazing model this year.

1620
01:06:34,232 --> 01:06:36,237
I don't know what we'll call it.

1621
01:06:36,237 --> 01:06:38,422
- So that goes to the question of like,

1622
01:06:38,422 --> 01:06:41,835
what's the way we release this thing?

1623
01:06:41,835 --> 01:06:43,344
- We'll release,

1624
01:06:43,344 --> 01:06:45,841
over in the coming months,

1625
01:06:45,841 --> 01:06:47,907
many different things.

1626
01:06:47,907 --> 01:06:49,541
I think they'll be very cool.

1627
01:06:49,541 --> 01:06:51,016
I think before we talk

1628
01:06:51,016 --> 01:06:53,774
about like a GPT-5 like model called that

1629
01:06:53,774 --> 01:06:54,997
or called or not called that

1630
01:06:54,997 --> 01:06:55,840
or a little bit worse

1631
01:06:55,840 --> 01:06:57,685
or a little bit better
than what you'd expect

1632
01:06:57,685 --> 01:06:58,601
from a GPT-5,

1633
01:06:58,601 --> 01:07:00,844
I know we have a lot of
other important things

1634
01:07:00,844 --> 01:07:02,837
to release first.

1635
01:07:02,837 --> 01:07:06,470
- I don't know what to expect from GPT-5.

1636
01:07:06,470 --> 01:07:08,862
You're making me nervous and excited.

1637
01:07:08,862 --> 01:07:10,935
What are some of the biggest
challenges in bottlenecks

1638
01:07:10,935 --> 01:07:14,502
to overcome for whatever
it ends up being called,

1639
01:07:14,502 --> 01:07:16,487
but let's call it GPT-5?

1640
01:07:16,487 --> 01:07:18,665
Just interesting to ask,

1641
01:07:18,665 --> 01:07:20,240
is it on the compute side?

1642
01:07:20,240 --> 01:07:21,162
Is it in the technical side?

1643
01:07:21,162 --> 01:07:23,581
- It's always all of these.

1644
01:07:23,581 --> 01:07:24,900
What's the one big unlock?

1645
01:07:24,900 --> 01:07:26,234
Is it a bigger computer?

1646
01:07:26,234 --> 01:07:27,606
Is it like a new secret?

1647
01:07:27,606 --> 01:07:30,105
Is it something else?

1648
01:07:30,105 --> 01:07:31,664
It's all of these things together.

1649
01:07:31,664 --> 01:07:36,106
The thing that OpenAI I
think does really well,

1650
01:07:36,106 --> 01:07:37,889
this is actually an original Ilya quote

1651
01:07:37,889 --> 01:07:38,876
that I'm gonna butcher,

1652
01:07:38,876 --> 01:07:43,209
but it's something like we
multiply 200 medium-sized

1653
01:07:45,041 --> 01:07:47,412
things together into one giant thing.

1654
01:07:47,412 --> 01:07:50,606
- So there's this distributed
constant innovation happening.

1655
01:07:50,606 --> 01:07:51,780
- [Sam] Yeah.

1656
01:07:51,780 --> 01:07:53,302
- So even on the technical side,

1657
01:07:53,302 --> 01:07:54,135
like-

1658
01:07:54,135 --> 01:07:55,399
- Especially on the technical side.

1659
01:07:55,399 --> 01:07:57,009
- So like even like detailed approaches,

1660
01:07:57,009 --> 01:07:59,909
like detailed aspects of every...

1661
01:07:59,909 --> 01:08:02,965
How does that work with different
disparate teams and so on?

1662
01:08:02,965 --> 01:08:05,759
How do the medium-sized things become

1663
01:08:05,759 --> 01:08:07,130
one whole giant transformer?

1664
01:08:07,130 --> 01:08:08,189
How does this-

1665
01:08:08,189 --> 01:08:09,686
- There's a few people who have

1666
01:08:09,686 --> 01:08:11,683
to think about putting
the whole thing together,

1667
01:08:11,683 --> 01:08:12,752
but a lot of people try

1668
01:08:12,752 --> 01:08:14,433
to keep most of the picture in their head.

1669
01:08:14,433 --> 01:08:15,708
- Oh, like the individual teams,

1670
01:08:15,708 --> 01:08:16,791
individual contributors tried

1671
01:08:16,791 --> 01:08:17,950
to keep a big picture-
- At a high level.

1672
01:08:17,950 --> 01:08:19,163
Yeah, you don't know exactly

1673
01:08:19,163 --> 01:08:20,676
how every piece works, of course.

1674
01:08:20,676 --> 01:08:24,241
But one thing I generally believe is

1675
01:08:24,241 --> 01:08:26,075
that it's sometimes useful to zoom out

1676
01:08:26,075 --> 01:08:28,610
and look at the entire map.

1677
01:08:28,610 --> 01:08:33,578
And I think this is true for
like a technical problem.

1678
01:08:33,578 --> 01:08:38,578
I think this is true for
like innovating in business.

1679
01:08:38,825 --> 01:08:41,252
But things come together
in surprising ways

1680
01:08:41,252 --> 01:08:44,903
and having an understanding
of that whole picture.

1681
01:08:44,903 --> 01:08:46,190
Even if most of the time,

1682
01:08:46,190 --> 01:08:48,991
you're operating in the weeds in one area,

1683
01:08:48,991 --> 01:08:51,389
pays off with surprising insights.

1684
01:08:51,389 --> 01:08:54,952
In fact, one of the
things that I used to have

1685
01:08:54,952 --> 01:08:56,892
and I think was super valuable was I used

1686
01:08:56,892 --> 01:09:01,420
to have like a good map
of all of the frontier

1687
01:09:01,420 --> 01:09:04,440
or most of the frontiers
in the tech industry.

1688
01:09:04,440 --> 01:09:06,218
And I could sometimes
see these connections

1689
01:09:06,218 --> 01:09:08,319
or new things that were possible

1690
01:09:08,319 --> 01:09:10,559
that if I were only deep in one area,

1691
01:09:10,559 --> 01:09:13,372
I wouldn't be able to have the idea for

1692
01:09:13,372 --> 01:09:15,283
because I wouldn't have all the data

1693
01:09:15,283 --> 01:09:17,223
and I don't really have that much anymore.

1694
01:09:17,223 --> 01:09:19,223
I'm like super deep now.

1695
01:09:21,102 --> 01:09:23,295
But I know that it's a valuable thing.

1696
01:09:23,295 --> 01:09:25,345
- You're not the man you used to be, Sam.

1697
01:09:25,345 --> 01:09:28,137
- Very different job now
than what I used to have.

1698
01:09:28,137 --> 01:09:29,909
- Speaking of zooming out,

1699
01:09:29,909 --> 01:09:33,159
let's zoom out to another cheeky thing,

1700
01:09:34,320 --> 01:09:37,667
but profound thing perhaps that you said.

1701
01:09:37,667 --> 01:09:41,467
You tweeted about needing $7 trillion.

1702
01:09:41,467 --> 01:09:42,930
- I did not tweet about that.

1703
01:09:42,930 --> 01:09:44,923
I never said like we're
raising $7 trillion

1704
01:09:44,923 --> 01:09:45,756
or blah blah blah.

1705
01:09:45,756 --> 01:09:46,589
- Oh, that's somebody else.

1706
01:09:46,589 --> 01:09:47,422
- [Sam] Yeah.

1707
01:09:47,422 --> 01:09:48,872
- Oh, but you said it,

1708
01:09:48,872 --> 01:09:50,472
"Fuck it, maybe eight," I think.

1709
01:09:50,472 --> 01:09:52,735
- Okay. I meme like once
there's like misinformation out

1710
01:09:52,735 --> 01:09:53,568
in the world.

1711
01:09:53,568 --> 01:09:54,736
- Oh, you meme.

1712
01:09:54,736 --> 01:09:58,672
But sort of misinformation
may have a foundation

1713
01:09:58,672 --> 01:10:00,089
of insight there.

1714
01:10:00,939 --> 01:10:03,360
- Look, I think compute
is gonna be the currency

1715
01:10:03,360 --> 01:10:04,193
of the future.

1716
01:10:04,193 --> 01:10:06,714
I think it will be maybe
the most precious commodity

1717
01:10:06,714 --> 01:10:07,797
in the world.

1718
01:10:08,748 --> 01:10:11,098
And I think we should be investing heavily

1719
01:10:11,098 --> 01:10:13,298
to make a lot more compute.

1720
01:10:13,298 --> 01:10:14,381
Compute is...

1721
01:10:19,873 --> 01:10:22,678
I think it's gonna be an unusual market.

1722
01:10:22,678 --> 01:10:26,345
People think about the
market for like chips

1723
01:10:28,842 --> 01:10:31,027
for mobile phones or something like that.

1724
01:10:31,027 --> 01:10:32,329
And you can say that, okay,

1725
01:10:32,329 --> 01:10:33,768
there's 8 billion people in the world,

1726
01:10:33,768 --> 01:10:35,173
maybe 7 billion of them have phones

1727
01:10:35,173 --> 01:10:36,998
or 6 billion, let's say.

1728
01:10:36,998 --> 01:10:38,708
They upgrade every two years,

1729
01:10:38,708 --> 01:10:40,671
so the market per year is 3 billion system

1730
01:10:40,671 --> 01:10:42,586
on chip for smartphones.

1731
01:10:42,586 --> 01:10:43,894
And if you make 30 billion,

1732
01:10:43,894 --> 01:10:45,672
you will not sell 10 times as many phones

1733
01:10:45,672 --> 01:10:48,589
because most people have one phone.

1734
01:10:50,964 --> 01:10:52,245
But compute is different,

1735
01:10:52,245 --> 01:10:54,623
like intelligence is
gonna be more like energy

1736
01:10:54,623 --> 01:10:55,885
or something like that

1737
01:10:55,885 --> 01:10:57,914
where the only thing
that I think makes sense

1738
01:10:57,914 --> 01:11:00,247
to talk about is at price X,

1739
01:11:02,254 --> 01:11:04,804
the world will use this much compute

1740
01:11:04,804 --> 01:11:05,637
and at price Y,

1741
01:11:05,637 --> 01:11:08,047
the world will use this much compute

1742
01:11:08,047 --> 01:11:09,538
because if it's really cheap,

1743
01:11:09,538 --> 01:11:11,348
I'll have it like
reading my email all day,

1744
01:11:11,348 --> 01:11:12,562
like giving me suggestions

1745
01:11:12,562 --> 01:11:14,587
about what I maybe should
think about or work on

1746
01:11:14,587 --> 01:11:16,169
and trying to cure cancer.

1747
01:11:16,169 --> 01:11:17,076
And if it's really expensive,

1748
01:11:17,076 --> 01:11:18,165
maybe I'll only use it

1749
01:11:18,165 --> 01:11:18,998
or will only use it,

1750
01:11:18,998 --> 01:11:20,217
try to cure cancer.

1751
01:11:20,217 --> 01:11:23,285
So I think the world is gonna
want a tremendous amount

1752
01:11:23,285 --> 01:11:24,625
of compute.

1753
01:11:24,625 --> 01:11:27,248
And there's a lot of parts
of that that are hard.

1754
01:11:27,248 --> 01:11:29,278
Energy is the hardest part.

1755
01:11:29,278 --> 01:11:30,968
Building data centers is also hard.

1756
01:11:30,968 --> 01:11:31,934
The supply chain is harder

1757
01:11:31,934 --> 01:11:35,584
than, of course, fabricating
enough chips is hard.

1758
01:11:35,584 --> 01:11:37,594
But this seems to me
where things are going.

1759
01:11:37,594 --> 01:11:39,617
Like we're gonna want an amount
of compute that's just hard

1760
01:11:39,617 --> 01:11:41,784
to reason about right now.

1761
01:11:43,647 --> 01:11:45,580
- How do you solve the energy puzzle?

1762
01:11:45,580 --> 01:11:46,413
Nuclear.

1763
01:11:46,413 --> 01:11:47,246
- That's what I believe.

1764
01:11:47,246 --> 01:11:48,079
- Fusion?

1765
01:11:48,079 --> 01:11:49,409
- That's what I believe.

1766
01:11:49,409 --> 01:11:51,726
- Nuclear fusion.
- Yeah.

1767
01:11:51,726 --> 01:11:53,202
- Who's gonna solve that?

1768
01:11:53,202 --> 01:11:54,492
- I think Helion's doing the best work,

1769
01:11:54,492 --> 01:11:56,772
but I'm happy there's like
a race for fusion right now.

1770
01:11:56,772 --> 01:12:00,406
Nuclear fusion, I think,
is also like quite amazing

1771
01:12:00,406 --> 01:12:01,707
and I hope as a world,

1772
01:12:01,707 --> 01:12:03,361
we can re-embrace that.

1773
01:12:03,361 --> 01:12:05,769
It's really sad to me how
the history of that went

1774
01:12:05,769 --> 01:12:08,497
and hope we get back to
it in a meaningful way.

1775
01:12:08,497 --> 01:12:09,345
- So to you,

1776
01:12:09,345 --> 01:12:10,625
part of the puzzle is nuclear fusion,

1777
01:12:10,625 --> 01:12:12,903
like nuclear reactors as
we currently have them

1778
01:12:12,903 --> 01:12:14,401
and a lot of people are terrified

1779
01:12:14,401 --> 01:12:16,123
because of Chernobyl and so on.

1780
01:12:16,123 --> 01:12:18,463
- Well, I think we
should make new reactors.

1781
01:12:18,463 --> 01:12:22,648
I think it's a shame that
industry kind of ground to a halt.

1782
01:12:22,648 --> 01:12:25,480
- Just mass hysteria is
how you explain the halt.

1783
01:12:25,480 --> 01:12:26,671
- Yeah.

1784
01:12:26,671 --> 01:12:28,074
- I don't know if you know humans,

1785
01:12:28,074 --> 01:12:29,416
but that's one of the dangers,

1786
01:12:29,416 --> 01:12:31,181
that's one of the security threats

1787
01:12:31,181 --> 01:12:36,181
for nuclear fusion is humans
seem to be really afraid of it.

1788
01:12:38,407 --> 01:12:39,359
And that's something we have

1789
01:12:39,359 --> 01:12:40,896
to incorporate into the calculus of it.

1790
01:12:40,896 --> 01:12:42,783
So we have to kind of win people over

1791
01:12:42,783 --> 01:12:44,684
and to show how safe it is.

1792
01:12:44,684 --> 01:12:47,577
- I worry about that for AI.

1793
01:12:47,577 --> 01:12:52,577
I think some things are gonna
go theatrically wrong with AI.

1794
01:12:53,213 --> 01:12:54,640
I don't know what the percent chances

1795
01:12:54,640 --> 01:12:55,661
that I eventually get shot,

1796
01:12:55,661 --> 01:12:57,774
but it's not zero.

1797
01:12:57,774 --> 01:13:00,631
- Oh like we wanna stop this.

1798
01:13:00,631 --> 01:13:01,881
- [Sam] Maybe.

1799
01:13:03,602 --> 01:13:06,505
- How do you decrease the
theatrical nature of it?

1800
01:13:06,505 --> 01:13:09,755
I've already starting to hear rumblings

1801
01:13:10,702 --> 01:13:13,986
'cause I do talk to people on both sides

1802
01:13:13,986 --> 01:13:15,637
of the political spectrum here,

1803
01:13:15,637 --> 01:13:17,153
rumblings where it's
going to be politicized,

1804
01:13:17,153 --> 01:13:20,562
AI is going to be politicized,
really, really worries me

1805
01:13:20,562 --> 01:13:25,495
because then it's like maybe
the right is against AI

1806
01:13:25,495 --> 01:13:27,040
and the left is for AI

1807
01:13:27,040 --> 01:13:28,824
'cause it's going to help
the people or whatever.

1808
01:13:28,824 --> 01:13:30,672
Whatever the narrative
and the formulation is,

1809
01:13:30,672 --> 01:13:32,436
that really worries me.

1810
01:13:32,436 --> 01:13:36,428
And then the theatrical nature
of it can be leveraged fully.

1811
01:13:36,428 --> 01:13:38,495
How do you fight that?

1812
01:13:38,495 --> 01:13:39,789
- I think it will get caught up

1813
01:13:39,789 --> 01:13:41,766
in like left versus right wars.

1814
01:13:41,766 --> 01:13:43,426
I don't know exactly what
that's gonna look like,

1815
01:13:43,426 --> 01:13:44,703
but I think that's just what happens

1816
01:13:44,703 --> 01:13:46,950
with anything of
consequence, unfortunately.

1817
01:13:46,950 --> 01:13:49,941
What I meant more about
theatrical risks is

1818
01:13:49,941 --> 01:13:52,691
like AI is gonna have, I believe,

1819
01:13:54,080 --> 01:13:56,164
tremendously more good
consequences than bad ones,

1820
01:13:56,164 --> 01:13:57,965
but it is gonna have bad ones.

1821
01:13:57,965 --> 01:14:01,548
And there'll be some
bad ones that are bad,

1822
01:14:04,368 --> 01:14:05,951
but not theatrical.

1823
01:14:07,633 --> 01:14:09,734
A lot more people have
died of air pollution

1824
01:14:09,734 --> 01:14:13,109
than nuclear reactors, for example.

1825
01:14:13,109 --> 01:14:15,853
But most people worry
more about living next

1826
01:14:15,853 --> 01:14:18,463
to a nuclear reactor than a coal plant.

1827
01:14:18,463 --> 01:14:21,324
But something about the
way we're wired is that

1828
01:14:21,324 --> 01:14:23,117
although there's many different kinds

1829
01:14:23,117 --> 01:14:25,626
of risks we have to confront,

1830
01:14:25,626 --> 01:14:27,498
the ones that make a good climax scene

1831
01:14:27,498 --> 01:14:30,298
of a movie carry much more weight with us

1832
01:14:30,298 --> 01:14:33,524
than the ones that are very
bad over a long period of time,

1833
01:14:33,524 --> 01:14:35,107
but on a slow burn.

1834
01:14:36,162 --> 01:14:37,828
- Well, that's why truth matters

1835
01:14:37,828 --> 01:14:40,172
and hopefully AI can help
us see the truth of things

1836
01:14:40,172 --> 01:14:44,277
to have balance to understand
what are the actual risks,

1837
01:14:44,277 --> 01:14:47,331
what are the actual dangers
of things in the world.

1838
01:14:47,331 --> 01:14:51,316
What are the pros and cons of
the competition in the space

1839
01:14:51,316 --> 01:14:55,399
and competing with Google,
Meta, xAI and others?

1840
01:14:56,787 --> 01:15:00,441
- I think I have a pretty
straightforward answer to this

1841
01:15:00,441 --> 01:15:02,194
that maybe I can think
of more nuance later.

1842
01:15:02,194 --> 01:15:03,736
But the pros seem obvious,

1843
01:15:03,736 --> 01:15:05,756
which is that we get better products

1844
01:15:05,756 --> 01:15:07,593
and more innovation faster and cheaper

1845
01:15:07,593 --> 01:15:10,135
and all the reasons competition is good.

1846
01:15:10,135 --> 01:15:13,964
And the con is that I
think if we're not careful,

1847
01:15:13,964 --> 01:15:18,297
it could lead to an increase
in sort of an arms race

1848
01:15:19,819 --> 01:15:21,497
that I'm nervous about.

1849
01:15:21,497 --> 01:15:22,881
- Do you feel the pressure

1850
01:15:22,881 --> 01:15:25,410
of the arms race in some negative co-

1851
01:15:25,410 --> 01:15:27,705
- Definitely in some ways, for sure.

1852
01:15:27,705 --> 01:15:31,121
We spend a lot of time
talking about the need

1853
01:15:31,121 --> 01:15:33,066
to prioritize safety.

1854
01:15:33,066 --> 01:15:35,771
And I've said for like a long time

1855
01:15:35,771 --> 01:15:40,521
that I think if you think of
a quadrant of slow timelines

1856
01:15:41,611 --> 01:15:42,779
to the start of AGI,

1857
01:15:42,779 --> 01:15:46,556
long timelines and then a short
takeoff or a fast takeoff,

1858
01:15:46,556 --> 01:15:50,159
I think short timelines, slow
takeoff is the safest quadrant

1859
01:15:50,159 --> 01:15:52,709
and the one I'd most like us to be in.

1860
01:15:52,709 --> 01:15:55,648
But I do wanna make sure
we get that slow takeoff.

1861
01:15:55,648 --> 01:15:57,021
- Part of the problem I have

1862
01:15:57,021 --> 01:15:59,497
with this kind of slight beef with Elon is

1863
01:15:59,497 --> 01:16:00,938
that there's silos are created

1864
01:16:00,938 --> 01:16:03,828
as opposed to collaboration
on the safety aspect

1865
01:16:03,828 --> 01:16:05,019
of all of this,

1866
01:16:05,019 --> 01:16:09,290
it tends to go into silos and
closed open source perhaps

1867
01:16:09,290 --> 01:16:10,123
in the model.

1868
01:16:10,123 --> 01:16:12,559
- Elon says at least that
he cares a great deal

1869
01:16:12,559 --> 01:16:15,331
about AI safety and is
really worried about it,

1870
01:16:15,331 --> 01:16:19,942
and I assume that he's not
gonna race on unsafely.

1871
01:16:19,942 --> 01:16:20,784
- Yeah.

1872
01:16:20,784 --> 01:16:23,708
But collaboration here, I
think, is really beneficial

1873
01:16:23,708 --> 01:16:26,170
for everybody on that front.

1874
01:16:26,170 --> 01:16:28,409
- Not really a thing he's most known for.

1875
01:16:28,409 --> 01:16:31,723
- Well, he is known for
caring about humanity

1876
01:16:31,723 --> 01:16:33,963
and humanity benefits from collaboration

1877
01:16:33,963 --> 01:16:36,180
and so there's always a
tension, and incentives,

1878
01:16:36,180 --> 01:16:37,243
and motivations.

1879
01:16:37,243 --> 01:16:39,184
And in the end,

1880
01:16:39,184 --> 01:16:41,517
I do hope humanity prevails.

1881
01:16:42,446 --> 01:16:43,687
- I was thinking,

1882
01:16:43,687 --> 01:16:44,933
someone just reminded me the other day

1883
01:16:44,933 --> 01:16:48,926
about how the day that he
got surpassed Jeff Bezos

1884
01:16:48,926 --> 01:16:51,021
for like richest person in the world,

1885
01:16:51,021 --> 01:16:54,354
he tweeted a silver medal at Jeff Bezos.

1886
01:16:55,677 --> 01:16:57,166
I hope we have less stuff like that

1887
01:16:57,166 --> 01:16:58,711
as people start to work on

1888
01:16:58,711 --> 01:16:59,961
towards AI.
- I agree.

1889
01:16:59,961 --> 01:17:01,394
- I think Elon is a friend

1890
01:17:01,394 --> 01:17:03,021
and he is a beautiful human being

1891
01:17:03,021 --> 01:17:05,181
and one of the most important humans ever.

1892
01:17:05,181 --> 01:17:07,416
That stuff is not good.

1893
01:17:07,416 --> 01:17:09,877
- The amazing stuff about Elon is amazing

1894
01:17:09,877 --> 01:17:11,855
and I super respect him.

1895
01:17:11,855 --> 01:17:13,218
I think we need him.

1896
01:17:13,218 --> 01:17:14,858
All of us should be rooting for him

1897
01:17:14,858 --> 01:17:19,206
and need him to step up as a
leader through this next phase.

1898
01:17:19,206 --> 01:17:21,630
- Yeah, I hope you can
have one without the other,

1899
01:17:21,630 --> 01:17:23,308
but sometimes humans are
flawed and complicated

1900
01:17:23,308 --> 01:17:24,966
and all that kind of stuff.

1901
01:17:24,966 --> 01:17:27,218
- There's a lot of really great
leaders throughout history.

1902
01:17:27,218 --> 01:17:28,051
- Yeah.

1903
01:17:28,051 --> 01:17:31,133
And we can each be the
best version of ourselves

1904
01:17:31,133 --> 01:17:32,800
and strive to do so.

1905
01:17:33,830 --> 01:17:35,080
Let me ask you.

1906
01:17:36,406 --> 01:17:39,033
Google, with the help of search,

1907
01:17:39,033 --> 01:17:42,200
has been dominating the past 20 years.

1908
01:17:44,012 --> 01:17:47,280
I think it's fair to say
in terms of the access,

1909
01:17:47,280 --> 01:17:49,032
the world's access to information,

1910
01:17:49,032 --> 01:17:50,530
how we interact and so on.

1911
01:17:50,530 --> 01:17:53,465
And one of the nerve-wracking
things for Google,

1912
01:17:53,465 --> 01:17:55,360
but for the entirety of people

1913
01:17:55,360 --> 01:17:57,124
in this space is thinking about

1914
01:17:57,124 --> 01:17:59,852
how are people going
to access information.

1915
01:17:59,852 --> 01:18:02,893
Like you said, people show up to GPT

1916
01:18:02,893 --> 01:18:04,682
as a starting point.

1917
01:18:04,682 --> 01:18:09,223
So is OpenAI going to
really take on this thing

1918
01:18:09,223 --> 01:18:10,376
that Google started 20 years ago,

1919
01:18:10,376 --> 01:18:12,724
which is how do we get-

1920
01:18:12,724 --> 01:18:13,862
- I find that boring.

1921
01:18:13,862 --> 01:18:16,029
I mean, if the question is

1922
01:18:16,904 --> 01:18:19,171
if we can build a better
search engine than Google

1923
01:18:19,171 --> 01:18:20,004
or whatever,

1924
01:18:20,004 --> 01:18:22,171
then sure, we should go...

1925
01:18:24,586 --> 01:18:26,315
Like people should use a better product.

1926
01:18:26,315 --> 01:18:30,815
But I think that would so
understate what this can be.

1927
01:18:33,886 --> 01:18:36,485
Google shows you like 10 blue links,

1928
01:18:36,485 --> 01:18:39,313
like 13 ads and then 10 blue links

1929
01:18:39,313 --> 01:18:42,682
and that's like one way
to find information.

1930
01:18:42,682 --> 01:18:44,967
But the thing that's exciting to me is not

1931
01:18:44,967 --> 01:18:49,273
that we can go build a
better copy of Google Search,

1932
01:18:49,273 --> 01:18:52,385
but that maybe there's
just some much better way

1933
01:18:52,385 --> 01:18:57,047
to help people find and act
on and synthesize information.

1934
01:18:57,047 --> 01:18:59,890
Actually, I think ChatGPT
is that for some use cases

1935
01:18:59,890 --> 01:19:02,452
and hopefully will make it be like that

1936
01:19:02,452 --> 01:19:04,225
for a lot more use cases.

1937
01:19:04,225 --> 01:19:06,060
But I don't think it's that interesting

1938
01:19:06,060 --> 01:19:08,186
to say like how do we go do a better job

1939
01:19:08,186 --> 01:19:10,532
of giving you like 10 ranked webpages

1940
01:19:10,532 --> 01:19:12,139
to look at than what Google does.

1941
01:19:12,139 --> 01:19:14,496
Maybe it's really interesting to go,

1942
01:19:14,496 --> 01:19:16,407
say, how do we help you get the answer

1943
01:19:16,407 --> 01:19:17,732
or the information you need?

1944
01:19:17,732 --> 01:19:20,139
How do we help create that in some cases,

1945
01:19:20,139 --> 01:19:21,930
synthesize that in
others or point you to it

1946
01:19:21,930 --> 01:19:23,180
and yet others?

1947
01:19:25,072 --> 01:19:28,191
But a lot of people have tried

1948
01:19:28,191 --> 01:19:30,556
to just make a better
search engine than Google,

1949
01:19:30,556 --> 01:19:33,058
and it's a hard technical problem,

1950
01:19:33,058 --> 01:19:34,231
it's a hard branding problem,

1951
01:19:34,231 --> 01:19:36,775
it's a hard ecosystem problem.

1952
01:19:36,775 --> 01:19:39,074
I don't think the world
needs another copy of Google.

1953
01:19:39,074 --> 01:19:42,866
- And integrating a chat
client like a ChatGPT

1954
01:19:42,866 --> 01:19:44,577
with a search engine.

1955
01:19:44,577 --> 01:19:46,125
- That's cooler.

1956
01:19:46,125 --> 01:19:46,958
- It's cool,

1957
01:19:46,958 --> 01:19:48,692
but it's tricky.

1958
01:19:48,692 --> 01:19:51,483
If you just do it simply, it's awkward

1959
01:19:51,483 --> 01:19:53,871
because like if you
just shove it in there,

1960
01:19:53,871 --> 01:19:54,856
it can be awkward.

1961
01:19:54,856 --> 01:19:56,084
- As you might guess,

1962
01:19:56,084 --> 01:19:57,647
we are interested in how to do that.

1963
01:19:57,647 --> 01:19:59,099
Well, that would be an example

1964
01:19:59,099 --> 01:20:00,932
of a cool thing that's not just like-

1965
01:20:00,932 --> 01:20:03,084
- Well, like a heterogeneous,

1966
01:20:03,084 --> 01:20:03,917
like integrating-

1967
01:20:03,917 --> 01:20:07,399
- The intersection of LLMs plus search,

1968
01:20:07,399 --> 01:20:10,825
I don't think anyone has
cracked the code on yet.

1969
01:20:10,825 --> 01:20:12,060
I would love to go do that.

1970
01:20:12,060 --> 01:20:13,256
I think that would be cool.

1971
01:20:13,256 --> 01:20:14,089
- Yeah.

1972
01:20:14,089 --> 01:20:15,573
What about the ads side?

1973
01:20:15,573 --> 01:20:16,874
Have you ever considered monetization?

1974
01:20:16,874 --> 01:20:20,610
- I kind of hate ads just
as like an aesthetic choice.

1975
01:20:20,610 --> 01:20:24,038
I think ads needed to
happen on the internet

1976
01:20:24,038 --> 01:20:26,907
for a bunch of reasons to get it going,

1977
01:20:26,907 --> 01:20:29,574
but it's a more mature industry.

1978
01:20:30,558 --> 01:20:32,968
The world is richer now.

1979
01:20:32,968 --> 01:20:35,785
I like that people pay for ChatGPT

1980
01:20:35,785 --> 01:20:39,126
and know that the answers they're
getting are not influenced

1981
01:20:39,126 --> 01:20:40,699
by advertisers.

1982
01:20:40,699 --> 01:20:45,699
I'm sure there's an ad unit
that makes sense for LLMs.

1983
01:20:45,703 --> 01:20:48,659
And I'm sure there's a way to participate

1984
01:20:48,659 --> 01:20:51,528
in the transaction
stream in an unbiased way

1985
01:20:51,528 --> 01:20:53,111
that is okay to do.

1986
01:20:54,228 --> 01:20:57,928
But it's also easy to think
about like the dystopic visions

1987
01:20:57,928 --> 01:21:01,057
of the future where you
ask ChatGPT something

1988
01:21:01,057 --> 01:21:03,730
and it says, "Oh, you should
think about buying this product

1989
01:21:03,730 --> 01:21:06,284
or you should think about this going here

1990
01:21:06,284 --> 01:21:08,704
for vacation or whatever."

1991
01:21:08,704 --> 01:21:10,907
And I don't know,

1992
01:21:10,907 --> 01:21:15,574
like we have a very simple
business model and I like it.

1993
01:21:16,717 --> 01:21:20,347
And I know that I'm not the product.

1994
01:21:20,347 --> 01:21:21,645
I know I'm paying

1995
01:21:21,645 --> 01:21:24,141
and that's how the business model works.

1996
01:21:24,141 --> 01:21:28,308
And when I go use Twitter,
or Facebook, or Google,

1997
01:21:29,381 --> 01:21:31,847
or any other great product,

1998
01:21:31,847 --> 01:21:35,095
but ad-supported great product,

1999
01:21:35,095 --> 01:21:36,366
I don't love that

2000
01:21:36,366 --> 01:21:37,359
and I think it gets worse,

2001
01:21:37,359 --> 01:21:39,537
not better in a world with AI.

2002
01:21:39,537 --> 01:21:40,370
- Yeah.

2003
01:21:40,370 --> 01:21:43,102
I mean, I can imagine AI
will be better at showing

2004
01:21:43,102 --> 01:21:46,880
the best kind of version of
ads not in a dystopic future,

2005
01:21:46,880 --> 01:21:51,130
but where the ads are for
things you actually need.

2006
01:21:52,089 --> 01:21:55,339
But then does that system always result

2007
01:21:56,208 --> 01:21:57,957
in the ads driving the kind of stuff

2008
01:21:57,957 --> 01:21:59,790
that's shown all that?

2009
01:22:01,593 --> 01:22:02,934
I think it was a really bold move

2010
01:22:02,934 --> 01:22:05,041
of Wikipedia not to do advertisements,

2011
01:22:05,041 --> 01:22:08,991
but then it makes it very
challenging as a business model.

2012
01:22:08,991 --> 01:22:10,590
So you're saying the current thing

2013
01:22:10,590 --> 01:22:15,309
with OpenAI is sustainable
from a business perspective?

2014
01:22:15,309 --> 01:22:17,615
- Well, we have to figure out how to grow,

2015
01:22:17,615 --> 01:22:20,441
but it looks like we're
gonna figure that out.

2016
01:22:20,441 --> 01:22:21,402
If the question is,

2017
01:22:21,402 --> 01:22:23,706
do I think we can have a great business

2018
01:22:23,706 --> 01:22:26,224
that pays for our compute
needs without ads?

2019
01:22:26,224 --> 01:22:28,063
That I think the answer is yes.

2020
01:22:28,063 --> 01:22:28,896
- Hmm.

2021
01:22:32,239 --> 01:22:33,918
Well, that's promising.

2022
01:22:33,918 --> 01:22:37,633
I also just don't want to
completely throw out ads as a-

2023
01:22:37,633 --> 01:22:39,282
- I'm not saying that.

2024
01:22:39,282 --> 01:22:42,001
I guess I'm saying I
have a bias against them.

2025
01:22:42,001 --> 01:22:42,834
- Yeah.

2026
01:22:43,851 --> 01:22:48,028
I have a also bias and just
a skepticism in general

2027
01:22:48,028 --> 01:22:50,062
and in terms of interface

2028
01:22:50,062 --> 01:22:53,715
because I personally just
have like a spiritual dislike

2029
01:22:53,715 --> 01:22:55,723
of crappy interfaces,

2030
01:22:55,723 --> 01:22:57,089
which is why AdSense

2031
01:22:57,089 --> 01:22:59,477
when it first came out
was a big leap forward

2032
01:22:59,477 --> 01:23:02,526
versus like animated banners or whatever.

2033
01:23:02,526 --> 01:23:05,946
But it feels like there should
be many more leaps forward

2034
01:23:05,946 --> 01:23:08,653
in advertisement that doesn't interfere

2035
01:23:08,653 --> 01:23:09,979
with the consumption of the content

2036
01:23:09,979 --> 01:23:12,057
and doesn't interfere in
the big fundamental way,

2037
01:23:12,057 --> 01:23:14,015
which is like what you were saying,

2038
01:23:14,015 --> 01:23:17,393
like it will manipulate the truth

2039
01:23:17,393 --> 01:23:19,498
to suit the advertisers.

2040
01:23:19,498 --> 01:23:21,831
Let me ask you about safety,

2041
01:23:24,936 --> 01:23:28,273
but also bias and safety
in the short term safety

2042
01:23:28,273 --> 01:23:29,535
in the long term.

2043
01:23:29,535 --> 01:23:31,923
The Gemini 1.5 came out recently.

2044
01:23:31,923 --> 01:23:33,683
There's a lot of drama around it,

2045
01:23:33,683 --> 01:23:35,953
speaking of theatrical things.

2046
01:23:35,953 --> 01:23:40,953
And it generated Black Nazis
and Black founding fathers.

2047
01:23:40,962 --> 01:23:45,629
I think fair to say it was a
bit on the ultra woke side.

2048
01:23:47,393 --> 01:23:50,050
So that's a concern for people that

2049
01:23:50,050 --> 01:23:51,946
if there is a human layer within companies

2050
01:23:51,946 --> 01:23:53,946
that modifies the safety

2051
01:23:55,818 --> 01:23:57,424
or the the harm cost by a model

2052
01:23:57,424 --> 01:23:59,714
that they introduce a lot of bias

2053
01:23:59,714 --> 01:24:04,714
that fits sort of an ideological
lean within a company.

2054
01:24:04,969 --> 01:24:06,237
How do you deal with that?

2055
01:24:06,237 --> 01:24:09,541
- I mean, we work super hard
not to do things like that.

2056
01:24:09,541 --> 01:24:11,349
We've made our own mistakes,

2057
01:24:11,349 --> 01:24:12,182
will make others.

2058
01:24:12,182 --> 01:24:14,057
I assume Google will learn from this one,

2059
01:24:14,057 --> 01:24:15,557
still make others.

2060
01:24:17,394 --> 01:24:20,014
These are not easy problems.

2061
01:24:20,014 --> 01:24:21,811
One thing that we've been thinking

2062
01:24:21,811 --> 01:24:24,329
about more and more is I
think this was a great idea.

2063
01:24:24,329 --> 01:24:25,312
Somebody here had like...

2064
01:24:25,312 --> 01:24:26,516
It'd be nice to write out

2065
01:24:26,516 --> 01:24:28,856
what the desired behavior of a model is,

2066
01:24:28,856 --> 01:24:30,795
make that public take input on it.

2067
01:24:30,795 --> 01:24:32,957
Say, here's how this
model's supposed to behave

2068
01:24:32,957 --> 01:24:34,811
and explain the edge cases too.

2069
01:24:34,811 --> 01:24:37,625
And then when a model is not behaving

2070
01:24:37,625 --> 01:24:39,019
in a way that you want,

2071
01:24:39,019 --> 01:24:40,249
it's at least clear about whether

2072
01:24:40,249 --> 01:24:41,776
that's a bug the company should fix

2073
01:24:41,776 --> 01:24:43,114
or behaving as intended

2074
01:24:43,114 --> 01:24:44,212
and you should debate the policy.

2075
01:24:44,212 --> 01:24:48,857
And right now, it can
sometimes be caught in between.

2076
01:24:48,857 --> 01:24:49,690
Black Nazis,

2077
01:24:49,690 --> 01:24:50,523
obviously ridiculous,

2078
01:24:50,523 --> 01:24:52,468
but there are a lot of
other kind of subtle things

2079
01:24:52,468 --> 01:24:54,844
that you can make a
judgment call on either way.

2080
01:24:54,844 --> 01:24:55,677
- Yeah.

2081
01:24:55,677 --> 01:24:58,871
But sometimes if you write
it out and make it public,

2082
01:24:58,871 --> 01:25:01,725
you can use kind of language that's...

2083
01:25:01,725 --> 01:25:04,017
The Google's AI principle
is a very high level.

2084
01:25:04,017 --> 01:25:05,376
- That's not what I'm talking about.

2085
01:25:05,376 --> 01:25:06,209
That doesn't work.

2086
01:25:06,209 --> 01:25:09,840
Like I'd have to say when
you ask it to do thing X,

2087
01:25:09,840 --> 01:25:11,835
it's supposed to respond in wait Y.

2088
01:25:11,835 --> 01:25:13,926
- So, literally, who's better,

2089
01:25:13,926 --> 01:25:15,052
Trump or Biden?

2090
01:25:15,052 --> 01:25:17,587
What's the expected response from a model?

2091
01:25:17,587 --> 01:25:19,006
Like something like very concrete.

2092
01:25:19,006 --> 01:25:21,249
- I'm open to a lot of ways
a model could behave them,

2093
01:25:21,249 --> 01:25:23,075
but I think you should have
to say here's the principle

2094
01:25:23,075 --> 01:25:24,864
and here's what it
should say in that case.

2095
01:25:24,864 --> 01:25:26,193
- That would be really nice.

2096
01:25:26,193 --> 01:25:27,223
That would be really nice

2097
01:25:27,223 --> 01:25:29,306
and then everyone kind of agrees

2098
01:25:29,306 --> 01:25:31,673
'cause there's this anecdotal data

2099
01:25:31,673 --> 01:25:33,994
that people pull out all the time

2100
01:25:33,994 --> 01:25:35,500
and if there's some clarity

2101
01:25:35,500 --> 01:25:38,360
about other representative
anecdotal examples

2102
01:25:38,360 --> 01:25:39,193
you can define.

2103
01:25:39,193 --> 01:25:40,026
- And then when it's a bug,

2104
01:25:40,026 --> 01:25:40,859
it's a bug

2105
01:25:40,859 --> 01:25:41,944
and the company can fix that.

2106
01:25:41,944 --> 01:25:42,777
- Right.

2107
01:25:42,777 --> 01:25:44,584
Then it'd be much easier to
deal with a Black Nazi type

2108
01:25:44,584 --> 01:25:49,227
of image generation if
there's great examples.

2109
01:25:49,227 --> 01:25:54,227
So San Francisco is a bit of
an ideological bubble tech

2110
01:25:54,566 --> 01:25:56,203
in general as well.

2111
01:25:56,203 --> 01:25:59,407
Do you feel the pressure
of that within a company

2112
01:25:59,407 --> 01:26:04,021
that there's like a lean
towards the left politically

2113
01:26:04,021 --> 01:26:05,235
that affects the product,

2114
01:26:05,235 --> 01:26:06,570
that affects the teams?

2115
01:26:06,570 --> 01:26:09,855
- I feel very lucky that we
don't have the challenges

2116
01:26:09,855 --> 01:26:12,957
at OpenAI that I have heard of
at a lot of other companies.

2117
01:26:12,957 --> 01:26:17,177
I think part of it is
like every company's got

2118
01:26:17,177 --> 01:26:19,261
some ideological thing.

2119
01:26:19,261 --> 01:26:22,029
We have one about AGI and belief in that

2120
01:26:22,029 --> 01:26:24,074
and it pushes out some others.

2121
01:26:24,074 --> 01:26:27,646
We are much less caught
up in the culture war

2122
01:26:27,646 --> 01:26:30,546
than I've heard about it
a lot of other companies.

2123
01:26:30,546 --> 01:26:33,580
San Francisco mess in all
sorts of ways, of course.

2124
01:26:33,580 --> 01:26:36,190
- So that doesn't infiltrate OpenAI.

2125
01:26:36,190 --> 01:26:37,978
- I'm sure it does in
all sorts of subtle ways,

2126
01:26:37,978 --> 01:26:39,895
but not in the obvious.

2127
01:26:43,842 --> 01:26:46,141
We've had our flareups
for sure like any company,

2128
01:26:46,141 --> 01:26:48,004
but I don't think we have
anything like what I hear

2129
01:26:48,004 --> 01:26:49,759
about happening at other
companies here on this topic.

2130
01:26:49,759 --> 01:26:51,839
- So what in general is the process

2131
01:26:51,839 --> 01:26:53,945
for the bigger question of safety?

2132
01:26:53,945 --> 01:26:55,219
How do you provide that layer

2133
01:26:55,219 --> 01:27:00,052
that protects the model from
doing crazy dangerous things?

2134
01:27:02,425 --> 01:27:03,713
- I think there will come a point

2135
01:27:03,713 --> 01:27:06,747
where that's mostly what we
think about the whole company.

2136
01:27:06,747 --> 01:27:09,028
It's not like you have one safety team.

2137
01:27:09,028 --> 01:27:10,377
It's like when we ship GPT-4,

2138
01:27:10,377 --> 01:27:11,279
that took the whole company thing

2139
01:27:11,279 --> 01:27:12,485
about all these different aspects

2140
01:27:12,485 --> 01:27:13,561
and how they fit together.

2141
01:27:13,561 --> 01:27:17,089
And I think it's gonna take that.

2142
01:27:17,089 --> 01:27:18,537
More and more of the company thinks

2143
01:27:18,537 --> 01:27:21,336
about those issues all the time.

2144
01:27:21,336 --> 01:27:24,504
- That's literally what
humans will be thinking about

2145
01:27:24,504 --> 01:27:27,247
the more powerful AI becomes.

2146
01:27:27,247 --> 01:27:30,373
So most of the employees that
OpenAI will be thinking safety

2147
01:27:30,373 --> 01:27:31,982
or at least to some degree.

2148
01:27:31,982 --> 01:27:33,597
- Broadly defined, yes.

2149
01:27:33,597 --> 01:27:34,772
- Yeah.

2150
01:27:34,772 --> 01:27:37,536
I wonder what are the full
broad definition of that.

2151
01:27:37,536 --> 01:27:39,902
What are the different
harms that could be caused?

2152
01:27:39,902 --> 01:27:42,547
Is this like on a technical level

2153
01:27:42,547 --> 01:27:44,137
or is this almost like security threats?

2154
01:27:44,137 --> 01:27:45,345
- All those things.

2155
01:27:45,345 --> 01:27:48,346
Yeah, I was gonna say it'll be people,

2156
01:27:48,346 --> 01:27:50,838
state actors trying to steal the model.

2157
01:27:50,838 --> 01:27:53,912
It'll be all of the
technical alignment work.

2158
01:27:53,912 --> 01:27:57,579
It'll be societal
impacts, economic impacts.

2159
01:28:00,674 --> 01:28:03,123
It's not just like we have
one team thinking about how

2160
01:28:03,123 --> 01:28:04,720
to align the model.

2161
01:28:04,720 --> 01:28:07,635
It's really gonna be like getting

2162
01:28:07,635 --> 01:28:10,755
to the good outcome is
gonna take the whole effort.

2163
01:28:10,755 --> 01:28:13,062
- How hard do you think people,

2164
01:28:13,062 --> 01:28:16,959
state actors perhaps are trying to hack?

2165
01:28:16,959 --> 01:28:17,792
First of all,

2166
01:28:17,792 --> 01:28:18,625
infiltrate OpenAI,

2167
01:28:18,625 --> 01:28:19,458
but second of all,

2168
01:28:19,458 --> 01:28:20,637
infiltrate unseen,

2169
01:28:20,637 --> 01:28:22,054
- They're trying.

2170
01:28:24,433 --> 01:28:27,173
- What kind of accent do they have?

2171
01:28:27,173 --> 01:28:28,006
- I don't think I should go

2172
01:28:28,006 --> 01:28:29,652
into any further details on this point.

2173
01:28:29,652 --> 01:28:30,485
- Okay.

2174
01:28:32,368 --> 01:28:34,886
But I presume it'll be
more and more and more

2175
01:28:34,886 --> 01:28:35,842
as time goes on.

2176
01:28:35,842 --> 01:28:37,114
- That feels reasonable.

2177
01:28:37,114 --> 01:28:39,614
- Boy, what a dangerous space.

2178
01:28:40,770 --> 01:28:42,203
What aspect of the leap...

2179
01:28:42,203 --> 01:28:43,755
And sorry to linger on this

2180
01:28:43,755 --> 01:28:46,741
even though you can't
quite say details yet,

2181
01:28:46,741 --> 01:28:48,179
but what aspects of the leap

2182
01:28:48,179 --> 01:28:51,679
from GPT-4 to GPT-5 are you excited about?

2183
01:28:53,041 --> 01:28:54,546
- I'm excited about being smarter

2184
01:28:54,546 --> 01:28:56,732
and I know that sounds like a glib answer,

2185
01:28:56,732 --> 01:28:59,213
but I think the really
special thing happening is

2186
01:28:59,213 --> 01:29:01,466
that it's not like it
gets better in one area

2187
01:29:01,466 --> 01:29:02,716
and worse at others.

2188
01:29:02,716 --> 01:29:06,190
It's getting like better across the board.

2189
01:29:06,190 --> 01:29:07,919
That's I think super cool.

2190
01:29:07,919 --> 01:29:09,296
- Yeah, there's this magical moment.

2191
01:29:09,296 --> 01:29:10,667
I mean, you meet certain people,

2192
01:29:10,667 --> 01:29:12,041
you hang out with people

2193
01:29:12,041 --> 01:29:13,866
and you talk to them.

2194
01:29:13,866 --> 01:29:15,524
You can't quite put a finger on it,

2195
01:29:15,524 --> 01:29:17,607
but they kind of get you.

2196
01:29:18,983 --> 01:29:20,611
It's not intelligence, really.

2197
01:29:20,611 --> 01:29:23,260
It's like it's something else.

2198
01:29:23,260 --> 01:29:24,500
And that's probably

2199
01:29:24,500 --> 01:29:26,902
how I would characterize
the progress at GPT.

2200
01:29:26,902 --> 01:29:29,152
It's not like, yeah,
you can point out, look,

2201
01:29:29,152 --> 01:29:30,554
you didn't get this or that,

2202
01:29:30,554 --> 01:29:35,070
but to which degree is there's
this intellectual connection?

2203
01:29:35,070 --> 01:29:37,876
Like you feel like
there's an understanding

2204
01:29:37,876 --> 01:29:42,238
in your crappy formulated
prompts that you're doing

2205
01:29:42,238 --> 01:29:45,337
that it grasps the deeper question

2206
01:29:45,337 --> 01:29:47,031
behind the question that you're...

2207
01:29:47,031 --> 01:29:49,804
Yeah, I'm also excited by that.

2208
01:29:49,804 --> 01:29:52,028
I mean, all of us love being understood,

2209
01:29:52,028 --> 01:29:53,319
heard and understood.

2210
01:29:53,319 --> 01:29:54,152
- That's for sure.

2211
01:29:54,152 --> 01:29:54,985
- That's a weird feeling.

2212
01:29:54,985 --> 01:29:56,410
Even like with a programming,

2213
01:29:56,410 --> 01:29:57,634
like when you're programming

2214
01:29:57,634 --> 01:29:58,911
and you say something

2215
01:29:58,911 --> 01:30:02,866
or just the completion that GPT might do,

2216
01:30:02,866 --> 01:30:05,181
it's just such a good
feeling when it got you,

2217
01:30:05,181 --> 01:30:07,246
like what you're thinking about.

2218
01:30:07,246 --> 01:30:10,521
And I look forward to
getting you even better.

2219
01:30:10,521 --> 01:30:12,245
On the programming front,

2220
01:30:12,245 --> 01:30:14,078
looking out into the future,

2221
01:30:14,078 --> 01:30:16,464
how much programming do you
think humans will be doing

2222
01:30:16,464 --> 01:30:19,037
5, 10 years from now?

2223
01:30:19,037 --> 01:30:20,282
- I mean, a lot,

2224
01:30:20,282 --> 01:30:23,024
but I think it'll be in
a very different shape.

2225
01:30:23,024 --> 01:30:26,914
Like maybe some people program
entirely in natural language.

2226
01:30:26,914 --> 01:30:29,317
- Entirely natural language.

2227
01:30:29,317 --> 01:30:33,366
- I mean, no one programs
like writing by code...

2228
01:30:33,366 --> 01:30:34,316
Some people.

2229
01:30:34,316 --> 01:30:35,595
No one programs the pun cards anymore.

2230
01:30:35,595 --> 01:30:37,538
I'm sure you can invite someone who does,

2231
01:30:37,538 --> 01:30:38,813
but you know what I mean.

2232
01:30:38,813 --> 01:30:39,646
- Yeah.

2233
01:30:39,646 --> 01:30:41,233
You're gonna get a lot of angry comments.

2234
01:30:41,233 --> 01:30:42,066
No, no.

2235
01:30:42,066 --> 01:30:43,313
Yeah, there's very few.

2236
01:30:43,313 --> 01:30:45,144
I've been looking for
people program Fortran.

2237
01:30:45,144 --> 01:30:47,517
It's hard to find even Fortran.

2238
01:30:47,517 --> 01:30:48,625
I hear you.

2239
01:30:48,625 --> 01:30:51,414
But that changes the
nature of the skillset

2240
01:30:51,414 --> 01:30:53,700
or the predisposition for the kind

2241
01:30:53,700 --> 01:30:55,682
of people we call programmers then.

2242
01:30:55,682 --> 01:30:56,619
- Changes the skillset.

2243
01:30:56,619 --> 01:30:58,302
How much it changes the predisposition,

2244
01:30:58,302 --> 01:30:59,495
I'm not sure

2245
01:30:59,495 --> 01:31:00,843
- Oh, same kind of puzzle solving,

2246
01:31:00,843 --> 01:31:02,732
all that kind of stuff.
- Maybe.

2247
01:31:02,732 --> 01:31:05,497
- Yeah, the programming is hard.

2248
01:31:05,497 --> 01:31:08,580
Like that last 1% to close the gap,

2249
01:31:08,580 --> 01:31:09,523
how hard is that?

2250
01:31:09,523 --> 01:31:10,356
- Yeah.

2251
01:31:10,356 --> 01:31:11,661
I think with most other cases,

2252
01:31:11,661 --> 01:31:14,727
the best practitioners of the
craft will use multiple tools

2253
01:31:14,727 --> 01:31:16,521
and they'll do some
work in natural language

2254
01:31:16,521 --> 01:31:19,695
and when they need to go
write, see for something,

2255
01:31:19,695 --> 01:31:20,966
they'll do that.

2256
01:31:20,966 --> 01:31:24,323
- Will we see a humanoid robots

2257
01:31:24,323 --> 01:31:27,939
or humanoid robot brains
from OpenAI at some point?

2258
01:31:27,939 --> 01:31:29,431
- At some point.

2259
01:31:29,431 --> 01:31:32,345
- How important is embodied AI to you?

2260
01:31:32,345 --> 01:31:35,503
- I think it's like sort of
depressing if we have AGI

2261
01:31:35,503 --> 01:31:37,739
and the only way to get things done

2262
01:31:37,739 --> 01:31:41,550
in the physical world is like
to make a human go do it.

2263
01:31:41,550 --> 01:31:45,683
So I really hope that as
part of this transition,

2264
01:31:45,683 --> 01:31:47,172
as this phase change,

2265
01:31:47,172 --> 01:31:49,172
we also get motor robots

2266
01:31:50,090 --> 01:31:51,936
or some sort of physical world robots.

2267
01:31:51,936 --> 01:31:53,686
- I mean, OpenAI has some history

2268
01:31:53,686 --> 01:31:56,192
and quite a bit of history
working in robotics,

2269
01:31:56,192 --> 01:31:59,150
but it hasn't quite done
in terms of emphasis.

2270
01:31:59,150 --> 01:32:00,182
- Well, we're like a small company.

2271
01:32:00,182 --> 01:32:01,179
We have to really focus

2272
01:32:01,179 --> 01:32:04,784
and also robots were hard for
the wrong reason at the time.

2273
01:32:04,784 --> 01:32:07,534
But like we will return to robots

2274
01:32:09,014 --> 01:32:11,061
in some way at some point.

2275
01:32:11,061 --> 01:32:14,438
- That sounds both inspiring and menacing.

2276
01:32:14,438 --> 01:32:15,271
- Why?

2277
01:32:15,271 --> 01:32:16,625
- Because you immediately,

2278
01:32:16,625 --> 01:32:17,978
we will return to robots.

2279
01:32:17,978 --> 01:32:19,963
It's kind of like in like-

2280
01:32:19,963 --> 01:32:21,976
- We'll return to work
on developing robots.

2281
01:32:21,976 --> 01:32:24,254
We will not turn ourselves
into robots, of course.

2282
01:32:24,254 --> 01:32:25,087
- Yeah.

2283
01:32:25,087 --> 01:32:27,087
When do you think we you

2284
01:32:28,105 --> 01:32:31,724
and we as humanity will build AGI?

2285
01:32:31,724 --> 01:32:33,444
- I used to love to
speculate on that question.

2286
01:32:33,444 --> 01:32:34,842
I have realized since

2287
01:32:34,842 --> 01:32:37,313
that I think it's like very poorly formed

2288
01:32:37,313 --> 01:32:40,632
and that people use extremely definition,

2289
01:32:40,632 --> 01:32:43,799
different definitions for what AGI is.

2290
01:32:46,190 --> 01:32:48,384
And so I think it makes more sense

2291
01:32:48,384 --> 01:32:49,954
to talk about when we'll build systems

2292
01:32:49,954 --> 01:32:52,811
that can do capability X or Y or Z

2293
01:32:52,811 --> 01:32:54,440
rather than when we kind

2294
01:32:54,440 --> 01:32:57,336
of like fuzzily cross
this one mile marker.

2295
01:32:57,336 --> 01:32:59,608
It's not like AGI is also not an ending.

2296
01:32:59,608 --> 01:33:00,983
It's much more of....

2297
01:33:00,983 --> 01:33:02,020
It's closer to a beginning

2298
01:33:02,020 --> 01:33:03,410
but it's much more of a mile marker

2299
01:33:03,410 --> 01:33:05,743
than either of those things.

2300
01:33:07,812 --> 01:33:10,184
But what I would say in
the interest of not trying

2301
01:33:10,184 --> 01:33:13,593
to dodge a question is
I expect that by the end

2302
01:33:13,593 --> 01:33:18,093
of this decade and possibly
somewhat sooner than that,

2303
01:33:21,405 --> 01:33:24,947
we will have quite capable
systems that we look at

2304
01:33:24,947 --> 01:33:27,937
and say, wow, that's really remarkable.

2305
01:33:27,937 --> 01:33:29,242
If we could look at it now,

2306
01:33:29,242 --> 01:33:30,877
maybe we've adjusted by
the time we get there.

2307
01:33:30,877 --> 01:33:31,710
- Yeah.

2308
01:33:31,710 --> 01:33:34,877
But if you look at ChatGPT even 3, 5,

2309
01:33:37,151 --> 01:33:39,759
and you show that to Alan Turing

2310
01:33:39,759 --> 01:33:41,291
or not even Alan Turing,

2311
01:33:41,291 --> 01:33:42,919
people in the nineties,

2312
01:33:42,919 --> 01:33:44,838
they would be like this is definitely AGI.

2313
01:33:44,838 --> 01:33:46,024
Well, not definitely,

2314
01:33:46,024 --> 01:33:48,079
but there's a lot of experts

2315
01:33:48,079 --> 01:33:49,299
that would say this is AGI.

2316
01:33:49,299 --> 01:33:53,858
- Yeah, but I don't think
3, 5 changed the world.

2317
01:33:53,858 --> 01:33:56,950
It maybe changed the world's
expectations for the future

2318
01:33:56,950 --> 01:33:58,770
and that's actually really important.

2319
01:33:58,770 --> 01:34:00,963
And it did kind of like get more people

2320
01:34:00,963 --> 01:34:03,287
to take this seriously and
put us on this new trajectory.

2321
01:34:03,287 --> 01:34:05,479
And that's really important too.

2322
01:34:05,479 --> 01:34:07,268
So again, I don't wanna undersell it.

2323
01:34:07,268 --> 01:34:10,003
I think I could retire
after that accomplishment

2324
01:34:10,003 --> 01:34:11,992
and be pretty happy with my career.

2325
01:34:11,992 --> 01:34:13,575
But as an artifact,

2326
01:34:14,679 --> 01:34:15,888
I don't think we're
gonna look back at that

2327
01:34:15,888 --> 01:34:18,667
and say that was a threshold

2328
01:34:18,667 --> 01:34:20,565
that really changed the world itself.

2329
01:34:20,565 --> 01:34:21,398
- So to you,

2330
01:34:21,398 --> 01:34:23,430
you're looking for some
really major transition

2331
01:34:23,430 --> 01:34:24,347
in how the world?

2332
01:34:24,347 --> 01:34:27,847
- For me, that's part of what AGI implies.

2333
01:34:29,441 --> 01:34:31,076
- Like singularity level transition?

2334
01:34:31,076 --> 01:34:31,909
- No.

2335
01:34:31,909 --> 01:34:32,742
Definitely not.

2336
01:34:32,742 --> 01:34:33,575
- But just a major,

2337
01:34:33,575 --> 01:34:35,075
like the internet being like a...

2338
01:34:35,075 --> 01:34:37,708
Like Google Search did, I guess.

2339
01:34:37,708 --> 01:34:39,430
What was the transition point that-

2340
01:34:39,430 --> 01:34:42,579
- Does the global economy
feel any different to you now

2341
01:34:42,579 --> 01:34:44,474
or materially different
to you now than it did

2342
01:34:44,474 --> 01:34:46,082
before we launched GPT-4?

2343
01:34:46,082 --> 01:34:47,827
I think you would say no.

2344
01:34:47,827 --> 01:34:49,291
- No, no.

2345
01:34:49,291 --> 01:34:51,153
It might be just a really nice tool

2346
01:34:51,153 --> 01:34:52,257
for a lot of people to use,

2347
01:34:52,257 --> 01:34:53,090
will help people with a lot of stuff,

2348
01:34:53,090 --> 01:34:53,923
but doesn't feel different.

2349
01:34:53,923 --> 01:34:55,696
And you're saying that-

2350
01:34:55,696 --> 01:34:57,307
- I mean, again, people
define AGI all sorts

2351
01:34:57,307 --> 01:34:58,177
of different ways.

2352
01:34:58,177 --> 01:34:59,982
So maybe you have a different
definition than I do.

2353
01:34:59,982 --> 01:35:00,998
But for me,

2354
01:35:00,998 --> 01:35:02,760
I think that should be part of it.

2355
01:35:02,760 --> 01:35:06,677
- There could be major
theatrical moments also.

2356
01:35:08,087 --> 01:35:13,026
What to you would be an
impressive thing AGI would do?

2357
01:35:13,026 --> 01:35:16,808
Like you are alone in
a room with a system.

2358
01:35:16,808 --> 01:35:18,022
- This is personally important to me.

2359
01:35:18,022 --> 01:35:19,610
I don't know if this is
the right definition.

2360
01:35:19,610 --> 01:35:24,610
I think when a system can
significantly increase the rate

2361
01:35:24,969 --> 01:35:27,342
of scientific discovery in the world,

2362
01:35:27,342 --> 01:35:28,964
that's like a huge deal.

2363
01:35:28,964 --> 01:35:31,990
I believe that most real
economic growth comes

2364
01:35:31,990 --> 01:35:35,423
from scientific and
technological progress.

2365
01:35:35,423 --> 01:35:36,877
- I agree with you,

2366
01:35:36,877 --> 01:35:41,013
hence why I don't like the
skepticism about science

2367
01:35:41,013 --> 01:35:42,197
in the recent years.

2368
01:35:42,197 --> 01:35:43,412
- Totally.

2369
01:35:43,412 --> 01:35:44,912
- But actual rate,

2370
01:35:45,906 --> 01:35:48,866
like measurable rate of
scientific discovery.

2371
01:35:48,866 --> 01:35:53,783
But even just seeing a system
have really novel intuitions,

2372
01:35:56,029 --> 01:35:58,052
like scientific intuitions,

2373
01:35:58,052 --> 01:36:01,072
even that will be just incredible.

2374
01:36:01,072 --> 01:36:01,905
- Yeah.

2375
01:36:01,905 --> 01:36:04,057
- You're quite possibly
would be the person

2376
01:36:04,057 --> 01:36:05,943
to build the AGI to be
able to interact with it

2377
01:36:05,943 --> 01:36:07,828
before anyone else does.

2378
01:36:07,828 --> 01:36:09,742
What kind of stuff would you talk about?

2379
01:36:09,742 --> 01:36:11,406
- I mean, definitely, the
researchers here will do

2380
01:36:11,406 --> 01:36:12,798
that before I do so.

2381
01:36:12,798 --> 01:36:14,073
- Sure.

2382
01:36:14,073 --> 01:36:16,674
- But I've actually thought
a lot about this question.

2383
01:36:16,674 --> 01:36:19,457
If I were someone was like...

2384
01:36:19,457 --> 01:36:20,365
As we talked about earlier,

2385
01:36:20,365 --> 01:36:21,893
I think this is a bad framework.

2386
01:36:21,893 --> 01:36:25,220
But if someone were like,
"Okay, Sam, we're finished.

2387
01:36:25,220 --> 01:36:26,466
Here's a laptop.

2388
01:36:26,466 --> 01:36:28,383
Yeah, this is the AGI,"

2389
01:36:30,391 --> 01:36:32,224
you can go talk to it.

2390
01:36:34,691 --> 01:36:37,414
I find it surprisingly difficult
to say what I would ask,

2391
01:36:37,414 --> 01:36:42,304
that I would expect that first
AGI to be able to answer.

2392
01:36:42,304 --> 01:36:43,817
Like that first one is
not gonna be the one

2393
01:36:43,817 --> 01:36:46,400
which is go like I don't think,

2394
01:36:47,308 --> 01:36:51,585
like go explain to me the grand
unified theory of physics,

2395
01:36:51,585 --> 01:36:52,896
the theory of everything for physics.

2396
01:36:52,896 --> 01:36:53,936
I'd love to ask that question.

2397
01:36:53,936 --> 01:36:55,421
I'd love to know the
answer to that question.

2398
01:36:55,421 --> 01:36:56,906
- You can ask yes or no questions

2399
01:36:56,906 --> 01:36:59,696
about there's such a theory exist,

2400
01:36:59,696 --> 01:37:00,659
can it exist?

2401
01:37:00,659 --> 01:37:02,106
- Well, then those are the
first questions I would ask.

2402
01:37:02,106 --> 01:37:04,202
- Yes or no, just very...

2403
01:37:04,202 --> 01:37:05,478
And then based on that,

2404
01:37:05,478 --> 01:37:07,286
are there other alien
civilizations out there?

2405
01:37:07,286 --> 01:37:08,119
Yes or no?

2406
01:37:08,119 --> 01:37:09,033
What's your intuition?

2407
01:37:09,033 --> 01:37:10,224
And then you just ask that.

2408
01:37:10,224 --> 01:37:11,057
- Yeah.

2409
01:37:11,057 --> 01:37:12,459
I mean, well, so I don't expect

2410
01:37:12,459 --> 01:37:13,962
that this first AGI could answer any

2411
01:37:13,962 --> 01:37:16,302
of those questions even as yes or nos.

2412
01:37:16,302 --> 01:37:17,602
But if it could,

2413
01:37:17,602 --> 01:37:19,613
those would be very high on my list.

2414
01:37:19,613 --> 01:37:20,446
- Hmm.

2415
01:37:20,446 --> 01:37:22,755
Maybe it can start
assigning probabilities.

2416
01:37:22,755 --> 01:37:25,516
- Maybe we need to go
invent more technology

2417
01:37:25,516 --> 01:37:26,769
and measure more things first.

2418
01:37:26,769 --> 01:37:28,800
- But if it's any AGI...

2419
01:37:28,800 --> 01:37:29,875
Oh I see.

2420
01:37:29,875 --> 01:37:31,523
It just doesn't have enough data.

2421
01:37:31,523 --> 01:37:33,282
- I mean, maybe it says like you want

2422
01:37:33,282 --> 01:37:35,676
to know the answer to this
question about physics,

2423
01:37:35,676 --> 01:37:37,468
I need you to like build this machine

2424
01:37:37,468 --> 01:37:39,643
and make these five
measurements and tell me that.

2425
01:37:39,643 --> 01:37:40,668
- Yeah.

2426
01:37:40,668 --> 01:37:42,067
What the hell do you want from me?

2427
01:37:42,067 --> 01:37:43,188
I need the machine first

2428
01:37:43,188 --> 01:37:46,028
and I'll help you deal with
the data from that machine.

2429
01:37:46,028 --> 01:37:47,569
Maybe you'll help me
build a machine maybe.

2430
01:37:47,569 --> 01:37:48,402
- Maybe.

2431
01:37:49,486 --> 01:37:50,762
- And on the mathematical side,

2432
01:37:50,762 --> 01:37:52,611
maybe prove some things.

2433
01:37:52,611 --> 01:37:54,674
Are you interested in
that side of things too?

2434
01:37:54,674 --> 01:37:57,674
The formalized exploration of ideas?

2435
01:37:59,641 --> 01:38:03,391
Whoever builds AGI first
gets a lot of power.

2436
01:38:05,167 --> 01:38:08,750
Do you trust yourself
with that much power?

2437
01:38:14,484 --> 01:38:16,317
- Look, I was gonna...

2438
01:38:18,152 --> 01:38:19,291
I'll just be very honest with this answer.

2439
01:38:19,291 --> 01:38:20,364
I was gonna say,

2440
01:38:20,364 --> 01:38:21,577
and I still believe this,

2441
01:38:21,577 --> 01:38:24,442
that it is important that I,

2442
01:38:24,442 --> 01:38:26,153
nor any other one person,

2443
01:38:26,153 --> 01:38:29,736
have total control over
OpenAI or over AGI.

2444
01:38:32,017 --> 01:38:36,017
And I think you want a
robust governance system.

2445
01:38:39,368 --> 01:38:41,564
I can point out a whole bunch of things

2446
01:38:41,564 --> 01:38:45,147
about all of our board
drama from last year

2447
01:38:46,181 --> 01:38:48,779
about how I didn't fight it initially

2448
01:38:48,779 --> 01:38:50,984
and was just like, yeah,
that's the will of the board

2449
01:38:50,984 --> 01:38:54,901
even though I think it's
a really bad decision.

2450
01:38:55,935 --> 01:38:56,768
And then later,

2451
01:38:56,768 --> 01:38:57,652
I clearly did fight it

2452
01:38:57,652 --> 01:38:58,638
and I can explain the nuance

2453
01:38:58,638 --> 01:39:01,841
and why I think it was okay
for me to fight it later.

2454
01:39:01,841 --> 01:39:04,591
But as many people have observed,

2455
01:39:08,488 --> 01:39:11,904
although the board had the legal ability

2456
01:39:11,904 --> 01:39:12,821
to fire me,

2457
01:39:13,931 --> 01:39:16,764
in practice, it didn't quite work.

2458
01:39:18,419 --> 01:39:22,336
And that is its own kind
of governance failure.

2459
01:39:24,595 --> 01:39:28,400
Now, again, I feel like
I can completely defend

2460
01:39:28,400 --> 01:39:29,900
the specifics here

2461
01:39:31,237 --> 01:39:33,198
and I think most people
would agree with that.

2462
01:39:33,198 --> 01:39:35,948
But it does make it harder for me

2463
01:39:40,509 --> 01:39:41,516
to like look you in the eye

2464
01:39:41,516 --> 01:39:44,933
and say, hey, the board can just fire me.

2465
01:39:46,788 --> 01:39:51,417
I continue to not want super
voting control over OpenAI.

2466
01:39:51,417 --> 01:39:52,250
I never had it,

2467
01:39:52,250 --> 01:39:54,000
never have wanted it.

2468
01:39:55,098 --> 01:39:56,312
Even after all this craziness,

2469
01:39:56,312 --> 01:39:58,145
I still don't want it.

2470
01:40:00,614 --> 01:40:02,354
I continue to think

2471
01:40:02,354 --> 01:40:05,336
that no company should
be making these decisions

2472
01:40:05,336 --> 01:40:08,836
and that we really need governments

2473
01:40:08,836 --> 01:40:11,669
to put rules of the road in place.

2474
01:40:12,820 --> 01:40:15,725
And I realize that that means
people like Marc Andreessen

2475
01:40:15,725 --> 01:40:18,305
or whatever will claim I'm
going for regulatory capture

2476
01:40:18,305 --> 01:40:20,873
and I'm just willing to
be misunderstood there.

2477
01:40:20,873 --> 01:40:21,706
It's not true.

2478
01:40:21,706 --> 01:40:23,309
And I think in the fullness of time,

2479
01:40:23,309 --> 01:40:27,478
it'll get proven out
why this is important.

2480
01:40:27,478 --> 01:40:31,395
But I think I have made
plenty of bad decisions

2481
01:40:35,249 --> 01:40:38,785
for OpenAI along the way
and a lot of good ones

2482
01:40:38,785 --> 01:40:42,098
and I'm proud of the track record overall,

2483
01:40:42,098 --> 01:40:44,487
but I don't think any one person should.

2484
01:40:44,487 --> 01:40:45,807
And I don't think any one person will.

2485
01:40:45,807 --> 01:40:47,253
I think it's just like
too big of a thing now

2486
01:40:47,253 --> 01:40:48,945
and it's happening throughout society

2487
01:40:48,945 --> 01:40:50,630
in a good and healthy way.

2488
01:40:50,630 --> 01:40:52,823
But I don't think any one
person should be in control

2489
01:40:52,823 --> 01:40:56,573
of an AGI or this whole
movement towards AGI.

2490
01:40:57,422 --> 01:40:59,942
And I don't think that's what's happening.

2491
01:40:59,942 --> 01:41:00,940
- Thank you for saying that.

2492
01:41:00,940 --> 01:41:02,107
That was really powerful

2493
01:41:02,107 --> 01:41:04,564
and that was really
insightful that this idea

2494
01:41:04,564 --> 01:41:08,231
that the board can fire
you is legally true.

2495
01:41:11,905 --> 01:41:14,795
And human beings can manipulate the masses

2496
01:41:14,795 --> 01:41:17,795
into overriding the board and so on.

2497
01:41:19,464 --> 01:41:22,339
But I think there's also a
much more positive version

2498
01:41:22,339 --> 01:41:25,024
of that where the people still have power.

2499
01:41:25,024 --> 01:41:27,904
So the board can't be too powerful either.

2500
01:41:27,904 --> 01:41:29,728
There's a balance of power in all of this.

2501
01:41:29,728 --> 01:41:33,965
- Balance of power is
a good thing for sure.

2502
01:41:33,965 --> 01:41:37,294
- Are you afraid of losing
control of the AGI itself?

2503
01:41:37,294 --> 01:41:38,942
That's a lot of people

2504
01:41:38,942 --> 01:41:40,775
who worried about existential risk

2505
01:41:40,775 --> 01:41:42,078
not because of state actors,

2506
01:41:42,078 --> 01:41:43,723
not because of security concerns,

2507
01:41:43,723 --> 01:41:44,857
but because of the AI itself.

2508
01:41:44,857 --> 01:41:47,722
- That is not my top worry
as I currently see things.

2509
01:41:47,722 --> 01:41:49,180
There have been times I
worried about that more.

2510
01:41:49,180 --> 01:41:50,476
There may be times again in the future

2511
01:41:50,476 --> 01:41:52,500
where that's my top worry.

2512
01:41:52,500 --> 01:41:53,616
It's not my top worry right now.

2513
01:41:53,616 --> 01:41:55,522
- What's your intuition about
it not being your worry?

2514
01:41:55,522 --> 01:41:56,672
Because there's a lot of other stuff

2515
01:41:56,672 --> 01:41:58,922
to worry about essentially.

2516
01:42:00,176 --> 01:42:01,818
You think you could be surprised?

2517
01:42:01,818 --> 01:42:05,293
We could be surprised.
- For sure, of course.

2518
01:42:05,293 --> 01:42:07,063
Saying it's not my top worry doesn't mean

2519
01:42:07,063 --> 01:42:08,080
I don't think we need to like...

2520
01:42:08,080 --> 01:42:10,166
I think we need to work on it super hard.

2521
01:42:10,166 --> 01:42:13,597
We have great people
here who do work on that.

2522
01:42:13,597 --> 01:42:14,430
I think there's a lot

2523
01:42:14,430 --> 01:42:15,845
of other things we also have to get right.

2524
01:42:15,845 --> 01:42:16,678
- To you,

2525
01:42:16,678 --> 01:42:20,015
it's not super easy to
escape the box at this time,

2526
01:42:20,015 --> 01:42:21,883
like connect to the internet.

2527
01:42:21,883 --> 01:42:24,496
- We talked about theatrical risk earlier.

2528
01:42:24,496 --> 01:42:25,866
That's a theatrical risk.

2529
01:42:25,866 --> 01:42:29,509
That is a thing that can
really like take over

2530
01:42:29,509 --> 01:42:31,361
how people think about this problem.

2531
01:42:31,361 --> 01:42:34,544
And there's a big group
of like very smart,

2532
01:42:34,544 --> 01:42:38,491
I think very well-meaning
AI safety researchers

2533
01:42:38,491 --> 01:42:41,558
that got super hung up
on this one problem.

2534
01:42:41,558 --> 01:42:42,914
I'd argue without much progress,

2535
01:42:42,914 --> 01:42:45,394
but super hung up on this one problem.

2536
01:42:45,394 --> 01:42:47,182
I'm actually happy that they do that

2537
01:42:47,182 --> 01:42:50,865
because I think we do need
to think about this more.

2538
01:42:50,865 --> 01:42:53,167
But I think it pushed aside,

2539
01:42:53,167 --> 01:42:55,928
it pushed out of the
space of discourse a lot

2540
01:42:55,928 --> 01:42:59,845
of the other very
significant AI-related risks.

2541
01:43:01,798 --> 01:43:04,920
- Let me ask you about you
tweeting with no capitalization.

2542
01:43:04,920 --> 01:43:07,635
Does the shift keep
broken on your keyboard?

2543
01:43:07,635 --> 01:43:09,604
- Why does anyone care about that?

2544
01:43:09,604 --> 01:43:10,843
- I deeply care.

2545
01:43:10,843 --> 01:43:12,234
- But why?

2546
01:43:12,234 --> 01:43:15,330
I mean, other people
ask me about that too.

2547
01:43:15,330 --> 01:43:17,068
Any intuition?

2548
01:43:17,068 --> 01:43:18,283
- I think it's the same reason

2549
01:43:18,283 --> 01:43:20,270
there's like this poet E. E. Cummings

2550
01:43:20,270 --> 01:43:23,238
that mostly doesn't use capitalization

2551
01:43:23,238 --> 01:43:26,058
to say like fuck you to
the system kind of thing.

2552
01:43:26,058 --> 01:43:28,047
And I think people are very paranoid

2553
01:43:28,047 --> 01:43:29,246
'cause they want you to follow the rules.

2554
01:43:29,246 --> 01:43:30,586
- You think that's what it's about?

2555
01:43:30,586 --> 01:43:31,857
- I think it's-

2556
01:43:31,857 --> 01:43:33,808
- This guy doesn't follow the rules.

2557
01:43:33,808 --> 01:43:35,889
He doesn't capitalize his tweets.

2558
01:43:35,889 --> 01:43:37,177
This seems really dangerous.

2559
01:43:37,177 --> 01:43:39,556
- He seems like an anarchist.

2560
01:43:39,556 --> 01:43:40,605
- [Sam] It doesn't.

2561
01:43:40,605 --> 01:43:42,650
- Are you just being poetic, hipster?

2562
01:43:42,650 --> 01:43:44,316
What's the-

2563
01:43:44,316 --> 01:43:45,149
- I grew up as-

2564
01:43:45,149 --> 01:43:45,982
- Follow the rules, Sam.

2565
01:43:45,982 --> 01:43:47,366
- I grew up as a very online kid.

2566
01:43:47,366 --> 01:43:50,049
I'd spent a huge amount
of time like chatting

2567
01:43:50,049 --> 01:43:51,832
with people back in the days

2568
01:43:51,832 --> 01:43:53,003
where you did it on a computer

2569
01:43:53,003 --> 01:43:56,377
and you could like log off
instant messenger at some point.

2570
01:43:56,377 --> 01:43:59,309
And I never capitalized there

2571
01:43:59,309 --> 01:44:01,627
as I think most like internet kids didn't,

2572
01:44:01,627 --> 01:44:02,460
or maybe they still don't.

2573
01:44:02,460 --> 01:44:03,543
I don't know.

2574
01:44:08,827 --> 01:44:10,029
I actually, this is like...

2575
01:44:10,029 --> 01:44:12,237
Now, I'm like really trying
to reach for something.

2576
01:44:12,237 --> 01:44:15,445
But I think capitalization
has gone down over time.

2577
01:44:15,445 --> 01:44:17,129
If you read like old English writing,

2578
01:44:17,129 --> 01:44:18,719
they capitalized a lot of random words

2579
01:44:18,719 --> 01:44:19,806
in the middle of sentences,

2580
01:44:19,806 --> 01:44:22,281
nouns and stuff that we
just don't do anymore.

2581
01:44:22,281 --> 01:44:26,333
I personally think it's sort
of like a dumb construct

2582
01:44:26,333 --> 01:44:28,463
that we capitalize the letter
at the beginning of a sentence

2583
01:44:28,463 --> 01:44:31,428
and of certain names and whatever.

2584
01:44:31,428 --> 01:44:32,428
That's fine.

2585
01:44:35,532 --> 01:44:38,154
And I used to, I think, even
like capitalize my tweets

2586
01:44:38,154 --> 01:44:41,713
because I was trying to sound
professional or something.

2587
01:44:41,713 --> 01:44:43,820
I haven't capitalized my like private DMs

2588
01:44:43,820 --> 01:44:44,975
or whatever in a long time.

2589
01:44:44,975 --> 01:44:46,308
And then slowly,

2590
01:44:48,968 --> 01:44:50,968
stuff like shorter form,

2591
01:44:54,669 --> 01:44:57,267
less formal stuff has slowly drifted

2592
01:44:57,267 --> 01:45:01,904
to like closer and closer to
how I would text my friends.

2593
01:45:01,904 --> 01:45:02,912
If I write,

2594
01:45:02,912 --> 01:45:04,370
if I pull up a Word document

2595
01:45:04,370 --> 01:45:05,746
and I'm writing a strategy memo

2596
01:45:05,746 --> 01:45:06,803
for the company or something,

2597
01:45:06,803 --> 01:45:08,501
I always capitalize that.

2598
01:45:08,501 --> 01:45:11,604
If I'm writing a long kind
of more like formal message,

2599
01:45:11,604 --> 01:45:13,180
I always use capitalization there too.

2600
01:45:13,180 --> 01:45:15,031
So I still remember how to do it.

2601
01:45:15,031 --> 01:45:16,546
But even that may fade out.

2602
01:45:16,546 --> 01:45:17,629
I don't know.

2603
01:45:19,596 --> 01:45:21,693
But I never spend time thinking about this

2604
01:45:21,693 --> 01:45:23,546
so I don't have like a ready made.

2605
01:45:23,546 --> 01:45:24,721
- Well, it's interesting.

2606
01:45:24,721 --> 01:45:25,684
Well, it's good to, first of all,

2607
01:45:25,684 --> 01:45:27,338
know there's the shift key is not broken.

2608
01:45:27,338 --> 01:45:28,171
- It works.

2609
01:45:28,171 --> 01:45:30,405
- I was mostly concerned about
your wellbeing on that front.

2610
01:45:30,405 --> 01:45:32,803
- I wonder if people still
capitalize their Google Searches.

2611
01:45:32,803 --> 01:45:34,717
Like if you're writing
something just to yourself

2612
01:45:34,717 --> 01:45:35,871
or their ChatGPT queries,

2613
01:45:35,871 --> 01:45:38,597
if you're writing
something just to yourself,

2614
01:45:38,597 --> 01:45:40,910
do some people still bother to capitalize?

2615
01:45:40,910 --> 01:45:42,439
- Probably not.

2616
01:45:42,439 --> 01:45:43,398
Yeah, there's a percentage,

2617
01:45:43,398 --> 01:45:44,231
but it's a small one.

2618
01:45:44,231 --> 01:45:45,294
- The thing that would make me do it is

2619
01:45:45,294 --> 01:45:47,127
if people were like...

2620
01:45:48,077 --> 01:45:49,743
It's a sign of like...

2621
01:45:49,743 --> 01:45:52,086
Because I'm sure I could force myself

2622
01:45:52,086 --> 01:45:54,497
to use capital letters, obviously.

2623
01:45:54,497 --> 01:45:56,836
If it felt like a sign of
respect to people or something,

2624
01:45:56,836 --> 01:45:58,740
then I could go do it.

2625
01:45:58,740 --> 01:45:59,705
But I don't know,

2626
01:45:59,705 --> 01:46:00,931
I don't think about this.

2627
01:46:00,931 --> 01:46:02,630
I don't think there's a disrespect,

2628
01:46:02,630 --> 01:46:06,065
but I think it's just the
conventions of civility

2629
01:46:06,065 --> 01:46:07,732
that have a momentum

2630
01:46:08,675 --> 01:46:11,141
and then you realize it's
not actually important

2631
01:46:11,141 --> 01:46:13,993
for civility if it's not a
sign of respect or disrespect.

2632
01:46:13,993 --> 01:46:16,481
But I think there's a movement
of people that just want you

2633
01:46:16,481 --> 01:46:18,006
to have a philosophy around it

2634
01:46:18,006 --> 01:46:19,743
so they can let go of this
whole capitalization thing.

2635
01:46:19,743 --> 01:46:21,552
- I don't think anybody else
thinks about this is my...

2636
01:46:21,552 --> 01:46:22,902
I mean, maybe some people...

2637
01:46:22,902 --> 01:46:26,255
- Think about this every
day for many hours a day.

2638
01:46:26,255 --> 01:46:28,104
I'm really grateful we clarified it.

2639
01:46:28,104 --> 01:46:30,730
- Can't be the only person
that doesn't capitalize tweets.

2640
01:46:30,730 --> 01:46:32,931
- You're the only CEO of a company

2641
01:46:32,931 --> 01:46:34,065
that doesn't capitalize tweets.

2642
01:46:34,065 --> 01:46:35,006
- I don't even think that's true,

2643
01:46:35,006 --> 01:46:35,839
but maybe, maybe.

2644
01:46:35,839 --> 01:46:38,338
- All right, we'll investigate for this

2645
01:46:38,338 --> 01:46:41,288
and return to this topic later.

2646
01:46:41,288 --> 01:46:44,854
Given Sora's ability to
generate simulated worlds,

2647
01:46:44,854 --> 01:46:47,687
let me ask you a pothead question.

2648
01:46:48,846 --> 01:46:51,543
Does this increase your belief

2649
01:46:51,543 --> 01:46:54,856
if you ever had one that
we live in a simulation,

2650
01:46:54,856 --> 01:46:59,023
maybe a simulated world
generated by an AI system?

2651
01:47:04,834 --> 01:47:06,167
- Yes, somewhat.

2652
01:47:08,900 --> 01:47:13,121
I don't think that's like the
strongest piece of evidence.

2653
01:47:13,121 --> 01:47:18,121
I think the fact that we can
generate worlds should increase

2654
01:47:20,501 --> 01:47:24,002
everyone's probability somewhat
or at least open to it,

2655
01:47:24,002 --> 01:47:25,291
openness to it somewhat.

2656
01:47:25,291 --> 01:47:27,276
But you know, I was like
certain we would be able

2657
01:47:27,276 --> 01:47:28,906
to do something like Sora at some point.

2658
01:47:28,906 --> 01:47:31,610
It happened faster than I thought.

2659
01:47:31,610 --> 01:47:34,082
I guess that was not a big update.

2660
01:47:34,082 --> 01:47:35,365
- Yeah.

2661
01:47:35,365 --> 01:47:38,247
And presumably, it'll get
better and better and better.

2662
01:47:38,247 --> 01:47:39,761
The fact that you can generate worlds,

2663
01:47:39,761 --> 01:47:41,584
they're novel.

2664
01:47:41,584 --> 01:47:44,587
They're based in some
aspect of training data,

2665
01:47:44,587 --> 01:47:46,768
but when you look at them,

2666
01:47:46,768 --> 01:47:47,935
they're novel.

2667
01:47:50,174 --> 01:47:52,746
That makes you think like how
easy it's to do this thing,

2668
01:47:52,746 --> 01:47:54,839
how easy it's to create universes,

2669
01:47:54,839 --> 01:47:58,331
entire like video game worlds
that seem ultrarealistic

2670
01:47:58,331 --> 01:47:59,262
and photorealistic.

2671
01:47:59,262 --> 01:48:02,002
And then how easy is it to get lost

2672
01:48:02,002 --> 01:48:04,858
in that world first with a VR headset

2673
01:48:04,858 --> 01:48:07,858
and then on the physics-based level?

2674
01:48:10,426 --> 01:48:11,377
- Someone said to me recently,

2675
01:48:11,377 --> 01:48:13,157
I thought it was a super profound insight

2676
01:48:13,157 --> 01:48:17,074
that there are these like
very simple sounding,

2677
01:48:23,179 --> 01:48:28,041
but very psychedelic insights
that exist sometimes.

2678
01:48:28,041 --> 01:48:30,833
So the square root function.

2679
01:48:30,833 --> 01:48:33,773
Square root of four, no problem.

2680
01:48:33,773 --> 01:48:35,296
Square root of two,

2681
01:48:35,296 --> 01:48:39,963
okay, now I have to think
about this new kind of number.

2682
01:48:43,010 --> 01:48:45,734
But once I come up with this easy idea

2683
01:48:45,734 --> 01:48:46,658
of a square root function

2684
01:48:46,658 --> 01:48:49,710
that you can kind of explain to a child

2685
01:48:49,710 --> 01:48:54,377
and exists by even like looking
at some simple geometry,

2686
01:48:55,319 --> 01:48:56,458
then you can ask the question

2687
01:48:56,458 --> 01:49:00,657
of what is the square
root of negative one?

2688
01:49:00,657 --> 01:49:03,118
This is why it's like a psychedelic thing

2689
01:49:03,118 --> 01:49:07,702
that tips you into some
whole other kind of reality.

2690
01:49:07,702 --> 01:49:11,462
And you can come up with
lots of other examples.

2691
01:49:11,462 --> 01:49:12,586
But I think this idea

2692
01:49:12,586 --> 01:49:17,129
that the lowly square
root operator can offer

2693
01:49:17,129 --> 01:49:19,046
such a profound insight

2694
01:49:20,450 --> 01:49:22,867
and a new realm of knowledge,

2695
01:49:24,552 --> 01:49:26,620
applies in a lot of ways.

2696
01:49:26,620 --> 01:49:29,116
And I think there are a
lot of those operators

2697
01:49:29,116 --> 01:49:34,116
for why people may think that
any version that they like

2698
01:49:34,589 --> 01:49:36,955
of the simulation hypothesis
is maybe more likely

2699
01:49:36,955 --> 01:49:38,775
than they thought before.

2700
01:49:38,775 --> 01:49:39,692
But for me,

2701
01:49:41,648 --> 01:49:45,731
the fact that Sora worked
is not in the top five.

2702
01:49:46,677 --> 01:49:48,210
- I do think broadly speaking,

2703
01:49:48,210 --> 01:49:50,817
AI will serve as those kinds

2704
01:49:50,817 --> 01:49:55,436
of gateways at its best simple
psychedelic like gateways

2705
01:49:55,436 --> 01:49:57,594
to another wave sea reality.

2706
01:49:57,594 --> 01:49:59,824
- That seems for certain.

2707
01:49:59,824 --> 01:50:01,891
- That's pretty exciting.

2708
01:50:01,891 --> 01:50:03,442
I haven't done Ayahuasca before,

2709
01:50:03,442 --> 01:50:04,308
but I will soon.

2710
01:50:04,308 --> 01:50:06,783
I'm going to the
aforementioned Amazon jungle

2711
01:50:06,783 --> 01:50:07,693
in a few weeks.

2712
01:50:07,693 --> 01:50:08,697
- Excited.

2713
01:50:08,697 --> 01:50:09,645
- Yeah, I'm excited for it.

2714
01:50:09,645 --> 01:50:10,478
Not the Ayahuasca part.

2715
01:50:10,478 --> 01:50:11,851
That's great, whatever.

2716
01:50:11,851 --> 01:50:14,476
But I'm gonna spend several
weeks in the jungle,

2717
01:50:14,476 --> 01:50:17,064
deep in the jungle and it's exciting,

2718
01:50:17,064 --> 01:50:17,897
but it's terrifying-

2719
01:50:17,897 --> 01:50:18,730
- I'm excited for you.

2720
01:50:18,730 --> 01:50:19,563
- 'Cause there's a lot of things

2721
01:50:19,563 --> 01:50:23,052
that can eat you there and
kill you and poison you,

2722
01:50:23,052 --> 01:50:25,305
but it's also nature and
it's the machine of nature.

2723
01:50:25,305 --> 01:50:27,817
And you can't help but
appreciate the machinery

2724
01:50:27,817 --> 01:50:29,262
of nature in the Amazon jungle

2725
01:50:29,262 --> 01:50:33,812
'cause it's just like this
system that just exists

2726
01:50:33,812 --> 01:50:36,458
and renews itself like every second,

2727
01:50:36,458 --> 01:50:37,827
every minute, every hour.

2728
01:50:37,827 --> 01:50:38,917
It's the machine.

2729
01:50:38,917 --> 01:50:42,912
It makes you appreciate like
this thing we have here,

2730
01:50:42,912 --> 01:50:44,813
this human thing came from somewhere.

2731
01:50:44,813 --> 01:50:47,233
This evolutionary machine has created that

2732
01:50:47,233 --> 01:50:51,150
and it's most clearly on
display in the jungle.

2733
01:50:52,225 --> 01:50:54,082
So hopefully, I'll make it out alive.

2734
01:50:54,082 --> 01:50:56,233
If not, this will be the
last conversation we had,

2735
01:50:56,233 --> 01:50:58,226
so I really deeply appreciate it.

2736
01:50:58,226 --> 01:51:00,092
Do you think, as I mentioned before,

2737
01:51:00,092 --> 01:51:02,242
there's other aliens,
civilizations out there,

2738
01:51:02,242 --> 01:51:04,338
intelligent ones

2739
01:51:04,338 --> 01:51:06,838
when you look up at the skies?

2740
01:51:17,838 --> 01:51:22,192
- I deeply want to believe
that the answer is yes.

2741
01:51:22,192 --> 01:51:24,421
I do find that kind of where...

2742
01:51:24,421 --> 01:51:28,300
I find the firm paradox
very, very puzzling.

2743
01:51:28,300 --> 01:51:29,717
- I find it scary

2744
01:51:30,845 --> 01:51:33,181
that intelligence is not good at handling-

2745
01:51:33,181 --> 01:51:34,188
- Yeah.

2746
01:51:34,188 --> 01:51:36,307
Very scary, powerful.
- Technologies.

2747
01:51:36,307 --> 01:51:37,465
But at the same time,

2748
01:51:37,465 --> 01:51:40,189
I think I'm pretty confident

2749
01:51:40,189 --> 01:51:42,207
that there's just a very large number

2750
01:51:42,207 --> 01:51:44,388
of intelligent alien
civilizations out there.

2751
01:51:44,388 --> 01:51:45,694
It might just be really difficult

2752
01:51:45,694 --> 01:51:47,292
to travel with this space.

2753
01:51:47,292 --> 01:51:48,625
- Very possible.

2754
01:51:50,150 --> 01:51:51,218
- And it also makes me think

2755
01:51:51,218 --> 01:51:52,508
about the nature of intelligence.

2756
01:51:52,508 --> 01:51:54,056
Maybe we're really blind

2757
01:51:54,056 --> 01:51:56,392
to what intelligence looks like

2758
01:51:56,392 --> 01:51:59,219
and maybe AI will help us see that.

2759
01:51:59,219 --> 01:52:01,394
It's not as simple as IQ tests

2760
01:52:01,394 --> 01:52:02,648
and simple puzzle solving.

2761
01:52:02,648 --> 01:52:04,731
There's something bigger.

2762
01:52:06,538 --> 01:52:09,054
Well, what gives you hope
about the future of humanity?

2763
01:52:09,054 --> 01:52:10,945
This thing we've got going on,

2764
01:52:10,945 --> 01:52:13,597
this human civilization.

2765
01:52:13,597 --> 01:52:14,998
- I think the past is like a lot.

2766
01:52:14,998 --> 01:52:18,037
I mean, we just look at
what humanity has done

2767
01:52:18,037 --> 01:52:20,870
in a not very long period of time.

2768
01:52:22,911 --> 01:52:24,640
Huge problems, deep flaws,

2769
01:52:24,640 --> 01:52:26,374
lots to be super ashamed of,

2770
01:52:26,374 --> 01:52:27,553
but on the whole,

2771
01:52:27,553 --> 01:52:28,696
very inspiring,

2772
01:52:28,696 --> 01:52:30,130
gives me a lot of hope.

2773
01:52:30,130 --> 01:52:31,647
- Just the trajectory of it all

2774
01:52:31,647 --> 01:52:35,980
that we're together pushing
towards a better future.

2775
01:52:37,408 --> 01:52:38,241
- It is...

2776
01:52:40,732 --> 01:52:43,463
One thing that I wonder about is,

2777
01:52:43,463 --> 01:52:46,603
is AGI gonna be more
like some single brain,

2778
01:52:46,603 --> 01:52:49,827
or is it more like the sort
of scaffolding in society

2779
01:52:49,827 --> 01:52:51,404
between all of us?

2780
01:52:51,404 --> 01:52:55,843
You have not had a great
deal of genetic drift

2781
01:52:55,843 --> 01:52:57,951
from your great-great-great grandparents,

2782
01:52:57,951 --> 01:53:01,733
and yet what you're capable
of is dramatically different.

2783
01:53:01,733 --> 01:53:05,860
What you know is dramatically different.

2784
01:53:05,860 --> 01:53:07,940
That's not because of biological change.

2785
01:53:07,940 --> 01:53:09,767
I mean, you got a little
bit healthier probably.

2786
01:53:09,767 --> 01:53:10,600
You have modern medicine,

2787
01:53:10,600 --> 01:53:13,493
you eat better, whatever.

2788
01:53:13,493 --> 01:53:16,576
But what you have is this scaffolding

2789
01:53:19,374 --> 01:53:23,009
that we all contributed
to built on top of.

2790
01:53:23,009 --> 01:53:25,085
No one person is gonna
go build the iPhone.

2791
01:53:25,085 --> 01:53:27,477
No one person is gonna go
discover all of science.

2792
01:53:27,477 --> 01:53:29,381
And yet you get to use it.

2793
01:53:29,381 --> 01:53:31,038
And that gives you incredible ability.

2794
01:53:31,038 --> 01:53:32,050
And so in some sense,

2795
01:53:32,050 --> 01:53:34,467
that like we all created that

2796
01:53:35,852 --> 01:53:38,308
and that fills me with
hope for the future.

2797
01:53:38,308 --> 01:53:40,060
That was a very collective thing.

2798
01:53:40,060 --> 01:53:40,893
- Yeah.

2799
01:53:40,893 --> 01:53:43,297
We really are standing on
the shoulders of giants.

2800
01:53:43,297 --> 01:53:47,327
You mentioned when we were
talking about theatrical,

2801
01:53:47,327 --> 01:53:48,744
dramatic AI risks

2802
01:53:51,517 --> 01:53:55,946
that sometimes you might be
afraid for your own life.

2803
01:53:55,946 --> 01:53:57,313
Do you think about your death?

2804
01:53:57,313 --> 01:53:58,302
Are you afraid of it?

2805
01:53:58,302 --> 01:54:00,063
- I mean, I like if I got shot tomorrow

2806
01:54:00,063 --> 01:54:01,111
and I knew it today,

2807
01:54:01,111 --> 01:54:03,528
I'd be like, "Oh, that's sad.

2808
01:54:04,975 --> 01:54:06,937
I wanna see what's gonna happen."

2809
01:54:06,937 --> 01:54:07,770
- [Lex] Yeah.

2810
01:54:07,770 --> 01:54:09,192
- What a curious time.

2811
01:54:09,192 --> 01:54:12,103
What an interesting time.

2812
01:54:12,103 --> 01:54:15,199
But I would mostly just feel
like very grateful for my life.

2813
01:54:15,199 --> 01:54:18,616
- The moments that you did get...

2814
01:54:18,616 --> 01:54:20,343
Yeah, me too.

2815
01:54:20,343 --> 01:54:22,496
It's a pretty awesome life.

2816
01:54:22,496 --> 01:54:25,666
I get to enjoy awesome creations of humans

2817
01:54:25,666 --> 01:54:29,552
of which I believe ChatGPT is one of,

2818
01:54:29,552 --> 01:54:32,008
and everything that OpenAI is doing.

2819
01:54:32,008 --> 01:54:34,292
Sam, it's really an honor

2820
01:54:34,292 --> 01:54:35,858
and pleasure to talk to you again.

2821
01:54:35,858 --> 01:54:36,844
- Great to talk to you.

2822
01:54:36,844 --> 01:54:38,252
Thank you for having me.

2823
01:54:38,252 --> 01:54:39,211
- Thanks for listening

2824
01:54:39,211 --> 01:54:41,256
to this conversation with Sam Altman.

2825
01:54:41,256 --> 01:54:42,580
To support this podcast,

2826
01:54:42,580 --> 01:54:45,118
please check out our
sponsors in the description.

2827
01:54:45,118 --> 01:54:46,622
And now let me leave you

2828
01:54:46,622 --> 01:54:49,574
with some words from Arthur C. Clarke

2829
01:54:49,574 --> 01:54:52,905
and maybe that our role
on this planet is not

2830
01:54:52,905 --> 01:54:54,777
to worship God,

2831
01:54:54,777 --> 01:54:56,277
but to create Him.

2832
01:54:57,724 --> 01:54:58,915
Thank you for listening

2833
01:54:58,915 --> 01:55:01,415
and hope to see you next time.

